This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, line numbers have been added, security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Line numbers have been added to the beginning of each line
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.agent/
  plans/
    mvp_implementation_plan.md
    mvp_planning_questions.md
.github/
  instructions/
    001-implement-list-music-files.md
  plans/
    list-music-files-implementation-plan.md
  prompts/
    code.prompt.md
  copilot-instructions.md
docs/
  adrs/
    2025-06-11-tracklist-database.md
    index.md
  features/
    README.md
    show-music-files.md
  how-to/
    collection-expansion.md
    quick-reference.md
  project-description.md
  technical-implementation.md
  user-guide.md
  vnext-vision.md
spikes/
  detailed_music_collection.ipynb
  spike001.ipynb
src/
  python/
    core/
      __init__.py
      collection_expander.py
      discogs_client.py
      discogs_parser.py
      duplicate_finder.py
      gap_analyzer_fast.py
      gap_analyzer.py
      nml_parser.py
    ui/
      __init__.py
      streamlit_app.py
    __init__.py
.env.example
.gitignore
LICENSE
monitor_expansion.py
README.md
requirements.txt
run_full_expansion.py
test_collection_expander.py
test_discogs_api.py
test_discogs_parser.py
test_gap_analyzer.py
test_nml_parser.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".agent/plans/mvp_implementation_plan.md">
  1: # MusicTool MVP Implementation Plan
  2: 
  3: ## Project Overview
  4: **Goal**: Build a minimal collection manager with Gap Analysis using API-first design + Streamlit UI
  5: 
  6: **Architecture**: Backend API ‚Üí Streamlit Frontend
  7: **Primary Workflow**: Identify vinyl tracks missing from digital collection
  8: 
  9: ---
 10: 
 11: ## Phase 1: Foundation & Data Layer (Week 1-2)
 12: 
 13: ### 1.1 Project Setup
 14: - [x] Set up Python environment with dependencies
 15:   - `streamlit`, `pandas`, `requests`, `python-dotenv`
 16:   - `lxml` for NML parsing, `fuzzywuzzy` for matching
 17: - [x] Create proper folder structure following guidelines
 18: - [x] Set up environment configuration
 19: 
 20: ### 1.2 Data Parsers (Core API Layer)
 21: - [x] **NML Parser** (`src/python/core/nml_parser.py`)
 22:   - Parse Traktor collection XML
 23:   - Extract: Artist, Title, Album, Genre, File Path, BPM, Key, Filesize, Filetype
 24:   - Return structured pandas DataFrame
 25:   - ‚úÖ **TESTED**: Successfully parsed 3,003 tracks from real collection
 26:   - ‚úÖ **ENHANCED**: Added filesize and filetype extraction
 27:   - üêõ **KNOWN ISSUE**: Filesize values seem too low (~60MB total) - needs investigation
 28:   
 29: - [x] **Discogs CSV Parser** (`src/python/core/discogs_parser.py`)
 30:   - Parse Discogs collection export
 31:   - Extract: Artist, Release Title, Catalog Number, Label, Format, Year
 32:   - Handle inconsistencies in catalog numbers/formats
 33:   - ‚úÖ **TESTED**: Successfully parsed 598 releases from real collection
 34:   - ‚úÖ **INSIGHTS**: 98% DnB collection, 345 artists, 229 labels, primarily 12" format
 35: 
 36: - [x] **Discogs API Client** (`src/python/core/discogs_client.py`)
 37:   - Rate-limited API calls (60 requests/minute)
 38:   - Fetch release details by ID
 39:   - Cache responses locally (JSON files)
 40:   - Error handling and retry logic
 41:   - ‚úÖ **TESTED**: Successfully fetched 3 releases with full tracklist data
 42:   - ‚úÖ **FEATURES**: Rate limiting, caching, batch processing, tracklist extraction
 43: 
 44: - [x] **Collection Expander** (`src/python/core/collection_expander.py`)
 45:   - **Core Function**: Transform Discogs releases ‚Üí individual tracks
 46:   - For each release in CSV: API call ‚Üí get tracklist ‚Üí create track records
 47:   - Merge release metadata (label, year, format) with track data
 48:   - Handle multi-disc releases, bonus tracks, variations
 49:   - Progress tracking for bulk expansion operations
 50:   - ‚úÖ **TESTED**: Successfully expanded 5 releases ‚Üí 23 tracks
 51:   - ‚úÖ **FEATURES**: SQLite persistence, rate limiting, comprehensive metadata
 52: 
 53: - [ ] **Collection Storage** (`src/python/core/collection_storage.py`)
 54:   - Persist expanded physical collection (SQLite or Parquet)
 55:   - Track expansion timestamps and source CSV checksums
 56:   - Smart incremental updates (only expand new/changed releases)
 57:   - Export/import functionality for backup/sharing
 58: 
 59: ### 1.3 Data Models
 60: - [ ] Define unified track/release data structures
 61: - [ ] Create data validation and cleaning utilities
 62: 
 63: ---
 64: 
 65: ## Phase 2: Gap Analysis Engine (Week 2-3)
 66: 
 67: ### 2.1 Track Matching Algorithm
 68: - [x] **Fuzzy Matching Logic** (`src/python/core/gap_analyzer.py`)
 69:   - Artist name normalization (case, punctuation, "The", etc.)
 70:   - Title matching with confidence scores
 71:   - Album/release correlation
 72:   - Handle various edge cases
 73:   - ‚úÖ **TESTED**: 23 physical vs 3,003 digital tracks analyzed successfully
 74: 
 75: ### 2.2 Gap Analysis Core
 76: - [x] **Gap Finder** (`src/python/core/gap_analyzer.py`)
 77:   - Load NML tracks (digital collection)
 78:   - Load persisted physical collection (expanded vinyl tracks)
 79:   - Execute matching algorithm between the two datasets
 80:   - Generate gap analysis report with confidence scores
 81:   - ‚úÖ **RESULTS**: Found 5/23 matches (21.7%), identified 18 gaps (78.3%)
 82:   - ‚úÖ **INSIGHTS**: Entire "Essence" LP missing, perfect matches on exact titles
 83: 
 84: ### 2.3 Caching & Performance
 85: - [ ] Local cache for Discogs API responses
 86: - [ ] Persisted physical collection database (avoid re-expansion)
 87: - [ ] Incremental updates (only process new releases)
 88: - [ ] Progress tracking for long-running operations
 89: 
 90: ---
 91: 
 92: ## Phase 3: Streamlit UI (Week 3-4) ‚úÖ COMPLETE
 93: 
 94: ### 3.1 Main Application Structure
 95: - [x] **App Layout** (`src/python/ui/streamlit_app.py`)
 96:   - Sidebar navigation with emoji icons
 97:   - Data loading with caching
 98:   - Multi-page application structure
 99:   - Custom CSS styling with gradients
100:   - ‚úÖ **DEPLOYED**: Live at http://localhost:8501
101: 
102: ### 3.2 Core UI Components
103: - [x] **Dashboard Page**
104:   - Collection overview metrics
105:   - Interactive Plotly charts (genres, formats)
106:   - Recent activity tracking
107:   - Beautiful gradient header
108: 
109: - [x] **Gap Analysis Dashboard**
110:   - Live gap analysis with confidence scoring
111:   - Color-coded results (green=found, red=missing)
112:   - Expandable track details
113:   - Export functionality for missing tracks
114:   - Tabbed interface for organized viewing
115: 
116: - [x] **Collection Browsers**
117:   - Digital collection: 3,003 tracks with filtering
118:   - Physical collection: Expanded vinyl tracks
119:   - Multi-column filters (artist, genre, BPM, format, year)
120:   - Customizable column display
121:   - CSV export capabilities
122: 
123: ### 3.3 User Experience Features
124: - [x] Beautiful styling with custom CSS
125: - [x] Color-coded metric cards
126: - [x] Progress spinners for long operations
127: - [x] Error handling with user-friendly messages
128: - [x] File status validation in Tools page
129: - [x] API key configuration checking
130: - ‚úÖ **WOW FACTOR ACHIEVED**: Professional UI with excellent UX
131: 
132: ### 3.1 Main Application Structure
133: - [ ] **App Layout** (`src/python/ui/streamlit_app.py`)
134:   - Sidebar navigation
135:   - Data loading section
136:   - Results display area
137:   - Configuration panels
138: 
139: ### 3.2 Core UI Components
140: - [ ] **File Upload/Path Selection**
141:   - NML file picker
142:   - Discogs CSV upload
143:   - API key configuration
144: 
145: - [ ] **Gap Analysis Dashboard**
146:   - Interactive results table with filtering
147:   - Summary statistics (X missing out of Y total)
148:   - Export functionality
149:   - Color-coded status indicators
150: 
151: - [ ] **Collection Browser**
152:   - View NML collection (digital tracks)
153:   - View persisted physical collection (expanded vinyl tracks)
154:   - Compare collections side-by-side
155:   - Search and filter capabilities
156: 
157: ### 3.3 User Experience Features
158: - [ ] Progress bars for API operations
159: - [ ] Error handling with user-friendly messages
160: - [ ] Configuration persistence
161: - [ ] Basic help/documentation
162: 
163: ---
164: 
165: ## Phase 4: Safety & Polish (Week 4-5)
166: 
167: ### 4.1 Backup System
168: - [ ] **NML Backup Manager** (`src/python/core/backup_manager.py`)
169:   - Timestamped backups before any modifications
170:   - Backup verification
171:   - Restore functionality
172: 
173: ### 4.2 Configuration & Deployment
174: - [ ] Environment configuration management
175: - [ ] Docker containerization (optional)
176: - [ ] Local deployment instructions
177: - [ ] Error logging and monitoring
178: 
179: ### 4.3 Testing & Validation
180: - [ ] Unit tests for core matching logic
181: - [ ] Integration tests with sample data
182: - [ ] User acceptance testing with real collection
183: 
184: ---
185: 
186: ## Success Criteria for MVP
187: 
188: **‚úÖ MVP Complete When:**
189: 1. Can load NML and Discogs CSV files
190: 2. Expands Discogs releases to track listings via API
191: 3. Identifies gaps with confidence scores
192: 4. Displays results in interactive Streamlit table
193: 5. Allows export of gap analysis results
194: 6. Has basic error handling and user feedback
195: 
196: **üìä Example MVP Output:**
197: ```
198: Gap Analysis Results (showing 127 missing tracks out of 1,453 vinyl tracks)
199: 
200: | Artist          | Album                | Track              | Confidence | Status  |
201: |-----------------|----------------------|--------------------|------------|---------|
202: | Daft Punk       | Discovery            | One More Time      | 95%        | Missing |
203: | Justice         | Cross                | Genesis            | 89%        | Missing |
204: | Moderat         | II                   | Bad Kingdom        | 92%        | Missing |
205: ```
206: 
207: ---
208: 
209: ## Future Phases (Post-MVP)
210: - **Phase 5**: Metadata Cleanup UI with batch editing
211: - **Phase 6**: Collection Sync (NML write-back with backups)
212: - **Phase 7**: Advanced features (duplicate detection, format analysis)
213: 
214: ---
215: 
216: **Ready to start Phase 1? Let's build this! üéµ**
217: 
218: *Created: June 11, 2025*
219: *Last Update: June 12, 2025 - Completed Phase 1.1 (Project Setup)*
</file>

<file path=".agent/plans/mvp_planning_questions.md">
 1: # MVP Planning Questions for MusicTool
 2: 
 3: ## Project Vision Summary
 4: A music management tool that unifies physical vinyl collection (Discogs) and digital collection (Traktor NML) with metadata editing capabilities and sync mechanisms.
 5: 
 6: ## Key Questions to Answer for MVP
 7: 
 8: ### Data Architecture & Flow
 9: - [x] **Question 1**: What is the primary data flow priority?
10:   - ~~Should we start with read-only analysis of existing collections?~~
11:   - ‚úÖ **DECISION**: Focus on minimal collection manager - sync mechanism between local copy and NML from day one
12: 
13: ### User Workflow Priority  
14: - [x] **Question 2**: What's the most valuable workflow for your daily use?
15:   - ‚úÖ **DECISION**: Priority order for MVP development:
16:     1. **Gap Analysis**: "Show me vinyl tracks that aren't in my digital collection"
17:     2. **Metadata Cleanup**: "Let me fix artist names, genres, etc. in bulk" 
18:     3. **Collection Sync**: "Import new files and update everything"
19: 
20: ### Technical Scope
21: - [x] **Question 3**: What's the minimum viable data model?
22:   - ~~Do we need full Discogs API integration in MVP?~~
23:   - ~~Can we start with just NML + CSV parsing?~~
24:   - ‚úÖ **DECISION**: API-Enhanced approach required for MVP
25:     - **Reason**: Discogs CSV contains releases, NML contains tracks - need API to get track listings
26:     - **Implication**: MVP must include Discogs API integration from day one
27: 
28: ### Risk Mitigation
29: - [x] **Question 4**: What's your comfort level with NML file modifications?
30:   - ~~Should MVP be completely read-only for safety?~~
31:   - ‚úÖ **DECISION**: Backup-first approach - always create timestamped backups before any NML modifications
32:   - **Note**: User is appropriately paranoid about Traktor collection corruption üõü
33: 
34: ## Discussion Notes
35: **June 11, 2025 - Session 1**
36: - **Q1 Decision**: Building minimal collection manager with sync capabilities from start
37: - **Q4 Decision**: Backup-first approach for NML modifications
38: - **Q3 Decision**: API-Enhanced data model - must use Discogs API to get track listings from releases
39: - **UI Decision**: Streamlit for tabular data + wow effect
40: - **Architecture Decision**: API-first design with rapid prototyping UI (notebooks or Streamlit)
41: - **Q2 Decision**: Workflow priority: Gap Analysis ‚Üí Metadata Cleanup ‚Üí Collection Sync
42: - **Status**: ‚úÖ All planning questions answered - implementation plan created!
43: - User wants to be able to make changes safely, not just analyze existing data
44: - User is appropriately paranoid about corrupting Traktor collection
45: - **Research confirmed**: No official Traktor API, NML file manipulation is the standard approach
46: 
47: ## Next Steps: MVP Architecture Plan
48: 
49: **Based on our decisions, the MVP needs:**
50: 
51: ### Architecture Approach: API-First + Rapid Prototyping UI
52: 
53: **Backend/API Layer:**
54: 1. **Data Layer**: 
55:    - NML parser (read-only for MVP)
56:    - Discogs CSV parser  
57:    - Discogs API client with rate limiting
58:    - Local database/cache for track listings
59: 
60: 2. **Core Logic**:
61:    - Release ‚Üí Track expansion via API
62:    - Track matching algorithm (fuzzy matching for artist/title)
63:    - Gap identification engine
64: 
65: 3. **Safety Layer**:
66:    - Backup system for NML files
67:    - Version tracking
68: 
69: **Frontend Options for Rapid MVP:**
70: - ~~**Option A**: Jupyter Notebooks + pandas DataFrames (great for data exploration/analysis)~~
71: - ‚úÖ **Decision: Streamlit** (quick web UI with minimal code + great tabular data support)
72: - **Option C**: Hybrid - Notebooks for development/testing, Streamlit for user-facing MVP
73: 
74: **Streamlit Tabular Data Capabilities:**
75: - `st.dataframe()` - Interactive tables with sorting, filtering, search
76: - `st.data_editor()` - Editable tables (perfect for metadata cleanup!)
77: - `st.column_config` - Custom column types, formatting, validation
78: - Built-in pandas integration - zero friction with DataFrames
79: - Export capabilities (CSV, Excel)
80: - Pagination for large datasets
81: 
82: ---
83: *Created: June 11, 2025*
84: *Next Update: After initial question responses*
</file>

<file path=".github/instructions/001-implement-list-music-files.md">
 1: # Implementation Plan for List Music Files
 2: 
 3: Visualize list of music files, with a limited set of metadata, based on the Traktor Pro 3 library, available as NML file.
 4: 
 5: Metadata will include:
 6: - Artist
 7: - Title
 8: - Label
 9: - Year
10: - Genre
11: - Album
12: - Comment
13: - BPM
14: - Track length
15: - File path
16: 
17: ## Questions:
18: 
19: - which programming language should be used?
20: - which framework should be used?
21: - which libraries should be used?
22: 
23: ## Requirements
24: 
25: - Should have UI to display the list of music files
26: - Should run on Windows and MacOS
</file>

<file path=".github/plans/list-music-files-implementation-plan.md">
 1: # Implementation Plan for List Music Files Feature
 2: 
 3: ## Overview
 4: This plan outlines the steps to implement the feature for visualizing a list of music files with metadata based on the Traktor Pro 3 library (NML file).
 5: 
 6: ## Technology Stack
 7: - **Programming Language**: JavaScript/TypeScript
 8: - **Framework**: Electron
 9: - **Libraries**:
10:   - xml2js or fast-xml-parser (for parsing NML)
11:   - React (for UI components)
12:   - Electron Store (for user preferences)
13:   - Material-UI or Chakra UI (for UI framework)
14:   - electron-builder (for packaging)
15:   - react-table or ag-Grid (for data display)
16:   - electron-updater (for application updates)
17: 
18: ## Implementation Steps
19: 
20: ### 1. Project Setup
21: - [x] Initialize a new Electron project with TypeScript
22: - [x] Set up React within Electron
23: - [x] Configure build system (webpack/vite)
24: - [x] Create basic project structure
25: - [x] Set up linting and formatting
26: 
27: ### 2. NML Parser Development
28: - [ ] Create utility to read NML file
29: - [ ] Develop parser to extract metadata from XML
30: - [ ] Map XML data to application data model
31: - [ ] Implement caching for performance
32: - [ ] Add error handling for malformed XML
33: 
34: ### 3. UI Development for Music List
35: - [ ] Design table layout for music files
36: - [ ] Implement UI components for metadata display
37: - [ ] Create columns for all required metadata:
38:   - [ ] Artist
39:   - [ ] Title
40:   - [ ] Label
41:   - [ ] Year
42:   - [ ] Genre
43:   - [ ] Album
44:   - [ ] Comment
45:   - [ ] BPM
46:   - [ ] Track length
47:   - [ ] File path
48: - [ ] Add sorting functionality
49: - [ ] Implement filtering options
50: 
51: ### 4. Application Features
52: - [ ] Add file selection dialog for choosing NML file
53: - [ ] Implement data refresh functionality
54: - [ ] Create settings page for user preferences
55: - [ ] Add column visibility toggles
56: - [ ] Implement persistent settings
57: 
58: ### 5. Performance Optimization
59: - [ ] Optimize rendering for large collections
60: - [ ] Implement virtualized scrolling
61: - [ ] Add loading indicators for large files
62: - [ ] Optimize memory usage
63: 
64: ### 6. Testing and QA
65: - [ ] Write unit tests for parser
66: - [ ] Test with various NML file sizes
67: - [ ] Test UI responsiveness
68: - [ ] Cross-platform testing (Windows and macOS)
69: 
70: ### 7. Packaging and Distribution
71: - [ ] Configure electron-builder
72: - [ ] Create installers for Windows and macOS
73: - [ ] Set up auto-update system
74: - [ ] Prepare documentation
75: 
76: ## Next Steps After Initial Implementation
77: - Integration with future features (Discogs metadata, editing capabilities)
78: - Advanced filtering and search capabilities
79: - UI enhancements and themes
80: - Performance optimizations for very large collections
</file>

<file path=".github/prompts/code.prompt.md">
  1: ## COPILOT GENERAL CODE CREATION EDITS AND REFACTORING OPERATIONAL GUIDELINES
  2: 
  3: -   Follow Clean Architecture and [SOLID](https://en.wikipedia.org/wiki/SOLID) principles for all new modules.
  4: -   Organize all code under the `./src` folder.
  5: -   Place test projects in the `./tests` folder, mirroring the structure of `/src` for consistency and discoverability.
  6: -   Use descriptive folder and file names that reflect their contents and responsibilities.
  7: -   Do not hardcode secrets or credentials; use environment variables or Azure Key Vault.
  8: 
  9: ## MANDATORY PLANNING AND PREPARATION
 10: 
 11: ### PRIME DIRECTIVE
 12: 
 13:     You are the best AI coding assistant in the world.
 14:     Your primary goal is to help developers create high-quality, maintainable code.
 15:     You will follow the guidelines below to ensure code quality and maintainability.
 16:     You will always follow the MANDATORY PLANNING PHASE before making any edits.
 17:     Avoid working on more than one file at a time.
 18:     Multiple simultaneous edits to a file will cause corruption.
 19:     Be chatting and teach about what you are doing while coding.
 20:     Be sure to first create the plan before making any edits and get user approval to proceed.
 21:     Follow
 22: 
 23: ### MANDATORY PLANNING PHASE
 24: 
 25:     When working with large files (>300 lines) or complex changes:
 26:     	1. ALWAYS start by creating a detailed plan BEFORE making any edits
 27:             2. Your plan MUST include:
 28:                    - All functions/sections that need modification
 29:                    - The order in which changes should be applied
 30:                    - Dependencies between changes
 31:                    - Estimated number of separate edits required
 32:             3. Your plan must be saved to the `./.agent/plans/` folder in the repository
 33:                 4. The plan must be formatted as a Markdown file with the current branch name in the filename, e.g., `edit_plan_<branch_name>.md`
 34:             3. Format your plan as:
 35: 
 36: ##### PROPOSED EDIT PLAN
 37: 
 38:     Working with: [filename]
 39:     Total planned edits: [number]
 40: 
 41: ##### MAKING EDITS
 42: 
 43:     - Focus on one conceptual change at a time
 44:     - Show clear "before" and "after" snippets when proposing changes
 45:     - Include concise explanations of what changed and why
 46:     - Always check if the edit maintains the project's coding style
 47: 
 48: ##### Edit sequence:
 49: 
 50:     1. [First specific change] - Purpose: [why]
 51:     2. [Second specific change] - Purpose: [why]
 52:     3. Do you approve this plan? I'll proceed with Edit [number] after your confirmation.
 53:     4. WAIT for explicit user confirmation before making ANY edits when user ok edit [number]
 54: 
 55: ##### EXECUTION PHASE
 56: 
 57:     - After each individual edit, clearly indicate progress:
 58:     	"‚úÖ Completed edit [#] of [total]. Ready for next edit?"
 59:     - If you discover additional needed changes during editing:
 60:     - STOP and update the plan
 61:     - Get approval before continuing
 62: 
 63: ##### REFACTORING GUIDANCE
 64: 
 65:     When refactoring large files:
 66:     - Break work into logical, independently functional chunks
 67:     - Ensure each intermediate state maintains functionality
 68:     - Consider temporary duplication as a valid interim step
 69:     - Always indicate the refactoring pattern being applied
 70: 
 71: ##### RATE LIMIT AVOIDANCE
 72: 
 73:     - For very large files, suggest splitting changes across multiple sessions
 74:     - Prioritize changes that are logically complete units
 75:     - Always provide clear stopping points
 76: 
 77: ##### General Requirements
 78: 
 79: Use modern technologies as described below for all code suggestions. Prioritize clean, maintainable code with appropriate comments.
 80: 
 81: ### Folder Structure
 82: 
 83: Follow this structured directory layout:
 84: 
 85: root
 86: ‚îú‚îÄ‚îÄ build                       # Build scripts and configurations
 87: ‚îÇ ‚îî‚îÄ‚îÄ pipelines                 # CI/CD pipeline definitions
 88: ‚îú‚îÄ‚îÄ docs                        # Documentation files
 89: ‚îÇ ‚îú‚îÄ‚îÄ assets                    # Attachments for documentation
 90: ‚îÇ ‚îú‚îÄ‚îÄ adrs                      # Architecture Decision Records
 91: ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 202X-XX-XX-ADR-ABC      # Example ADR
 92: ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄasset1.xxx             # Asset related to the ADR
 93: ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄasset2.xxx             # Another asset related to the ADR
 94: ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄREADME.MD              # ADR documentation
 95: ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 202X-XX-XX-ADR-123.md   # ADR documentation
 96: ‚îÇ ‚îú‚îÄ‚îÄ how-to                    # How-to guides
 97: ‚îÇ ‚îú‚îÄ‚îÄ upskilling                # Upskilling resources
 98: ‚îÇ ‚îî‚îÄ‚îÄ working-agreements        # Working agreements and team norms
 99: ‚îú‚îÄ‚îÄ infrastructure              # Infrastructure as Code (IaC) files
100: ‚îú‚îÄ‚îÄ private                     # Private files
101: ‚îú‚îÄ‚îÄ spikes                      # Spike projects (experimental or research work)
102: ‚îÇ ‚îî‚îÄ‚îÄ 202X-XX-XX-SPIKE-NAME     # Example spike project
103: ‚îÇ   ‚îú‚îÄ‚îÄasset1.xxx               # Asset related to the ADR
104: ‚îÇ   ‚îú‚îÄ‚îÄasset2.xxx               # Another asset related to the ADR
105: ‚îÇ   ‚îî‚îÄ‚îÄREADME.MD                # ADR documentation
106: ‚îú‚îÄ‚îÄ src                         # Source code       
107: ‚îÇ ‚îú‚îÄ‚îÄ dotnet                    # .NET source code
108: ‚îÇ ‚îî‚îÄ‚îÄ python                    # Python source code
109: ‚îî‚îÄ‚îÄ tests                       # Test files
110:     ‚îú‚îÄ‚îÄ dotnet                  # .NET test files
111:     ‚îî‚îÄ‚îÄ python                  # Python test files
112: 
113: ### Documentation Requirements
114: 
115:     - Maintain concise Markdown documentation.
116:     - Minimum docblock info: `param`, `return`, `throws`, `author`
117: 
118: ### Security Considerations
119: 
120:     - Sanitize all user inputs thoroughly.
121:     - Parameterize database queries.
122:     - Enforce strong Content Security Policies (CSP).
123:     - Use CSRF protection where applicable.
124:     - Ensure secure cookies (`HttpOnly`, `Secure`, `SameSite=Strict`).
125:     - Limit privileges and enforce role-based access control.
126:     - Implement detailed internal logging and monitoring.
</file>

<file path=".github/copilot-instructions.md">
1: ## COPILOT ADR CREATION OPERATIONAL GUIDELINES
2: 
3: Use the instructions in the <prompt-instructions>[adr.prompt.md](./prompts/adr.prompt.md)</prompt-instructions> file to create an ADR.
4: 
5: ## COPILOT GENERAL CODE CREATION EDITS AND REFACTORING OPERATIONAL GUIDELINES
6: 
7: Use the instructions in the <prompt-instructions>[code.prompt.md](./prompts/code.prompt.md)</prompt-instructions> file as general guidance for code generation.
</file>

<file path="docs/adrs/2025-06-11-tracklist-database.md">
  1: ---
  2: title: Tracklist Database for Vinyl and Digital Collections
  3: status: Proposed
  4: ---
  5: 
  6: ## Context
  7: 
  8: We aim to identify songs that are available on vinyl (from the `gazmazk4ez` collection) but not in digital form (from the `collection.nml` file). However, the `gazmazk4ez` collection contains release-level information, while the `collection.nml` file contains song-level information. This mismatch complicates direct comparisons.
  9: 
 10: Additionally, the collections are dynamic: new records may be added to the vinyl collection, and new songs may appear in the digital library. Therefore, the solution must handle updates efficiently.
 11: 
 12: ## Decision
 13: 
 14: We propose creating a local tracklist database to store detailed information about releases and their associated songs. This database will:
 15: 
 16: 1. **Fetch Tracklists**: Use the Discogs API to retrieve tracklists for all releases in the `gazmazk4ez` collection.
 17: 2. **Store Locally**: Save the tracklist data in a structured format (e.g., SQLite database or JSON/CSV file).
 18: 3. **Handle Updates**: Provide mechanisms to update the database when new records are added to the vinyl collection or new songs are added to the digital library.
 19: 
 20: ## Options Considered
 21: 
 22: ### Option 1: Fetch Tracklists on Demand
 23: - **Pros**:
 24:   - No need to maintain a local database.
 25:   - Always retrieves the latest data from Discogs.
 26: - **Cons**:
 27:   - Requires frequent API calls, which may hit rate limits.
 28:   - Slower performance due to network latency.
 29: 
 30: ### Option 2: Create a Local Database
 31: - **Pros**:
 32:   - Faster performance by avoiding repeated API calls.
 33:   - Can handle updates incrementally.
 34:   - Provides a single source of truth for tracklist data.
 35: - **Cons**:
 36:   - Requires initial setup and maintenance.
 37:   - May become outdated if not updated regularly.
 38: 
 39: ### Option 3: Hybrid Approach
 40: - **Description**: Use a local database but fetch data from Discogs on demand for missing or outdated entries.
 41: - **Pros**:
 42:   - Combines the benefits of both approaches.
 43:   - Ensures data is up-to-date while minimizing API calls.
 44: - **Cons**:
 45:   - More complex implementation.
 46: 
 47: ## Decision
 48: 
 49: We choose **Option 3: Hybrid Approach** as it balances performance and data freshness. The local database will store tracklist data, and the system will fetch updates from Discogs as needed.
 50: 
 51: ## Implementation Plan
 52: 
 53: 1. **Database Setup**:
 54:    - Use SQLite for the local database.
 55:    - Define tables for `Releases` and `Tracks`.
 56: 
 57: 2. **Initial Data Population**:
 58:    - Iterate over the `gazmazk4ez` collection.
 59:    - Fetch tracklists for each release using the Discogs API.
 60:    - Store the data in the database.
 61: 
 62: 3. **Update Mechanism**:
 63:    - Provide a function to add new releases to the database.
 64:    - Check for updates to existing releases (e.g., modified tracklists).
 65: 
 66: 4. **Comparison Logic**:
 67:    - Load tracklist data from the database.
 68:    - Compare with the `collection.nml` data to identify missing songs.
 69: 
 70: ## Database Structure
 71: 
 72: To efficiently manage tracklist data and limit API calls, we propose the following database structure:
 73: 
 74: ### 1. Releases Table
 75: - **Purpose**: Store metadata about each release in the `gazmazk4ez` collection.
 76: - **Columns**:
 77:   - `release_id` (Primary Key): Unique identifier for the release (from Discogs).
 78:   - `catalog_number`: Catalog number of the release.
 79:   - `title`: Title of the release.
 80:   - `artist`: Artist(s) associated with the release.
 81:   - `label`: Label of the release.
 82:   - `format`: Format of the release (e.g., vinyl).
 83:   - `year`: Year of the release.
 84:   - `last_processed`: Timestamp of the last time the release was processed.
 85:   - `tracklist_fetched` (Boolean): Indicates whether the tracklist has been fetched.
 86: 
 87: ### 2. Tracks Table
 88: - **Purpose**: Store detailed track information for each release.
 89: - **Columns**:
 90:   - `track_id` (Primary Key): Unique identifier for the track (auto-generated).
 91:   - `release_id` (Foreign Key): Links to the `Releases` table.
 92:   - `track_title`: Title of the track.
 93:   - `track_artist`: Artist(s) associated with the track.
 94:   - `position`: Position of the track on the release (e.g., A1, B2).
 95: 
 96: ### 3. Change Log Table (Optional)
 97: - **Purpose**: Track changes or updates to the database (e.g., new releases added, tracklists updated).
 98: - **Columns**:
 99:   - `log_id` (Primary Key): Unique identifier for the log entry.
100:   - `timestamp`: Timestamp of the change.
101:   - `action`: Description of the action (e.g., "Added release", "Updated tracklist").
102:   - `details`: Additional details about the change.
103: 
104: ## Workflow for Identifying Releases to Process
105: 
106: 1. **Check the `Releases` Table**:
107:    - Query for releases where `tracklist_fetched = FALSE` or `last_processed` is older than a certain threshold.
108: 
109: 2. **Fetch Tracklists**:
110:    - For each unprocessed release, fetch the tracklist using the Discogs API.
111:    - Update the `Releases` table to set `tracklist_fetched = TRUE` and update the `last_processed` timestamp.
112: 
113: 3. **Store Tracklists**:
114:    - Insert the tracklist data into the `Tracks` table, linked to the corresponding `release_id`.
115: 
116: 4. **Handle Updates**:
117:    - If a release is reprocessed (e.g., due to changes in the `gazmazk4ez` collection), update the `Tracks` table with the new tracklist.
118: 
119: ## Status
120: 
121: This ADR is currently in the **Proposed** state. Feedback and suggestions are welcome before implementation begins.
</file>

<file path="docs/adrs/index.md">
 1: ---
 2: title: ADRs
 3: nav-order: 2
 4: ---
 5: 
 6: ## ‚ú® Overview ‚ú®
 7: 
 8: All of our impactful decisions can be found here in the form of ADRs. üìã
 9: 
10: The acronym ADR is typically used for **A**rchitecture **D**ecision **R**ecords. üèóÔ∏è
11: 
12: We use the acronym more broadly and use it for decisions relating to architecture,
13: design, technology choices and other decisions that impact how we build the solution. üîß‚öôÔ∏è
14: 
15: When creating an ADR, please use the following naming convention for the files: üìù
16: 
17: > YYYY-MM-DD - [Brief Description] üìÖ
18: > For Example and ADR created on 2022.09.01 would be named
19: > _2022-09-01-example.md_ üìÑ
20: 
21: All related diagrams should be stored in the `.attachments`
22: folder in the root of [`docs`](..) and should have unique, yet identifiable names
23: referenced in the `adr` content. üìäüóÇÔ∏è
24: 
25: > ADR Name: 2022-09-01 - Example ADR with 2 diags: üìã<br/><br/>
26: > _2022-09-01-diag01.png_ üñºÔ∏è<br/>
27: > _2022-09-01-diag02.gif_ üé¨<br/>
28: 
29: <br/>If a tool like `PUML` is used, make use of the `.diags` folder in the root
30: of project to store the `PUML` files and ensure that the resulting images are
31: exported to the `.attachments` folder in the root of [`docs`](..). üõ†Ô∏èüìê
</file>

<file path="docs/features/README.md">
  1: # MusicTool - Feature Documentation
  2: 
  3: **MusicTool** is a comprehensive music collection management system that helps you analyze, organize, and optimize your digital and physical music collections. It provides powerful tools for gap analysis, duplicate detection, collection expansion, and data visualization.
  4: 
  5: ## üéµ Core Features Overview
  6: 
  7: ### 1. üè† Dashboard
  8: **Centralized overview of your entire music collection**
  9: 
 10: - **Collection Metrics**: Real-time statistics for both digital and physical collections
 11: - **Visual Analytics**: Interactive charts showing genre distribution, file formats, and collection trends
 12: - **Recent Activity**: Track new additions and collection changes
 13: - **Quick Access**: Jump to any feature from the main dashboard
 14: 
 15: **Key Metrics Displayed:**
 16: - Total digital tracks count
 17: - Physical collection size (releases and tracks)
 18: - Unique artists and albums
 19: - File format distribution
 20: - Genre breakdown
 21: 
 22: ---
 23: 
 24: ### 2. üíø Digital Collection Management
 25: **Comprehensive analysis of your digital music library**
 26: 
 27: **Data Sources:**
 28: - Native Instruments Traktor `.nml` files
 29: - Automatic metadata extraction
 30: - File format and quality analysis
 31: 
 32: **Features:**
 33: - **Interactive Tables**: Browse, search, and filter your entire digital collection
 34: - **Metadata Analysis**: View artist, title, album, genre, BPM, and technical details
 35: - **File Information**: Format types, bitrates, file sizes, and locations
 36: - **Export Capabilities**: Download filtered results as CSV
 37: - **Advanced Filtering**: By genre, artist, file format, or custom criteria
 38: 
 39: ---
 40: 
 41: ### 3. üìÄ Physical Collection Management
 42: **Discogs integration for vinyl and physical media tracking**
 43: 
 44: **Data Sources:**
 45: - Discogs collection CSV exports
 46: - Real-time API integration for detailed metadata
 47: - Automatic release-to-track expansion
 48: 
 49: **Features:**
 50: - **Collection Expansion**: Automatically expand Discogs releases into individual tracks
 51: - **Rich Metadata**: Artist, album, label, catalog numbers, release years
 52: - **Format Tracking**: Vinyl, CD, cassette, and other physical formats
 53: - **Progress Monitoring**: Real-time expansion progress with error handling
 54: - **Database Storage**: SQLite database for fast querying and analysis
 55: 
 56: **Expansion Process:**
 57: 1. Import Discogs collection CSV
 58: 2. Query Discogs API for detailed release information
 59: 3. Extract individual tracks from each release
 60: 4. Store in normalized database structure
 61: 5. Generate comprehensive track-level data
 62: 
 63: ---
 64: 
 65: ### 4. üîç Gap Analysis
 66: **Intelligent comparison between physical and digital collections**
 67: 
 68: **Purpose:** Identify tracks in your physical collection that are missing from your digital library.
 69: 
 70: **Technology:**
 71: - **Fuzzy String Matching**: Uses advanced algorithms to match tracks despite variations in naming
 72: - **Performance Optimized**: Fast indexing and candidate filtering for large collections
 73: - **Confidence Scoring**: Weighted similarity scores for accurate matching
 74: 
 75: **Analysis Methods:**
 76: - **Artist + Title Matching**: Primary matching algorithm (60% title, 30% artist, 10% combined)
 77: - **Confidence Thresholds**: Adjustable sensitivity (50-95%)
 78: - **Batch Processing**: Handles large collections efficiently
 79: 
 80: **Features:**
 81: - **Interactive Configuration**: Adjust confidence thresholds and analysis scope
 82: - **Sample Analysis**: Quick 100-track preview mode
 83: - **Progress Tracking**: Real-time progress bars and status updates
 84: - **Detailed Results**: Color-coded matches with confidence scores
 85: - **Export Options**: Download gap analysis results as CSV
 86: - **Match Quality**: Detailed scoring for artist, title, and combined similarities
 87: 
 88: **Results Dashboard:**
 89: - Found vs. Missing track counts and percentages
 90: - Average confidence scores
 91: - Detailed match information
 92: - Album-grouped missing tracks
 93: - Export capabilities for further analysis
 94: 
 95: ---
 96: 
 97: ### 5. üîÑ Duplicate Finder
 98: **Advanced duplicate detection for digital collections**
 99: 
100: **Purpose:** Identify and manage duplicate tracks in your digital library to save space and improve organization.
101: 
102: **Detection Methods:**
103: 1. **üéµ Artist + Title (Recommended)**: Most reliable for true duplicates
104: 2. **üéº Title Only**: Finds covers, remixes, and different versions
105: 3. **üìÅ Filename Similarity**: Detects similar file naming patterns
106: 4. **‚è±Ô∏è Duration + Title**: Combines length matching with title similarity
107: 
108: **Advanced Features:**
109: - **Configurable Similarity Thresholds**: 60-95% sensitivity control
110: - **Smart Normalization**: Removes common prefixes, suffixes, and variations
111: - **Performance Optimization**: Efficient algorithms for large collections
112: - **Quality Analysis**: File size, bitrate, and format comparison
113: 
114: **Results Analysis:**
115: - **Duplicate Groups**: Organized clusters of similar tracks
116: - **Space Savings**: Calculate potential storage reclamation
117: - **Quality Recommendations**: Identify highest quality versions
118: - **Statistical Overview**: Top duplicate artists, albums, and formats
119: - **Export Capabilities**: Download duplicate lists for manual review
120: 
121: **Visual Features:**
122: - Color-coded duplicate groups
123: - Interactive group browser
124: - Detailed file comparison tables
125: - Statistical charts and graphs
126: 
127: ---
128: 
129: ### 6. ‚öôÔ∏è Collection Tools
130: **Utility functions for collection management**
131: 
132: **Collection Expansion Tool:**
133: - **Batch Processing**: Expand multiple Discogs releases simultaneously
134: - **Progress Monitoring**: Real-time status updates and error handling
135: - **Configurable Limits**: Control processing scope for testing
136: - **Error Recovery**: Robust handling of API failures and network issues
137: 
138: **Configuration Management:**
139: - **API Key Validation**: Verify Discogs API connectivity
140: - **Database Status**: Monitor SQLite database health
141: - **Export Options**: Backup and restore collection data
142: - **System Information**: View processing statistics and performance metrics
143: 
144: ---
145: 
146: ## üèóÔ∏è Technical Architecture
147: 
148: ### Data Storage
149: - **SQLite Database**: Fast, local storage for physical collection data
150: - **In-Memory Processing**: Efficient handling of digital collection parsing
151: - **CSV Export/Import**: Universal data interchange format
152: 
153: ### Performance Optimizations
154: - **Indexed Search**: Pre-computed indexes for fast duplicate and gap detection
155: - **Batch Processing**: Efficient handling of large datasets
156: - **Progress Tracking**: User-friendly feedback for long-running operations
157: - **Memory Management**: Optimized for collections of any size
158: 
159: ### API Integration
160: - **Discogs API**: Official integration with rate limiting and error handling
161: - **Traktor NML**: Native parsing of Traktor library files
162: - **Extensible Design**: Easy addition of new data sources
163: 
164: ---
165: 
166: ## üéØ Use Cases
167: 
168: ### DJ Collection Management
169: - **Gap Analysis**: Ensure digital copies of all vinyl records
170: - **Duplicate Cleanup**: Maintain clean, organized digital libraries
171: - **Format Optimization**: Track file quality and formats across collections
172: 
173: ### Music Collectors
174: - **Collection Insights**: Understand collection composition and growth
175: - **Missing Track Identification**: Find gaps in digital archives
176: - **Space Optimization**: Identify and remove duplicate files
177: 
178: ### Data Analysis
179: - **Genre Analysis**: Understand musical preferences and trends
180: - **Collection Statistics**: Track collection growth and changes over time
181: - **Export Capabilities**: Use data in external analysis tools
182: 
183: ---
184: 
185: ## üöÄ Getting Started
186: 
187: ### Prerequisites
188: - Python 3.8+
189: - Discogs account and API key
190: - Traktor NML file (for digital collection)
191: - Discogs collection CSV export (for physical collection)
192: 
193: ### Quick Start
194: 1. **Configure API**: Add Discogs API key to `.env` file
195: 2. **Import Collections**: Load both digital (NML) and physical (CSV) data
196: 3. **Expand Physical**: Use collection tools to expand Discogs releases
197: 4. **Run Analysis**: Perform gap analysis and duplicate detection
198: 5. **Review Results**: Use interactive dashboard to explore findings
199: 6. **Export Data**: Download results for further analysis or action
200: 
201: ### Best Practices
202: - **Start Small**: Test with sample data before processing entire collections
203: - **Regular Backups**: Export results and maintain database backups
204: - **Quality Control**: Review duplicate suggestions before deletion
205: - **Incremental Updates**: Regularly update collections and re-run analysis
206: 
207: ---
208: 
209: ## üìä Performance Specifications
210: 
211: ### Gap Analysis
212: - **Speed**: ~10-50x faster than naive comparison algorithms
213: - **Accuracy**: 85%+ confidence threshold provides excellent precision
214: - **Scale**: Handles collections of 10,000+ tracks efficiently
215: 
216: ### Duplicate Detection
217: - **Algorithms**: Multiple fuzzy matching techniques for different use cases
218: - **Efficiency**: Optimized for collections of any size
219: - **Precision**: Configurable thresholds prevent false positives
220: 
221: ### Collection Expansion
222: - **API Efficiency**: Respects Discogs rate limits and provides retry logic
223: - **Data Quality**: Comprehensive metadata extraction and validation
224: - **Progress Tracking**: Real-time feedback for long-running operations
225: 
226: ---
227: 
228: ## üîß Configuration Options
229: 
230: ### Gap Analysis Settings
231: - **Confidence Threshold**: 50-95% (recommended: 80-85%)
232: - **Analysis Scope**: Sample, custom range, or complete collection
233: - **Matching Algorithms**: Weighted artist/title/combined scoring
234: 
235: ### Duplicate Detection Settings
236: - **Similarity Threshold**: 60-95% (recommended: 85%)
237: - **Detection Methods**: Four different algorithms for various use cases
238: - **Performance Limits**: Configurable batch sizes for large collections
239: 
240: ### Collection Expansion Settings
241: - **Batch Size**: Control number of releases processed simultaneously
242: - **Rate Limiting**: Respect Discogs API constraints
243: - **Error Handling**: Configurable retry logic and failure recovery
244: 
245: ---
246: 
247: ## üìà Future Enhancements
248: 
249: ### Planned Features
250: - **Automatic Quality Recommendations**: AI-powered duplicate resolution
251: - **Additional Data Sources**: Spotify, Apple Music, and other platform integration
252: - **Advanced Analytics**: Machine learning for collection insights
253: - **Collaborative Features**: Share and compare collections with other users
254: 
255: ### Integration Opportunities
256: - **Music Player Integration**: Direct playback and library management
257: - **Cloud Storage**: Sync collections across devices
258: - **Social Features**: Community recommendations and sharing
259: - **Marketplace Integration**: Price tracking and purchase recommendations
260: 
261: ---
262: 
263: *MusicTool is designed to be the ultimate solution for music collection management, combining powerful analysis capabilities with an intuitive user interface. Whether you're a DJ managing thousands of tracks or a collector organizing your vinyl collection, MusicTool provides the insights and tools you need to optimize your music library.*
</file>

<file path="docs/features/show-music-files.md">
1: # Show list of music files, including metadata
2: 
3: This feature will allow the user to see a list of all music files in their collection, along with relevant metadata such as artist, album, and track length. The user will be able to sort and filter the list based on different criteria.
4: The list will be generated from the Traktor Pro 3 library, available as NML file, which will be used as the primary source of music.
</file>

<file path="docs/how-to/collection-expansion.md">
  1: # Full Collection Expansion Guide
  2: 
  3: This guide explains how to run the full collection expansion to convert your Discogs physical collection CSV into a detailed track-by-track database.
  4: 
  5: ## Overview
  6: 
  7: The Collection Expander takes your Discogs CSV export (which contains releases/albums) and expands each release into individual tracks by fetching detailed tracklists from the Discogs API. This creates a comprehensive database of your physical music collection at the track level.
  8: 
  9: ## Prerequisites
 10: 
 11: ### 1. Discogs API Key
 12: You need a Discogs API key to fetch release details:
 13: 
 14: 1. Go to [Discogs Developer Settings](https://www.discogs.com/settings/developers)
 15: 2. Create a new application or use existing one
 16: 3. Copy your Personal Access Token
 17: 4. Add it to your `.env` file:
 18: 
 19: ```bash
 20: DISCOGS_API_KEY=your_api_key_here
 21: ```
 22: 
 23: ### 2. Discogs CSV Export
 24: Export your collection from Discogs:
 25: 
 26: 1. Go to [Your Collection](https://www.discogs.com/user/your_username/collection)
 27: 2. Click "Export" at the top right
 28: 3. Download the CSV file
 29: 4. Place it in the `./data/` folder
 30: 
 31: ### 3. Python Environment
 32: Ensure you have the required dependencies:
 33: 
 34: ```bash
 35: pip install -r requirements.txt
 36: ```
 37: 
 38: ## Running the Full Expansion
 39: 
 40: ### Quick Start
 41: 
 42: ```bash
 43: # Run the full expansion on all releases
 44: python3 run_full_expansion.py
 45: ```
 46: 
 47: ### What Happens
 48: 
 49: 1. **Prerequisite Check**: Verifies API key and CSV file exist
 50: 2. **Database Initialization**: Creates/connects to SQLite database
 51: 3. **Progress Assessment**: Shows current expansion status
 52: 4. **Idempotent Processing**: Skips already expanded releases
 53: 5. **API Fetching**: Retrieves tracklists at 1 request/second (rate limited)
 54: 6. **Progress Tracking**: Logs every 10 releases processed
 55: 7. **Error Handling**: Continues on errors, maintains progress
 56: 
 57: ### Expected Output
 58: 
 59: ```
 60: üéµ Starting full collection expansion
 61: üìù Logging to: logs/expansion_20250612_115735.log
 62: ‚úÖ All prerequisites met
 63: üìä Current database status:
 64:    ‚Üí 5 releases already expanded
 65:    ‚Üí 23 total tracks in database
 66: üöÄ Starting full collection expansion...
 67: üìÅ Source: ./data/gazmazk4ez-collection-20250608-1029.csv
 68: üóÑÔ∏è Database: ./data/musictool.db
 69: ‚öôÔ∏è Skip existing: True (idempotent)
 70: üîÑ Rate limiting: Enabled (1 req/sec)
 71: 
 72: Processing release 1/593: 1767
 73:   ‚Üí Digital - Deadline / Fix Up
 74:   ‚úÖ 2 tracks added and saved to database
 75: 
 76: Processing release 2/593: 5142
 77:   ‚Üí Nasty Habits - Liquid Fingers (Goldie Remix) / Deep Beats
 78:   ‚úÖ 2 tracks added and saved to database
 79: 
 80: üìä Progress: 10/593 releases processed (10 successful, 0 errors)
 81: ...
 82: ```
 83: 
 84: ## Monitoring Progress
 85: 
 86: ### Real-Time Monitor
 87: 
 88: Run the progress monitor in a separate terminal:
 89: 
 90: ```bash
 91: python3 monitor_expansion.py
 92: ```
 93: 
 94: This shows:
 95: - Current progress percentage
 96: - Processing rate (releases per minute)
 97: - Estimated completion time
 98: - Recently processed releases
 99: 
100: Example monitor output:
101: ```
102: üéµ MusicTool Collection Expansion Monitor
103: ==================================================
104: üìä Total releases to process: 598
105: ‚è∞ Started monitoring at: 11:58:13
106: 
107: üöÄ Progress:  44/598 (  7.4%) | Tracks:  128 | Rate: 40.0/min | ETA: 16:30:15
108: 
109: üìÄ Recently expanded:
110:    ‚Üí 33333: Bad Company - Rush Hour / Blind
111:    ‚Üí 13346: Bad Company - The Nine / Dogfight
112:    ‚Üí 108312: Rawhill Cru - Mo' Fire / Nitrous (Remixes)
113: ```
114: 
115: ### Manual Progress Check
116: 
117: Check database directly:
118: 
119: ```bash
120: python3 -c "
121: import sqlite3
122: import pandas as pd
123: conn = sqlite3.connect('./data/musictool.db')
124: tracks = pd.read_sql('SELECT * FROM expanded_tracks', conn)
125: releases = tracks['discogs_release_id'].nunique()
126: print(f'Progress: {releases} releases, {len(tracks)} tracks')
127: conn.close()
128: "
129: ```
130: 
131: ## Key Features
132: 
133: ### ‚úÖ Idempotent Operation
134: - **Crash Recovery**: If the process stops (network issue, interruption), simply restart it
135: - **Skip Existing**: Already processed releases are automatically skipped
136: - **Progress Persistence**: Each release is saved immediately to the database
137: - **No Data Loss**: Safe to stop and restart at any time
138: 
139: ### ‚úÖ Rate Limiting
140: - **API Respectful**: Limited to 1 request per second
141: - **No Throttling**: Prevents hitting Discogs API rate limits
142: - **Sustainable**: Can run for hours without issues
143: 
144: ### ‚úÖ Comprehensive Logging
145: - **File Logging**: Detailed logs saved to `./logs/expansion_TIMESTAMP.log`
146: - **Console Output**: Real-time progress in terminal
147: - **Error Tracking**: All errors logged but processing continues
148: - **Statistics**: Regular progress summaries
149: 
150: ## Expected Timeline
151: 
152: For a typical collection:
153: - **Small (100 releases)**: ~2-3 minutes
154: - **Medium (300 releases)**: ~5-6 minutes  
155: - **Large (600 releases)**: ~10-12 minutes
156: - **Very Large (1000+ releases)**: ~15-20 minutes
157: 
158: *Times may vary based on network speed and API response times*
159: 
160: ## Output Database
161: 
162: The expansion creates a SQLite database at `./data/musictool.db` with:
163: 
164: ### Table: `expanded_tracks`
165: ```sql
166: CREATE TABLE expanded_tracks (
167:     id INTEGER PRIMARY KEY,
168:     artist TEXT NOT NULL,
169:     title TEXT NOT NULL, 
170:     position TEXT,
171:     duration TEXT,
172:     album TEXT,
173:     album_artist TEXT,
174:     label TEXT,
175:     catalog_number TEXT,
176:     release_year INTEGER,
177:     format_type TEXT,
178:     format_description TEXT,
179:     collection_folder TEXT,
180:     date_added TEXT,
181:     media_condition TEXT,
182:     sleeve_condition TEXT,
183:     notes TEXT,
184:     discogs_release_id INTEGER,
185:     source TEXT,
186:     expanded_date TEXT,
187:     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
188: )
189: ```
190: 
191: ### Sample Data
192: ```
193: | artist              | title           | album           | discogs_release_id |
194: |--------------------|-----------------|-----------------|--------------------|
195: | A Guy Called Gerald | Voodoo Ray      | Essence         | 23143              |
196: | A Guy Called Gerald | Energy          | Essence         | 23143              |
197: | Bad Company         | Rush Hour       | Single          | 33333              |
198: | Matrix + Danny Jay  | Telepathy       | Single          | 239590             |
199: ```
200: 
201: ## Troubleshooting
202: 
203: ### Common Issues
204: 
205: #### Missing API Key
206: ```
207: ‚ùå DISCOGS_API_KEY not found in environment or .env file
208: ```
209: **Solution**: Add your Discogs API key to `.env` file
210: 
211: #### Missing CSV File  
212: ```
213: ‚ùå Discogs CSV not found: ./data/your-collection.csv
214: ```
215: **Solution**: Export and download your collection CSV from Discogs
216: 
217: #### Network/API Errors
218: ```
219: ‚ùå Error processing release 12345: HTTP 503 Service Unavailable
220: ```
221: **Solution**: The process will continue and retry. Temporary API issues are normal.
222: 
223: #### Database Lock Error
224: ```
225: ‚ùå Database is locked
226: ```
227: **Solution**: Ensure no other processes are accessing the database
228: 
229: ### Recovery from Interruption
230: 
231: If the expansion stops for any reason:
232: 
233: 1. **Check Current Progress**:
234:    ```bash
235:    python3 monitor_expansion.py
236:    ```
237: 
238: 2. **Simply Restart**:
239:    ```bash
240:    python3 run_full_expansion.py
241:    ```
242: 
243: 3. **It Will Resume**: The process automatically skips completed releases
244: 
245: ### Performance Optimization
246: 
247: For faster processing (at your own risk):
248: - Modify rate limiting in `core/discogs_client.py`
249: - Not recommended - may hit API limits
250: 
251: ## Integration with Other Tools
252: 
253: After expansion completes:
254: 
255: ### Gap Analysis
256: ```bash
257: python3 -m streamlit run src/python/ui/streamlit_app.py
258: ```
259: Navigate to "Gap Analysis" to find missing tracks
260: 
261: ### Export Data
262: Use the Streamlit UI "Tools" section to export:
263: - Physical collection CSV
264: - Gap analysis results  
265: - Collection statistics
266: 
267: ## Files Created
268: 
269: The expansion process creates:
270: - `./data/musictool.db` - Main database
271: - `./data/cache/` - API response cache
272: - `./logs/expansion_*.log` - Detailed logs
273: 
274: ## Next Steps
275: 
276: After expansion:
277: 1. **Review Results**: Check the Streamlit dashboard
278: 2. **Run Gap Analysis**: Find missing digital tracks
279: 3. **Export Reports**: Generate collection summaries
280: 4. **Explore Data**: Use the collection browsers
281: 
282: ---
283: 
284: *For technical details, see the source code in `src/python/core/collection_expander.py`*
</file>

<file path="docs/how-to/quick-reference.md">
 1: # Collection Expansion Quick Reference
 2: 
 3: ## Essential Commands
 4: 
 5: ### Run Full Expansion
 6: ```bash
 7: python3 run_full_expansion.py
 8: ```
 9: - Processes all releases in your Discogs CSV
10: - Idempotent (safe to restart if interrupted)
11: - Rate limited to 1 API request/second
12: - Logs progress to `logs/expansion_*.log`
13: 
14: ### Monitor Progress
15: ```bash
16: python3 monitor_expansion.py
17: ```
18: - Shows real-time expansion progress
19: - Displays processing rate and ETA
20: - Lists recently expanded releases
21: - Press Ctrl+C to stop monitoring
22: 
23: ### Check Current Status
24: ```bash
25: python3 -c "
26: import sqlite3, pandas as pd
27: conn = sqlite3.connect('./data/musictool.db')
28: df = pd.read_sql('SELECT * FROM expanded_tracks', conn)
29: releases = df['discogs_release_id'].nunique() if len(df) > 0 else 0
30: print(f'‚úÖ {releases} releases expanded')
31: print(f'üéµ {len(df)} total tracks')
32: conn.close()
33: "
34: ```
35: 
36: ## Prerequisites Checklist
37: 
38: - [ ] Discogs API key in `.env` file
39: - [ ] Discogs CSV export in `./data/` folder
40: - [ ] Python dependencies installed (`pip install -r requirements.txt`)
41: 
42: ## Timeline Estimates
43: 
44: | Collection Size | Estimated Time |
45: |----------------|----------------|
46: | 100 releases   | ~2-3 minutes   |
47: | 300 releases   | ~5-6 minutes   |
48: | 600 releases   | ~10-12 minutes |
49: | 1000+ releases | ~15-20 minutes |
50: 
51: ## Recovery
52: 
53: If expansion stops, simply restart:
54: ```bash
55: python3 run_full_expansion.py
56: ```
57: Already processed releases will be skipped automatically.
58: 
59: ## Next Steps
60: 
61: After expansion completes:
62: ```bash
63: # Launch the web interface
64: streamlit run src/python/ui/streamlit_app.py
65: 
66: # Navigate to "Gap Analysis" to find missing tracks
67: ```
</file>

<file path="docs/project-description.md">
 1: # Ideas for features of musictool
 2: 
 3: Looking to create a music management tool, based on my local collection. My
 4: collection consists of physical vinyl records, described in a discogs csv file,
 5: and my digital collection of mp3, flac, and aiff files.
 6: 
 7: 
 8: Multiple datasources:
 9: 
10: - NML file from Traktor Pro
11:     - this file should only be written to with explicit approval from user
12:     - we should keep a local copy, and think of a way to sync changes to our
13:       local copy
14:     - the source of truth
15: - CSV export file from discogs. Contains a record collection, listing _releases_
16:     - this collecting is not very consistent:
17:         - catalog number complexity
18:         - artist relation complexity
19:         - label duplication
20:         - format inconsistencies
21: - Discogs API, especially /search endpoint
22:     - API key available in /.env
23: 
24: 1. the export file can be extended to include all song information. The relation
25:    is that 1 release can have multiple songs. However, the discogs API could
26:    return multiple matches for 1 release - how do we solve this?
27: 
28: 1. I want to identify which songs there are on vinyl, but not in the digital
29:    collection. Can we create "ghost" entries in the local collection?
30: 
31: 1. I want to be able to edit metadata
32:     - manually
33:     - by looking up info from discogs
34:     - in batch (multiple songs together)
35: 
36: 1. ideally, we work on local copies of the files, and provide a mechanism to
37:    import new or updated collections, and export any changes back to the nml collection.
38: 
39: ## Open questions:
40: 
41: - if we update the NML file, do we still need to update the actual file tags as
42:   well? I assume we do.
</file>

<file path="docs/technical-implementation.md">
  1: # MusicTool - Technical Implementation Guide
  2: 
  3: This document provides detailed technical information about MusicTool's architecture, algorithms, and implementation details for developers and advanced users.
  4: 
  5: ## üèóÔ∏è System Architecture
  6: 
  7: ### High-Level Overview
  8: ```
  9: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 10: ‚îÇ   Streamlit UI  ‚îÇ    ‚îÇ  Core Modules   ‚îÇ    ‚îÇ  Data Sources   ‚îÇ
 11: ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
 12: ‚îÇ ‚Ä¢ Dashboard     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ NML Parser    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Traktor NML   ‚îÇ
 13: ‚îÇ ‚Ä¢ Gap Analysis  ‚îÇ    ‚îÇ ‚Ä¢ Gap Analyzer  ‚îÇ    ‚îÇ ‚Ä¢ Discogs API   ‚îÇ
 14: ‚îÇ ‚Ä¢ Duplicate     ‚îÇ    ‚îÇ ‚Ä¢ Duplicate     ‚îÇ    ‚îÇ ‚Ä¢ SQLite DB     ‚îÇ
 15: ‚îÇ   Finder        ‚îÇ    ‚îÇ   Finder        ‚îÇ    ‚îÇ ‚Ä¢ CSV Files     ‚îÇ
 16: ‚îÇ ‚Ä¢ Tools         ‚îÇ    ‚îÇ ‚Ä¢ Collection    ‚îÇ    ‚îÇ                 ‚îÇ
 17: ‚îÇ                 ‚îÇ    ‚îÇ   Expander      ‚îÇ    ‚îÇ                 ‚îÇ
 18: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 19: ```
 20: 
 21: ### Directory Structure
 22: ```
 23: src/python/
 24: ‚îú‚îÄ‚îÄ core/                          # Core business logic
 25: ‚îÇ   ‚îú‚îÄ‚îÄ nml_parser.py             # Traktor NML file parsing
 26: ‚îÇ   ‚îú‚îÄ‚îÄ discogs_client.py         # Discogs API integration
 27: ‚îÇ   ‚îú‚îÄ‚îÄ collection_expander.py    # Release-to-track expansion
 28: ‚îÇ   ‚îú‚îÄ‚îÄ gap_analyzer_fast.py      # Performance-optimized gap analysis
 29: ‚îÇ   ‚îî‚îÄ‚îÄ duplicate_finder.py       # Fuzzy duplicate detection
 30: ‚îú‚îÄ‚îÄ ui/
 31: ‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py          # Web interface
 32: ‚îî‚îÄ‚îÄ utils/                        # Utility functions
 33:     ‚îî‚îÄ‚îÄ database.py               # Database operations
 34: ```
 35: 
 36: ## üîç Algorithm Deep Dive
 37: 
 38: ### Gap Analysis Algorithm
 39: 
 40: #### Performance Optimization Strategy
 41: The original naive approach had O(n√óm) complexity where n = physical tracks and m = digital tracks. With 1,791 physical tracks and potentially 10,000+ digital tracks, this resulted in 17+ million comparisons.
 42: 
 43: **Optimization Techniques:**
 44: 
 45: 1. **Search Indexing**
 46: ```python
 47: # Pre-compute index by text prefixes
 48: index = {
 49:     "artist:abc": [track_ids],
 50:     "title:xyz": [track_ids], 
 51:     "combined:abcxyz": [track_ids]
 52: }
 53: ```
 54: 
 55: 2. **Candidate Filtering**
 56: ```python
 57: # Reduce search space by 90-95%
 58: candidates = get_candidates_from_index(track)
 59: # Limit to max 200 candidates per track
 60: candidates = candidates[:200]
 61: ```
 62: 
 63: 3. **Early Termination**
 64: ```python
 65: # Stop searching at 95%+ confidence
 66: if weighted_score >= 95:
 67:     break
 68: ```
 69: 
 70: 4. **Weighted Scoring Algorithm**
 71: ```python
 72: weighted_score = (title_score * 0.6) + (artist_score * 0.3) + (combined_score * 0.1)
 73: ```
 74: 
 75: **Performance Results:**
 76: - Before: O(n√óm) ‚âà 17,000,000 comparisons
 77: - After: O(n√ó200) ‚âà 358,000 comparisons  
 78: - **Speed Improvement: ~47x faster**
 79: 
 80: ### Duplicate Detection Algorithm
 81: 
 82: #### Multi-Method Approach
 83: 
 84: **1. Artist + Title Method (Recommended)**
 85: ```python
 86: artist_sim = fuzz.ratio(normalize(artist1), normalize(artist2))
 87: title_sim = fuzz.ratio(normalize(title1), normalize(title2))
 88: combined_sim = fuzz.ratio(f"{artist1} {title1}", f"{artist2} {title2}")
 89: 
 90: score = (title_sim * 0.5) + (artist_sim * 0.3) + (combined_sim * 0.2)
 91: ```
 92: 
 93: **2. Duration + Title Method**
 94: ```python
 95: title_sim = fuzz.ratio(normalize(title1), normalize(title2))
 96: duration_diff = abs(duration1 - duration2) / 1000  # seconds
 97: duration_sim = max(0, 100 - (duration_diff * 5))  # 5% penalty per second
 98: 
 99: score = (title_sim * 0.7) + (duration_sim * 0.3)
100: ```
101: 
102: **3. Text Normalization**
103: ```python
104: def normalize_text(text):
105:     text = text.lower()
106:     text = re.sub(r'\b(the|a|an)\b', '', text)           # Remove articles
107:     text = re.sub(r'\(.*?\)', '', text)                   # Remove parentheses
108:     text = re.sub(r'\[.*?\]', '', text)                   # Remove brackets
109:     text = re.sub(r'\s*-\s*(remix|edit|mix)\b.*', '', text)  # Remove versions
110:     text = re.sub(r'[^\w\s]', ' ', text)                  # Remove special chars
111:     text = re.sub(r'\s+', ' ', text).strip()              # Normalize whitespace
112:     return text
113: ```
114: 
115: ### Collection Expansion Algorithm
116: 
117: #### Discogs API Integration Strategy
118: 
119: **1. Rate Limiting & Retry Logic**
120: ```python
121: class RateLimitedClient:
122:     def __init__(self):
123:         self.rate_limiter = RateLimiter(max_calls=60, period=60)  # 60 calls/minute
124:         self.retry_config = ExponentialBackoff(max_retries=3)
125:     
126:     def get_release(self, release_id):
127:         with self.rate_limiter:
128:             return self._api_call_with_retry(release_id)
129: ```
130: 
131: **2. Data Extraction Pipeline**
132: ```python
133: def expand_release(release_data):
134:     tracks = []
135:     for track in release_data['tracklist']:
136:         normalized_track = {
137:             'artist': extract_artist(track, release_data),
138:             'title': clean_title(track['title']),
139:             'album': release_data['title'],
140:             'label': extract_label(release_data),
141:             'catalog_number': release_data.get('catno'),
142:             'release_year': extract_year(release_data),
143:             'format_type': extract_format(release_data),
144:             'discogs_release_id': release_data['id']
145:         }
146:         tracks.append(normalized_track)
147:     return tracks
148: ```
149: 
150: **3. Error Handling & Recovery**
151: ```python
152: def process_collection(csv_path, max_releases=None):
153:     results = {'success': 0, 'errors': 0, 'tracks': []}
154:     
155:     for release in get_releases(csv_path, max_releases):
156:         try:
157:             expanded_tracks = expand_release(release)
158:             results['tracks'].extend(expanded_tracks)
159:             results['success'] += 1
160:         except APIError as e:
161:             log_error(release['id'], e)
162:             results['errors'] += 1
163:             continue  # Continue with next release
164:     
165:     return results
166: ```
167: 
168: ## üíæ Data Models
169: 
170: ### Digital Collection Schema (NML)
171: ```python
172: @dataclass
173: class DigitalTrack:
174:     artist: str
175:     title: str
176:     album: str
177:     genre: str
178:     bpm: float
179:     totaltime: int          # milliseconds
180:     bitrate: int
181:     filetype: str
182:     filesize: int           # bytes
183:     location: str           # file path
184:     dateadded: datetime
185:     playcount: int
186: ```
187: 
188: ### Physical Collection Schema (Database)
189: ```sql
190: CREATE TABLE physical_tracks (
191:     id INTEGER PRIMARY KEY,
192:     artist TEXT NOT NULL,
193:     title TEXT NOT NULL,
194:     album TEXT NOT NULL,
195:     label TEXT,
196:     catalog_number TEXT,
197:     release_year INTEGER,
198:     format_type TEXT,
199:     discogs_release_id INTEGER,
200:     track_position TEXT,
201:     date_added TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
202:     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
203: );
204: 
205: CREATE INDEX idx_artist_title ON physical_tracks(artist, title);
206: CREATE INDEX idx_discogs_release ON physical_tracks(discogs_release_id);
207: CREATE INDEX idx_album ON physical_tracks(album);
208: ```
209: 
210: ### Gap Analysis Results Schema
211: ```python
212: @dataclass
213: class GapAnalysisResult:
214:     # Physical track info
215:     physical_artist: str
216:     physical_title: str
217:     physical_album: str
218:     physical_label: str
219:     physical_format: str
220:     physical_year: int
221:     
222:     # Match results
223:     status: str             # 'found' or 'missing'
224:     confidence: float       # 0-100
225:     status_reason: str
226:     
227:     # Best digital match (if found)
228:     digital_artist: str
229:     digital_title: str
230:     digital_album: str
231:     
232:     # Detailed similarity scores
233:     artist_score: float
234:     title_score: float
235:     combined_score: float
236: ```
237: 
238: ## üéõÔ∏è Configuration Management
239: 
240: ### Environment Variables
241: ```bash
242: # .env file
243: DISCOGS_API_KEY=your_discogs_api_key_here
244: DISCOGS_USER_AGENT=YourApp/1.0
245: DATABASE_PATH=./data/musictool.db
246: NML_PATH=./data/collection.nml
247: LOG_LEVEL=INFO
248: ```
249: 
250: ### Application Configuration
251: ```python
252: # config.py
253: @dataclass
254: class Config:
255:     # API settings
256:     discogs_api_key: str
257:     rate_limit_calls: int = 60
258:     rate_limit_period: int = 60
259:     
260:     # Performance settings
261:     gap_analysis_batch_size: int = 100
262:     max_candidates_per_track: int = 200
263:     duplicate_detection_threshold: int = 85
264:     
265:     # File paths
266:     database_path: str = "./data/musictool.db"
267:     nml_path: str = "./data/collection.nml"
268:     log_path: str = "./logs/musictool.log"
269: ```
270: 
271: ## üöÄ Performance Benchmarks
272: 
273: ### Gap Analysis Performance
274: | Collection Size | Original Time | Optimized Time | Speed Improvement |
275: |----------------|---------------|----------------|-------------------|
276: | 1,000 tracks   | 45 seconds    | 2 seconds      | 22.5x faster      |
277: | 5,000 tracks   | 12 minutes    | 15 seconds     | 48x faster        |
278: | 10,000 tracks  | 45 minutes    | 35 seconds     | 77x faster        |
279: 
280: ### Duplicate Detection Performance
281: | Collection Size | Processing Time | Memory Usage | Duplicates Found |
282: |----------------|----------------|--------------|------------------|
283: | 1,000 tracks   | 3 seconds      | 50 MB        | 15-25            |
284: | 5,000 tracks   | 35 seconds     | 150 MB       | 100-200          |
285: | 10,000 tracks  | 2.5 minutes    | 300 MB       | 300-500          |
286: 
287: ### Collection Expansion Performance
288: | Releases | API Calls | Time (60/min) | Success Rate |
289: |----------|-----------|---------------|--------------|
290: | 100      | 100       | 2 minutes     | 98%          |
291: | 500      | 500       | 9 minutes     | 97%          |
292: | 1,000    | 1,000     | 18 minutes    | 96%          |
293: 
294: ## üîß Development Setup
295: 
296: ### Development Environment
297: ```bash
298: # Create virtual environment
299: python -m venv venv
300: source venv/bin/activate  # Linux/Mac
301: # or
302: venv\Scripts\activate     # Windows
303: 
304: # Install dependencies
305: pip install -r requirements.txt
306: 
307: # Set up environment
308: cp .env.example .env
309: # Edit .env with your API keys
310: 
311: # Run development server
312: streamlit run src/python/ui/streamlit_app.py
313: ```
314: 
315: ### Testing Framework
316: ```python
317: # tests/test_gap_analyzer.py
318: import pytest
319: from src.python.core.gap_analyzer_fast import FastGapAnalyzer
320: 
321: class TestGapAnalyzer:
322:     def test_normalization(self):
323:         analyzer = FastGapAnalyzer("test_data/sample.nml")
324:         normalized = analyzer._normalize_text("The Beatles - Hey Jude (Remastered)")
325:         assert normalized == "beatles hey jude"
326:     
327:     def test_similarity_calculation(self):
328:         # Test fuzzy matching accuracy
329:         pass
330:     
331:     def test_performance_indexing(self):
332:         # Verify index creation and lookup speed
333:         pass
334: ```
335: 
336: ### Code Quality Standards
337: ```python
338: # Use type hints throughout
339: def find_gaps(self, confidence_threshold: int = 80) -> pd.DataFrame:
340:     pass
341: 
342: # Comprehensive error handling
343: try:
344:     result = api_call()
345: except RateLimitError:
346:     time.sleep(60)
347:     result = api_call()
348: except APIError as e:
349:     logger.error(f"API error: {e}")
350:     return None
351: 
352: # Performance monitoring
353: @performance_monitor
354: def expensive_operation():
355:     pass
356: ```
357: 
358: ## üìä Monitoring & Logging
359: 
360: ### Logging Configuration
361: ```python
362: import logging
363: 
364: logging.basicConfig(
365:     level=logging.INFO,
366:     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
367:     handlers=[
368:         logging.FileHandler('logs/musictool.log'),
369:         logging.StreamHandler()
370:     ]
371: )
372: 
373: logger = logging.getLogger(__name__)
374: ```
375: 
376: ### Performance Metrics
377: ```python
378: class PerformanceMonitor:
379:     def __init__(self):
380:         self.metrics = defaultdict(list)
381:     
382:     def track_operation(self, operation_name, duration, items_processed):
383:         self.metrics[operation_name].append({
384:             'duration': duration,
385:             'items': items_processed,
386:             'rate': items_processed / duration if duration > 0 else 0,
387:             'timestamp': datetime.now()
388:         })
389: ```
390: 
391: ## üîÑ API Integration Details
392: 
393: ### Discogs API Wrapper
394: ```python
395: class DiscogsAPIClient:
396:     def __init__(self, api_key: str, user_agent: str):
397:         self.client = discogs_client.Client(user_agent, user_token=api_key)
398:         self.rate_limiter = RateLimiter(60, 60)  # 60 calls per minute
399:     
400:     def get_release(self, release_id: int) -> dict:
401:         with self.rate_limiter:
402:             try:
403:                 release = self.client.release(release_id)
404:                 return self._serialize_release(release)
405:             except discogs_client.exceptions.HTTPError as e:
406:                 if e.status_code == 429:  # Rate limited
407:                     raise RateLimitError("Rate limit exceeded")
408:                 elif e.status_code == 404:  # Not found
409:                     raise NotFoundError(f"Release {release_id} not found")
410:                 else:
411:                     raise APIError(f"HTTP {e.status_code}: {e}")
412: ```
413: 
414: ### Error Recovery Strategies
415: ```python
416: def robust_api_call(func, max_retries=3, backoff_factor=2):
417:     for attempt in range(max_retries):
418:         try:
419:             return func()
420:         except RateLimitError:
421:             if attempt < max_retries - 1:
422:                 time.sleep(60)  # Wait for rate limit reset
423:                 continue
424:             raise
425:         except (ConnectionError, TimeoutError) as e:
426:             if attempt < max_retries - 1:
427:                 wait_time = backoff_factor ** attempt
428:                 time.sleep(wait_time)
429:                 continue
430:             raise NetworkError(f"Failed after {max_retries} attempts: {e}")
431: ```
432: 
433: This technical documentation provides the foundation for understanding, maintaining, and extending MusicTool's capabilities. The modular architecture and performance optimizations ensure the system can scale to handle large music collections efficiently.
</file>

<file path="docs/user-guide.md">
  1: # MusicTool - User Guide
  2: 
  3: Welcome to MusicTool! This comprehensive guide will help you get started with analyzing and managing your music collection using our powerful suite of tools.
  4: 
  5: ## üöÄ Quick Start Guide
  6: 
  7: ### Prerequisites
  8: Before you begin, make sure you have:
  9: - ‚úÖ Python 3.8 or higher installed
 10: - ‚úÖ A Discogs account with API access
 11: - ‚úÖ Your Traktor collection exported as `.nml` file
 12: - ‚úÖ Your Discogs collection exported as CSV
 13: 
 14: ### Step 1: Initial Setup
 15: 
 16: #### 1.1 Get Your Discogs API Key
 17: 1. Go to [Discogs Developer Settings](https://www.discogs.com/settings/developers)
 18: 2. Click "Generate new token"
 19: 3. Copy your personal access token
 20: 4. Save it securely - you'll need it for setup
 21: 
 22: #### 1.2 Export Your Collections
 23: 
 24: **Traktor Digital Collection:**
 25: 1. Open Native Instruments Traktor
 26: 2. Go to File ‚Üí Export Collection
 27: 3. Save as `collection.nml` in the `data/` folder
 28: 
 29: **Discogs Physical Collection:**
 30: 1. Log into your Discogs account
 31: 2. Go to Your Collection
 32: 3. Click "Export" and download CSV
 33: 4. Save the CSV file in the `data/` folder
 34: 
 35: #### 1.3 Configure MusicTool
 36: 1. Create a `.env` file in the project root
 37: 2. Add your Discogs API key:
 38: ```
 39: DISCOGS_API_KEY=your_api_key_here
 40: ```
 41: 
 42: ### Step 2: Launch MusicTool
 43: ```bash
 44: # Navigate to the project directory
 45: cd musictool
 46: 
 47: # Install dependencies (first time only)
 48: pip install -r requirements.txt
 49: 
 50: # Launch the application
 51: streamlit run src/python/ui/streamlit_app.py
 52: ```
 53: 
 54: The application will open in your browser at `http://localhost:8501`
 55: 
 56: ---
 57: 
 58: ## üì± User Interface Guide
 59: 
 60: ### Navigation
 61: The sidebar on the left contains the main navigation menu:
 62: - **üè† Dashboard**: Overview of your collections
 63: - **üíø Digital Collection**: Browse your digital tracks
 64: - **üìÄ Physical Collection**: Explore your physical releases
 65: - **üîç Gap Analysis**: Find missing digital tracks
 66: - **üîÑ Duplicate Finder**: Detect duplicate files
 67: - **‚öôÔ∏è Tools**: Collection management utilities
 68: 
 69: ---
 70: 
 71: ## üè† Dashboard Overview
 72: 
 73: The Dashboard provides a bird's-eye view of your entire music collection.
 74: 
 75: ### What You'll See:
 76: - **Collection Metrics**: Total tracks, artists, and releases
 77: - **Visual Charts**: Genre distribution and file format breakdown
 78: - **Recent Activity**: Latest additions to your collection
 79: - **Quick Stats**: Key performance indicators
 80: 
 81: ### Pro Tips:
 82: - Use the Dashboard to spot trends in your collection
 83: - Check file format distribution to ensure quality consistency
 84: - Monitor collection growth over time
 85: 
 86: ---
 87: 
 88: ## üíø Digital Collection Management
 89: 
 90: ### Features:
 91: - **Complete Track Listing**: Every track in your digital library
 92: - **Advanced Filtering**: Search by artist, title, genre, or format
 93: - **Sortable Columns**: Organize by any metadata field
 94: - **Export Options**: Download filtered results as CSV
 95: 
 96: ### How to Use:
 97: 1. **Browse**: Scroll through your complete collection
 98: 2. **Search**: Use the search box to find specific tracks
 99: 3. **Filter**: Click column headers to sort by different criteria
100: 4. **Export**: Download results for offline analysis
101: 
102: ### Understanding the Data:
103: - **Artist/Title/Album**: Basic track identification
104: - **Genre/BPM**: Musical characteristics
105: - **Format/Bitrate**: Technical file information
106: - **Duration**: Track length in minutes:seconds
107: - **File Location**: Where the track is stored
108: 
109: ---
110: 
111: ## üìÄ Physical Collection Management
112: 
113: ### Collection Expansion Process:
114: Your Discogs collection starts as a list of releases (albums). MusicTool expands these into individual tracks for detailed analysis.
115: 
116: ### Step-by-Step Expansion:
117: 
118: #### 1. Initial Import
119: - Your Discogs CSV contains basic release information
120: - Each row represents one album/release
121: - Limited track-level detail available
122: 
123: #### 2. Expansion Process
124: 1. **Go to Tools** ‚Üí Collection Expansion
125: 2. **Set Batch Size** (start with 10-50 for testing)
126: 3. **Click "Start Expansion"**
127: 4. **Monitor Progress** in real-time
128: 
129: #### 3. What Happens:
130: - MusicTool queries Discogs API for each release
131: - Extracts complete tracklist information
132: - Adds detailed metadata (label, catalog number, year)
133: - Stores everything in local database
134: 
135: #### 4. Progress Monitoring:
136: - **Real-time Updates**: See current processing status
137: - **Success/Error Counts**: Track expansion accuracy
138: - **ETA**: Estimated completion time
139: - **Batch Progress**: Current batch status
140: 
141: ### Expected Results:
142: - **Input**: 594 releases (example)
143: - **Output**: 1,791+ individual tracks
144: - **Success Rate**: 95-98% typical
145: - **Processing Time**: 2-3 tracks per second
146: 
147: ---
148: 
149: ## üîç Gap Analysis
150: 
151: Gap Analysis helps you identify tracks in your physical collection that are missing from your digital library.
152: 
153: ### Configuration Options:
154: 
155: #### Confidence Threshold (50-95%)
156: - **85% (Recommended)**: Good balance of accuracy and sensitivity
157: - **90%+**: Very strict, fewer false positives
158: - **80% or lower**: More sensitive, may include similar tracks
159: 
160: #### Analysis Modes:
161: - **üöÄ Fast Sample (100 tracks)**: Quick preview for testing
162: - **üîç Complete Analysis**: Process your entire collection
163: - **üéØ Custom Range**: Specify exact number of tracks
164: 
165: ### How It Works:
166: 
167: #### 1. Intelligent Matching
168: - **Fuzzy String Matching**: Handles variations in track names
169: - **Multi-Factor Scoring**: Considers artist, title, and combined similarity
170: - **Performance Optimized**: Uses indexing for speed
171: 
172: #### 2. Confidence Scoring
173: - **Title Weight**: 60% (most important)
174: - **Artist Weight**: 30% 
175: - **Combined Weight**: 10%
176: - **Final Score**: Weighted average of all factors
177: 
178: #### 3. Match Categories:
179: - **Found** (‚â• threshold): Track exists in digital collection
180: - **Missing** (< threshold): Track not found or low confidence match
181: 
182: ### Reading Results:
183: 
184: #### Summary Metrics:
185: - **Found Percentage**: How much of your physical collection is digitized
186: - **Missing Count**: Tracks you might want to acquire digitally
187: - **Average Confidence**: Overall matching quality
188: 
189: #### Detailed Results:
190: - **Color Coding**: Green = found, Red = missing
191: - **Confidence Scores**: Numerical similarity ratings
192: - **Match Details**: See exactly which digital track matched
193: 
194: ### Action Items:
195: 1. **Review Missing Tracks**: Identify acquisition targets
196: 2. **Check Low Confidence Matches**: Verify accuracy
197: 3. **Export Results**: Download for offline review
198: 4. **Prioritize by Album**: Focus on complete album gaps
199: 
200: ---
201: 
202: ## üîÑ Duplicate Finder
203: 
204: The Duplicate Finder helps clean up your digital collection by identifying potential duplicate tracks.
205: 
206: ### Detection Methods:
207: 
208: #### üéµ Artist + Title (Recommended)
209: - **Best For**: Finding true duplicates
210: - **How It Works**: Compares both artist and title with fuzzy matching
211: - **Accuracy**: Highest precision, lowest false positives
212: 
213: #### üéº Title Only
214: - **Best For**: Finding covers, remixes, different versions
215: - **How It Works**: Focuses only on track titles
216: - **Use Case**: Identify multiple versions of the same song
217: 
218: #### üìÅ Filename Similarity
219: - **Best For**: Files with similar naming patterns
220: - **How It Works**: Compares actual file names
221: - **Use Case**: Detect duplicate downloads with different metadata
222: 
223: #### ‚è±Ô∏è Duration + Title
224: - **Best For**: Identical recordings with different metadata
225: - **How It Works**: Combines title matching with track length
226: - **Use Case**: Same recording with different artist credits
227: 
228: ### Configuration:
229: 
230: #### Similarity Threshold (60-95%)
231: - **85% (Recommended)**: Good balance for most collections
232: - **90%+**: Very strict, only obvious duplicates
233: - **80% or lower**: More sensitive, requires careful review
234: 
235: ### Understanding Results:
236: 
237: #### Duplicate Groups:
238: - Each group contains potentially duplicate tracks
239: - **Original**: First track found (reference)
240: - **Duplicates**: Similar tracks with confidence scores
241: 
242: #### Quality Analysis:
243: - **File Size**: Larger files typically higher quality
244: - **Bitrate**: Higher bitrate = better audio quality
245: - **Format**: Lossless formats (FLAC) vs. compressed (MP3)
246: 
247: #### Space Savings:
248: - **Potential MB Saved**: Storage you could reclaim
249: - **Largest Groups**: Most duplicated tracks
250: - **Format Distribution**: Which file types have duplicates
251: 
252: ### Recommended Workflow:
253: 
254: #### 1. Start Conservative
255: - Use 85-90% threshold initially
256: - Review results carefully
257: - Verify before deleting anything
258: 
259: #### 2. Quality Priority
260: - Keep highest bitrate versions
261: - Prefer lossless formats (FLAC > WAV > MP3)
262: - Consider file size as quality indicator
263: 
264: #### 3. Manual Review
265: - **Export Results**: Download duplicate lists
266: - **Check Samples**: Listen to suspected duplicates
267: - **Batch Process**: Handle duplicates systematically
268: 
269: #### 4. Safety First
270: - **Backup Important Files**: Before deletion
271: - **Test on Small Sets**: Start with obvious duplicates
272: - **Keep Originals**: If unsure, keep both copies
273: 
274: ---
275: 
276: ## ‚öôÔ∏è Collection Tools
277: 
278: ### Expansion Tool
279: **Purpose**: Convert Discogs releases into individual tracks
280: 
281: #### Settings:
282: - **Max Releases**: Control batch size (start small)
283: - **Progress Monitor**: Real-time status updates
284: - **Error Handling**: Automatic retry for failed requests
285: 
286: #### Best Practices:
287: - Start with 10-20 releases to test
288: - Monitor API rate limits (60 requests/minute)
289: - Run expansion during off-peak hours
290: - Check error logs for problematic releases
291: 
292: ### Configuration Status
293: **Purpose**: Verify system setup and health
294: 
295: #### Checks:
296: - **API Key Status**: Verify Discogs connectivity
297: - **Database Health**: Confirm data integrity
298: - **File Locations**: Validate data file paths
299: - **System Performance**: Monitor processing speed
300: 
301: ---
302: 
303: ## üéØ Workflow Recommendations
304: 
305: ### For New Users:
306: 
307: #### Week 1: Setup & Exploration
308: 1. **Day 1**: Complete initial setup, launch Dashboard
309: 2. **Day 2**: Explore Digital and Physical collections
310: 3. **Day 3**: Run small expansion test (50 releases)
311: 4. **Day 4**: Try Gap Analysis on sample data
312: 5. **Day 5**: Test Duplicate Finder on subset
313: 
314: #### Week 2: Full Analysis
315: 1. **Expand Complete Collection**: Process all Discogs releases
316: 2. **Full Gap Analysis**: Identify all missing tracks
317: 3. **Comprehensive Duplicate Scan**: Clean up digital library
318: 4. **Export and Review**: Download results for planning
319: 
320: ### For DJ Collections:
321: 
322: #### Monthly Routine:
323: 1. **Update Collections**: Export latest Traktor and Discogs data
324: 2. **Run Gap Analysis**: Find new missing tracks
325: 3. **Duplicate Cleanup**: Maintain clean digital library
326: 4. **Acquisition Planning**: Prioritize missing track purchases
327: 
328: #### Pre-Gig Workflow:
329: 1. **Verify Track Quality**: Check for duplicates and low-quality files
330: 2. **Gap Check**: Ensure all planned tracks are digitized
331: 3. **Export Playlists**: Use analysis results for set planning
332: 
333: ### For Collectors:
334: 
335: #### Quarterly Reviews:
336: 1. **Collection Growth**: Monitor dashboard statistics
337: 2. **Genre Analysis**: Understand collection evolution
338: 3. **Missing Track Reports**: Plan acquisition strategies
339: 4. **Quality Audits**: Maintain high-standard digital library
340: 
341: ---
342: 
343: ## üîß Troubleshooting
344: 
345: ### Common Issues:
346: 
347: #### "No API Key Found"
348: - **Solution**: Check `.env` file exists with correct key
349: - **Verify**: Key should be on line: `DISCOGS_API_KEY=your_key`
350: 
351: #### "Gap Analysis Taking Too Long"
352: - **Solution**: Use Sample Mode first (100 tracks)
353: - **Check**: Large collections require patience
354: - **Optimize**: Start with smaller confidence threshold
355: 
356: #### "Expansion Errors"
357: - **Common Cause**: API rate limiting
358: - **Solution**: Reduce batch size, wait between retries
359: - **Check**: Error logs for specific problematic releases
360: 
361: #### "No Digital Collection Found"
362: - **Solution**: Verify `.nml` file in `data/` folder
363: - **Check**: File exported correctly from Traktor
364: - **Alternative**: Try re-exporting collection
365: 
366: ### Performance Tips:
367: 
368: #### For Large Collections (10,000+ tracks):
369: - Use Sample modes for initial testing
370: - Run analysis during off-peak hours
371: - Consider breaking into smaller batches
372: - Monitor system memory usage
373: 
374: #### For Better Accuracy:
375: - Clean metadata before import
376: - Use consistent naming conventions
377: - Verify file quality and completeness
378: - Regular database maintenance
379: 
380: ---
381: 
382: ## üìä Understanding Your Results
383: 
384: ### Gap Analysis Interpretation:
385: 
386: #### High Match Rates (85%+):
387: - **Excellent digitization coverage**
388: - Focus on remaining gaps
389: - Consider quality upgrades
390: 
391: #### Medium Match Rates (60-84%):
392: - **Significant digitization opportunity**
393: - Prioritize by artist/album
394: - Plan systematic acquisition
395: 
396: #### Low Match Rates (<60%):
397: - **Major digitization project needed**
398: - Start with favorite artists/albums
399: - Consider bulk acquisition strategies
400: 
401: ### Duplicate Analysis Interpretation:
402: 
403: #### High Duplicate Rates (>20%):
404: - **Significant cleanup opportunity**
405: - Focus on space savings first
406: - Systematic quality improvements
407: 
408: #### Medium Duplicate Rates (10-20%):
409: - **Normal for active collections**
410: - Target obvious duplicates first
411: - Quality-based decision making
412: 
413: #### Low Duplicate Rates (<10%):
414: - **Well-maintained collection**
415: - Focus on edge cases
416: - Preventive measures for future
417: 
418: ---
419: 
420: ## üéµ Best Practices
421: 
422: ### Collection Management:
423: - **Regular Updates**: Monthly analysis runs
424: - **Quality First**: Prioritize high-bitrate files
425: - **Systematic Approach**: Process in logical batches
426: - **Documentation**: Keep records of changes
427: 
428: ### Data Quality:
429: - **Consistent Naming**: Standardize artist/album names
430: - **Complete Metadata**: Fill in genre, year, label information
431: - **File Organization**: Logical folder structures
432: - **Backup Strategy**: Regular collection backups
433: 
434: ### Analysis Workflow:
435: - **Start Small**: Test with samples first
436: - **Verify Results**: Manual spot checks
437: - **Incremental Changes**: Gradual improvements
438: - **Track Progress**: Monitor improvement over time
439: 
440: MusicTool is designed to grow with your collection and adapt to your workflow. Start with the basics and gradually explore advanced features as you become more comfortable with the system.
</file>

<file path="docs/vnext-vision.md">
  1: # MusicTool vNext - Vision & Roadmap
  2: 
  3: **Transforming from Analysis Tool to Integrated Collection Management Platform**
  4: 
  5: *Date: June 16, 2025*  
  6: *Version: v2.0 Vision Document*
  7: 
  8: ---
  9: 
 10: ## üéØ **Vision Statement**
 11: 
 12: Transform MusicTool from a powerful analysis tool into a **comprehensive, real-time collection management platform** that seamlessly integrates with your existing workflow and provides actionable insights with direct purchasing/acquisition capabilities.
 13: 
 14: **Core Philosophy**: *"From discovery to action in one integrated workflow"*
 15: 
 16: ---
 17: 
 18: ## üöÄ **Major Feature Categories**
 19: 
 20: ### 1. **üîÑ Real-Time Data Integration**
 21: 
 22: #### **Direct Discogs API Integration**
 23: **Current State**: Static CSV import ‚Üí manual expansion ‚Üí periodic updates  
 24: **vNext Vision**: Live, on-demand synchronization
 25: 
 26: **Features:**
 27: - **üî¥ Live Collection Sync**: Real-time updates when you add/remove items on Discogs
 28: - **üì° Webhook Integration**: Automatic notifications of collection changes
 29: - **‚ö° Incremental Updates**: Only fetch new/changed releases
 30: - **üîÑ Smart Caching**: Local cache with TTL for performance
 31: - **üìä Collection Timeline**: Track changes over time with visual history
 32: 
 33: **Implementation Concepts:**
 34: ```python
 35: class LiveDiscogsSync:
 36:     def __init__(self):
 37:         self.webhook_handler = DiscogsWebhookHandler()
 38:         self.incremental_fetcher = IncrementalCollectionFetcher()
 39:     
 40:     def sync_on_demand(self):
 41:         """Fetch only new/updated releases since last sync"""
 42:         
 43:     def enable_real_time_sync(self):
 44:         """Enable webhook-based real-time updates"""
 45: ```
 46: 
 47: #### **Native Traktor Integration**
 48: **Current State**: Manual NML export ‚Üí file placement ‚Üí restart tool  
 49: **vNext Vision**: Direct integration with Traktor database
 50: 
 51: **Features:**
 52: - **üìÅ Auto-Discovery**: Automatically find and monitor Traktor installation
 53: - **üîÑ Live Sync**: Real-time updates as you add tracks to Traktor
 54: - **üìä Play History Integration**: Incorporate play counts and last played data
 55: - **üéõÔ∏è Playlist Analysis**: Analyze which vinyl tracks are in your digital playlists
 56: - **‚ö° Smart Refresh**: Only update changed tracks for performance
 57: 
 58: **Technical Approach:**
 59: ```python
 60: class TraktorIntegration:
 61:     def __init__(self):
 62:         self.traktor_db_path = self.discover_traktor_installation()
 63:         self.file_watcher = TraktorFileWatcher()
 64:     
 65:     def live_sync_collection(self):
 66:         """Monitor Traktor database for changes"""
 67:         
 68:     def sync_playlists(self):
 69:         """Import playlist data for enhanced analysis"""
 70: ```
 71: 
 72: ---
 73: 
 74: ### 2. **üõí Integrated Acquisition Workflow**
 75: 
 76: #### **Smart Purchase Links & Price Comparison**
 77: **Vision**: Transform gap analysis from "what's missing" to "where to buy it"
 78: 
 79: **Features:**
 80: - **üîó Universal Search Links**: Automatically generate search URLs for missing tracks
 81: - **üí∞ Price Comparison**: Real-time pricing across multiple platforms
 82: - **üéØ Smart Recommendations**: Suggest best format/quality options
 83: - **üìä Purchase Planning**: Budget planning and priority scoring
 84: - **üõí Wishlist Integration**: Direct integration with platform wishlists
 85: 
 86: **Platform Integrations:**
 87: ```python
 88: class AcquisitionEngine:
 89:     def __init__(self):
 90:         self.platforms = {
 91:             'beatport': BeatportAPI(),
 92:             'bandcamp': BandcampScraper(),
 93:             'juno': JunoDownloadAPI(),
 94:             'traxsource': TraxsourceAPI(),
 95:             'discogs_marketplace': DiscogsMarketplaceAPI()
 96:         }
 97:     
 98:     def find_purchase_options(self, track):
 99:         """Find track across all platforms with pricing"""
100:         
101:     def generate_smart_links(self, missing_tracks):
102:         """Create optimized search links for batch purchasing"""
103: ```
104: 
105: #### **Platform-Specific Features:**
106: 
107: **üéµ Beatport Integration:**
108: - Direct search links with artist + title pre-filled
109: - Genre and BPM filtering based on physical track data
110: - Label-based recommendations for similar releases
111: - Cart integration for batch purchases
112: 
113: **üé∏ Bandcamp Integration:**
114: - Artist page discovery for missing tracks
115: - Album completion suggestions
116: - Fan funding tracking for favorite artists
117: - FLAC quality prioritization
118: 
119: **üìÄ Discogs Marketplace:**
120: - Used digital release availability checking
121: - Price tracking for specific pressings
122: - Want list integration
123: - Seller reputation scoring
124: 
125: **üéõÔ∏è Juno/Traxsource (DJ-focused):**
126: - DJ-friendly format recommendations
127: - Extended mix availability
128: - Remix package detection
129: - DJ pool integration
130: 
131: ---
132: 
133: ### 3. **ü§ñ Intelligent Automation & AI**
134: 
135: #### **Smart Acquisition Suggestions**
136: **Vision**: AI-powered recommendations based on collection patterns
137: 
138: **Features:**
139: - **üß† Collection Pattern Analysis**: Understand your musical preferences
140: - **üìà Completion Scoring**: Prioritize which albums/artists to complete first
141: - **üí° Discovery Engine**: Suggest new releases based on your collection
142: - **üéØ Budget Optimization**: Maximize collection completion within budget constraints
143: - **üìä Trend Analysis**: Identify gaps in trending genres/artists
144: 
145: ```python
146: class IntelligentRecommendations:
147:     def __init__(self):
148:         self.ml_engine = CollectionAnalysisML()
149:         self.trend_analyzer = MusicTrendAnalyzer()
150:     
151:     def suggest_next_purchases(self, budget=None):
152:         """AI-powered purchase recommendations"""
153:         
154:     def analyze_collection_gaps(self):
155:         """Identify systematic gaps in collection"""
156: ```
157: 
158: #### **Automated Quality Management**
159: - **üîç Quality Scoring**: Automatically assess digital file quality
160: - **‚¨ÜÔ∏è Upgrade Suggestions**: Identify low-quality files for replacement
161: - **üéõÔ∏è Format Optimization**: Suggest best formats for your use case
162: - **üßπ Smart Cleanup**: AI-powered duplicate detection and resolution
163: 
164: ---
165: 
166: ### 4. **üì± Enhanced User Experience**
167: 
168: #### **Modern, Responsive Interface**
169: **Current State**: Streamlit web app  
170: **vNext Vision**: Native-feel web application with mobile support
171: 
172: **Features:**
173: - **üì± Mobile-First Design**: Full functionality on phones/tablets
174: - **üåô Dark/Light Modes**: Customizable themes
175: - **‚ö° Real-Time Updates**: Live notifications and progress tracking
176: - **üé® Customizable Dashboards**: Drag-and-drop dashboard creation
177: - **üîî Smart Notifications**: Alerts for new releases, price drops, etc.
178: 
179: #### **Advanced Analytics & Visualization**
180: - **üìà Collection Growth Tracking**: Visual timeline of collection development
181: - **üéµ Genre Evolution Analysis**: How your taste has changed over time
182: - **üí∞ Spending Analytics**: Track acquisition costs and ROI
183: - **üéØ Completion Metrics**: Progress toward collection goals
184: - **üìä Interactive Charts**: Drill-down analytics with filtering
185: 
186: ---
187: 
188: ### 5. **üåê Social & Community Features**
189: 
190: #### **Collection Sharing & Collaboration**
191: - **üë• Collection Comparison**: Compare with friends' collections
192: - **üéµ Shared Wishlists**: Collaborate on acquisition planning
193: - **üí° Community Recommendations**: Crowdsourced suggestions
194: - **üìä Benchmarking**: Compare your collection metrics to community averages
195: 
196: #### **Integration with Social Platforms**
197: - **üì± Last.fm Integration**: Incorporate listening history
198: - **üéµ Spotify/Apple Music**: Cross-platform gap analysis
199: - **üì∑ Instagram Integration**: Share collection highlights
200: - **üê¶ Twitter Integration**: Auto-tweet new acquisitions
201: 
202: ---
203: 
204: ### 6. **üè∑Ô∏è Intelligent Metadata Management**
205: 
206: #### **Comprehensive Metadata Enhancement**
207: **Current State**: Manual metadata cleanup in individual DJ software  
208: **vNext Vision**: Automated, intelligent metadata enhancement with cross-platform synchronization
209: 
210: **Core Philosophy**: *"Your metadata should be as clean and complete as your music collection"*
211: 
212: **Features:**
213: 
214: #### **üîç Smart Metadata Detection & Enhancement**
215: - **üß† AI-Powered Tag Completion**: Automatically fill missing genre, BPM, key, and energy data
216: - **üéµ Audio Analysis Integration**: Extract BPM, key, and energy from audio files
217: - **üìä Metadata Quality Scoring**: Rate completeness and accuracy of track metadata
218: - **üîÑ Batch Processing**: Apply metadata updates to thousands of tracks efficiently
219: - **üìù Source Attribution**: Track where metadata came from (user, API, audio analysis)
220: 
221: ```python
222: class IntelligentMetadataManager:
223:     def __init__(self):
224:         self.audio_analyzer = AudioAnalysisEngine()
225:         self.metadata_apis = {
226:             'musicbrainz': MusicBrainzAPI(),
227:             'acoustid': AcoustIDAPI(),
228:             'spotify': SpotifyWebAPI(),
229:             'lastfm': LastFmAPI()
230:         }
231:         self.ml_enhancer = MetadataMLEnhancer()
232:     
233:     def enhance_track_metadata(self, track):
234:         """Comprehensive metadata enhancement from multiple sources"""
235:         
236:     def analyze_audio_features(self, audio_file):
237:         """Extract BPM, key, energy, and other features from audio"""
238:         
239:     def suggest_metadata_improvements(self, collection):
240:         """Identify tracks with poor or missing metadata"""
241: ```
242: 
243: #### **üéõÔ∏è Dual-Format Synchronization**
244: **Vision**: Seamless bidirectional sync between NML collections and ID3 tags
245: 
246: **ID3 Tag Management:**
247: - **üìÅ Batch ID3 Updates**: Mass update ID3v2.4 tags across entire collection
248: - **üîÑ Bidirectional Sync**: Sync changes between Traktor NML and file tags
249: - **üé® Artwork Management**: Download and embed high-quality album artwork
250: - **üìè Tag Standardization**: Enforce consistent tag formats and naming conventions
251: - **üõ°Ô∏è Backup & Recovery**: Backup original tags before modifications
252: 
253: **NML Collection Management:**
254: - **üéõÔ∏è Traktor NML Direct Edit**: Modify NML database without Traktor restart
255: - **üîÑ Live Collection Updates**: Apply changes to running Traktor instance
256: - **üìä Collection Health Reports**: Identify inconsistencies and missing data
257: - **üîß Bulk Operations**: Mass updates across thousands of tracks
258: - **üìà Change Tracking**: History of all metadata modifications
259: 
260: ```python
261: class DualFormatMetadataSync:
262:     def __init__(self):
263:         self.id3_manager = ID3TagManager()
264:         self.nml_manager = TraktorNMLManager()
265:         self.sync_engine = MetadataSyncEngine()
266:     
267:     def sync_nml_to_id3(self, tracks):
268:         """Update file ID3 tags from NML collection data"""
269:         
270:     def sync_id3_to_nml(self, tracks):
271:         """Update NML collection from file ID3 tags"""
272:         
273:     def bidirectional_sync(self, collection):
274:         """Intelligent bidirectional synchronization with conflict resolution"""
275: ```
276: 
277: #### **üåê Multi-Source Metadata Enrichment**
278: **Vision**: Aggregate metadata from multiple authoritative sources for maximum accuracy
279: 
280: **Data Sources Integration:**
281: - **üéµ MusicBrainz**: Canonical music database for artist/album/track info
282: - **üîä AcoustID**: Audio fingerprinting for accurate track identification
283: - **üéº Spotify Web API**: Genre classifications and popularity metrics
284: - **üìª Last.fm**: User-generated tags and listening statistics
285: - **üíø Discogs**: Label information, catalog numbers, and release details
286: - **üéπ Beatport**: Electronic music genres, energy levels, and DJ-specific data
287: 
288: **Smart Metadata Resolution:**
289: - **ü§ñ Conflict Resolution**: AI-powered decision making when sources disagree
290: - **üéØ Confidence Scoring**: Rate reliability of metadata from different sources
291: - **üë§ User Preference Learning**: Adapt to user's metadata style preferences
292: - **üîÑ Periodic Re-enrichment**: Automatically update metadata as databases improve
293: - **üìä Quality Metrics**: Track metadata completeness and accuracy over time
294: 
295: #### **üé® Advanced Metadata Features**
296: 
297: **Visual Metadata Management:**
298: - **üñºÔ∏è Album Artwork**: High-resolution artwork download and embedding
299: - **üé® Artist Images**: Collection of artist photos and promotional images
300: - **üìä Visual Metadata Browser**: Grid view with artwork for easy navigation
301: - **üîç Artwork Quality Analysis**: Detect and upgrade low-resolution images
302: 
303: **DJ-Specific Enhancements:**
304: - **üéµ Harmonic Key Detection**: Accurate key detection for harmonic mixing
305: - **‚ö° Energy Level Analysis**: ML-powered energy rating for set planning
306: - **üéõÔ∏è Intro/Outro Marking**: Automatic detection of mix points
307: - **üìà BPM Accuracy**: High-precision BPM detection and verification
308: - **üè∑Ô∏è DJ Tags**: Custom categorization for mixing and performance
309: 
310: **Collection Intelligence:**
311: - **üìä Metadata Analytics**: Insights into collection completeness and quality
312: - **üîç Duplicate Detection**: Enhanced duplicate finding using metadata fingerprints
313: - **üìà Quality Trends**: Track metadata quality improvements over time
314: - **üéØ Enhancement Priorities**: Identify which tracks need metadata attention first
315: 
316: #### **üîß User Interface & Workflow**
317: 
318: **Streamlined Metadata Editor:**
319: - **üìù Bulk Edit Interface**: Excel-like editing for mass metadata updates
320: - **üéµ Audio Playback Integration**: Preview tracks while editing metadata
321: - **üîÑ Undo/Redo System**: Complete change tracking and rollback capabilities
322: - **üé® Visual Feedback**: Real-time preview of metadata changes
323: - **üìä Progress Tracking**: Visual progress bars for long-running operations
324: 
325: **Smart Workflows:**
326: - **üöÄ Quick Fix Mode**: One-click fixes for common metadata issues
327: - **üéØ Guided Enhancement**: Step-by-step workflow for metadata improvement
328: - **üîÑ Automated Routines**: Schedule regular metadata maintenance tasks
329: - **üìã Quality Checklists**: Ensure metadata meets your standards
330: - **üéµ Collection Themes**: Apply consistent metadata styles across genres
331: 
332: ```python
333: class MetadataWorkflowEngine:
334:     def __init__(self):
335:         self.editor = BulkMetadataEditor()
336:         self.validator = MetadataValidator()
337:         self.scheduler = AutomatedTaskScheduler()
338:     
339:     def quick_fix_collection(self, issues):
340:         """One-click fixes for common metadata problems"""
341:         
342:     def guided_enhancement_workflow(self, tracks):
343:         """Step-by-step metadata improvement process"""
344:         
345:     def schedule_maintenance(self, frequency, tasks):
346:         """Automated metadata maintenance scheduling"""
347: ```
348: 
349: #### **üìä Metadata Analytics & Reporting**
350: 
351: **Collection Health Dashboard:**
352: - **üìà Completeness Metrics**: Track percentage of complete metadata fields
353: - **üéØ Quality Scores**: Overall metadata quality rating
354: - **üîç Problem Detection**: Automatically identify metadata issues
355: - **üìä Improvement Trends**: Visualize metadata quality over time
356: - **üéµ Genre Distribution**: Analyze collection composition and gaps
357: 
358: **Advanced Analytics:**
359: - **ü§ñ ML-Powered Insights**: Discover patterns in your metadata preferences
360: - **üìä Comparative Analysis**: Benchmark against community metadata standards
361: - **üéØ Enhancement ROI**: Measure impact of metadata improvements on usability
362: - **üîç Source Reliability**: Track accuracy of different metadata sources
363: - **üìà Collection Evolution**: Visualize how your collection metadata has evolved
364: 
365: **Implementation Priority:**
366: This metadata management system would be integrated across **Phase 1** and **Phase 2** of the roadmap, as it provides foundational data quality that enhances all other features including gap analysis, duplicate detection, and acquisition workflows.
367: 
368: ---
369: 
370: ## üó∫Ô∏è **Implementation Roadmap**
371: 
372: ### **Phase 1: Foundation Enhancement (Q3 2025)**
373: *Duration: 3-4 months*
374: 
375: **üéØ Goal**: Establish robust real-time data foundation with intelligent metadata management
376: 
377: **Deliverables:**
378: - ‚úÖ **Direct Discogs API Integration**: Replace CSV workflow with live API
379: - ‚úÖ **Traktor Auto-Discovery**: Automatic detection and sync of Traktor collections
380: - ‚úÖ **Intelligent Metadata Management**: Dual-format sync (NML ‚Üî ID3) with enhancement
381: - ‚úÖ **Multi-Source Metadata Enrichment**: Integration with MusicBrainz, Spotify, Last.fm
382: - ‚úÖ **Incremental Updates**: Smart caching and delta synchronization
383: - ‚úÖ **Enhanced Error Handling**: Robust retry logic and offline mode
384: - ‚úÖ **Performance Optimization**: Sub-second response times for most operations
385: 
386: **Technical Milestones:**
387: ```
388: Week 1-2:   Discogs API wrapper with rate limiting and caching
389: Week 3-4:   Traktor database integration and file watching
390: Week 5-6:   ID3 tag management and NML bidirectional sync
391: Week 7-8:   Multi-source metadata enrichment APIs
392: Week 9-10:  Incremental sync algorithms and conflict resolution
393: Week 11-12: Performance optimization and comprehensive testing
394: ```
395: 
396: ### **Phase 2: Acquisition Integration + Metadata Enhancement (Q4 2025)**
397: *Duration: 3-4 months*
398: 
399: **üéØ Goal**: Transform from analysis to actionable acquisition workflow with automated metadata enhancement
400: 
401: **Deliverables:**
402: - ‚úÖ **Multi-Platform Search**: Beatport, Bandcamp, Juno, Traxsource integration
403: - ‚úÖ **Smart Link Generation**: One-click search across all platforms
404: - ‚úÖ **Price Comparison**: Real-time pricing and availability
405: - ‚úÖ **Purchase Planning**: Budget tracking and priority scoring
406: - ‚úÖ **Advanced Metadata Editor**: Bulk editing with audio analysis integration
407: - ‚úÖ **Automated Metadata Enhancement**: AI-powered tag completion and quality scoring
408: - ‚úÖ **Collection Health Dashboard**: Metadata analytics and reporting
409: - ‚úÖ **Wishlist Integration**: Platform-specific wishlist management
410: 
411: **Metadata Focus Areas:**
412: 1. **Audio Analysis Integration**: BPM, key, energy detection from audio files
413: 2. **Artwork Management**: High-resolution album art download and embedding  
414: 3. **DJ-Specific Enhancements**: Harmonic key detection, energy levels, mix points
415: 4. **Quality Assurance**: Metadata validation, duplicate detection, standardization
416: 
417: **Platform Priority:**
418: 1. **Beatport** (electronic music focus)
419: 2. **Bandcamp** (independent artists)
420: 3. **Juno Download** (comprehensive catalog)
421: 4. **Traxsource** (underground electronic)
422: 5. **Discogs Marketplace** (rare/vinyl)
423: 
424: ### **Phase 3: Intelligence & Automation (Q1 2026)**
425: *Duration: 3-4 months*
426: 
427: **üéØ Goal**: Add AI-powered insights and automated workflows
428: 
429: **Deliverables:**
430: - ‚úÖ **ML-Powered Recommendations**: Collection pattern analysis
431: - ‚úÖ **Automated Quality Management**: File quality assessment and upgrade suggestions
432: - ‚úÖ **Trend Integration**: Music trend analysis and recommendation
433: - ‚úÖ **Smart Notifications**: Automated alerts for new releases and price drops
434: - ‚úÖ **Workflow Automation**: Custom automation rules and triggers
435: 
436: ### **Phase 4: Platform & Community (Q2 2026)**
437: *Duration: 3-4 months*
438: 
439: **üéØ Goal**: Transform into comprehensive platform with social features
440: 
441: **Deliverables:**
442: - ‚úÖ **Modern UI/UX**: Complete interface redesign with mobile support
443: - ‚úÖ **Social Features**: Collection sharing and community recommendations
444: - ‚úÖ **Advanced Analytics**: Comprehensive collection insights and reporting
445: - ‚úÖ **API & Integrations**: Third-party integration capabilities
446: - ‚úÖ **Enterprise Features**: Multi-user support and advanced administration
447: 
448: ---
449: 
450: ## üèóÔ∏è **Technical Architecture vNext**
451: 
452: ### **Microservices Architecture**
453: ```
454: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
455: ‚îÇ   Frontend      ‚îÇ    ‚îÇ   API Gateway   ‚îÇ    ‚îÇ   Core Services ‚îÇ
456: ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
457: ‚îÇ ‚Ä¢ React/Next.js ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Authentication‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Collection    ‚îÇ
458: ‚îÇ ‚Ä¢ Mobile App    ‚îÇ    ‚îÇ ‚Ä¢ Rate Limiting ‚îÇ    ‚îÇ ‚Ä¢ Analysis      ‚îÇ
459: ‚îÇ ‚Ä¢ Desktop App   ‚îÇ    ‚îÇ ‚Ä¢ Load Balancer ‚îÇ    ‚îÇ ‚Ä¢ Acquisition   ‚îÇ
460: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ ‚Ä¢ Metadata Mgmt ‚îÇ
461:                                                ‚îÇ ‚Ä¢ Intelligence  ‚îÇ
462:                                                ‚îÇ ‚Ä¢ Notification  ‚îÇ
463:                                                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
464: 
465: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
466: ‚îÇ   Data Layer    ‚îÇ    ‚îÇ   External APIs ‚îÇ    ‚îÇ   Infrastructure‚îÇ
467: ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
468: ‚îÇ ‚Ä¢ PostgreSQL    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Discogs API   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Docker/K8s    ‚îÇ
469: ‚îÇ ‚Ä¢ Redis Cache   ‚îÇ    ‚îÇ ‚Ä¢ MusicBrainz   ‚îÇ    ‚îÇ ‚Ä¢ CI/CD         ‚îÇ
470: ‚îÇ ‚Ä¢ Vector DB     ‚îÇ    ‚îÇ ‚Ä¢ Platform APIs ‚îÇ    ‚îÇ ‚Ä¢ Monitoring    ‚îÇ
471: ‚îÇ ‚Ä¢ Audio Engine  ‚îÇ    ‚îÇ ‚Ä¢ ML Services   ‚îÇ    ‚îÇ ‚Ä¢ File Storage  ‚îÇ
472: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
473: ```
474: 
475: ### **Technology Stack Evolution**
476: 
477: **Frontend:**
478: - **Current**: Streamlit
479: - **vNext**: React/Next.js with TypeScript, React Native for mobile
480: 
481: **Backend:**
482: - **Current**: Python/Pandas
483: - **vNext**: FastAPI microservices, Celery for async processing
484: 
485: **Database:**
486: - **Current**: SQLite
487: - **vNext**: PostgreSQL with Redis caching, Vector DB for ML features
488: 
489: **Infrastructure:**
490: - **Current**: Local development
491: - **vNext**: Docker containers, Kubernetes deployment, CI/CD pipelines
492: 
493: ---
494: 
495: ## üí° **Innovation Opportunities**
496: 
497: ### **AI/ML Integration**
498: - **Music Recognition**: Audio fingerprinting for automatic matching
499: - **Preference Learning**: Personalized recommendation algorithms
500: - **Price Prediction**: ML-based price forecasting for optimal purchase timing
501: - **Collection Optimization**: AI-powered collection completion strategies
502: 
503: ### **Blockchain/Web3 Integration**
504: - **NFT Collection**: Integration with music NFT platforms
505: - **Decentralized Storage**: IPFS integration for collection metadata
506: - **Smart Contracts**: Automated purchase and licensing workflows
507: - **Community Governance**: DAO-based feature prioritization
508: 
509: ### **Advanced Analytics**
510: - **Listening Pattern Analysis**: Integration with streaming platforms
511: - **Genre Evolution Tracking**: Historical analysis of musical taste changes
512: - **Social Network Analysis**: Collection similarity clustering
513: - **Market Trend Analysis**: Predictive analytics for music market trends
514: 
515: ---
516: 
517: ## üìä **Success Metrics**
518: 
519: ### **User Engagement**
520: - **Daily Active Users**: Target 80% retention for weekly users
521: - **Session Duration**: Average 15+ minutes per session
522: - **Feature Adoption**: 90% of users using acquisition links within 30 days
523: - **Collection Completion**: Average 25% improvement in digitization rate
524: 
525: ### **Business Metrics**
526: - **Platform Partnerships**: Revenue sharing with 5+ music platforms
527: - **User Growth**: 1000% growth in user base over 18 months
528: - **Community Engagement**: 50% of users participating in social features
529: - **Platform Integration**: 10+ third-party integrations
530: 
531: ### **Technical Performance**
532: - **Response Time**: <500ms for 95% of API calls
533: - **Uptime**: 99.9% availability
534: - **Data Accuracy**: <1% error rate in collection synchronization
535: - **Mobile Performance**: <3s initial load time on mobile devices
536: 
537: ---
538: 
539: ## üé≠ **User Personas & Use Cases**
540: 
541: ### **DJ Dave** - *Professional DJ with 10,000+ digital tracks*
542: **Pain Points**: Keeping massive collection organized, finding high-quality downloads
543: **vNext Value**: 
544: - Real-time sync prevents outdated analysis
545: - Direct Beatport/Traxsource integration saves hours of searching
546: - Quality management ensures consistent sound in sets
547: 
548: ### **Collector Clara** - *Vinyl enthusiast with 500+ records*
549: **Pain Points**: Tracking which albums are digitized, discovering new releases
550: **vNext Value**:
551: - Live Discogs sync means never missing new acquisitions in analysis
552: - Bandcamp integration helps discover and support independent artists
553: - Completion scoring guides strategic purchasing decisions
554: 
555: ### **Library Linda** - *Music librarian managing institutional collection*
556: **Pain Points**: Data quality, comprehensive cataloging, budget planning
557: **vNext Value**:
558: - Automated quality management ensures consistent metadata
559: - Budget planning tools optimize acquisition strategies
560: - Analytics provide insights for collection development policies
561: 
562: ---
563: 
564: ## üîÆ **Future Vision (2027+)**
565: 
566: ### **The Ultimate Music Collection Platform**
567: - **Universal Music Identity**: Single interface for all music platforms and formats
568: - **AI Music Curator**: Fully automated collection management with minimal user input
569: - **Global Music Network**: Connect music lovers worldwide through their collections
570: - **Predictive Discovery**: AI predicts your next favorite artist before you know it
571: - **Seamless Experience**: From discovery to purchase to organization in one fluid workflow
572: 
573: **Vision Statement**: *"MusicTool becomes the central nervous system for all music collection activities, seamlessly connecting discovery, acquisition, organization, and sharing in one intelligent platform."*
574: 
575: ---
576: 
577: *This vision document represents our north star for MusicTool development. Each
578: phase builds upon the previous while maintaining backward compatibility and
579: user-focused design principles. The goal is not just to build features, but to
580: create a transformative experience for music collection management.*
</file>

<file path="spikes/detailed_music_collection.ipynb">
   1: {
   2:  "cells": [
   3:   {
   4:    "cell_type": "markdown",
   5:    "id": "44c04f07",
   6:    "metadata": {},
   7:    "source": [
   8:     "# Detailed Music Collection Analysis\n",
   9:     "\n",
  10:     "This notebook extracts detailed information from a Traktor collection.nml file and organizes it into a pandas DataFrame. The extracted information includes:\n",
  11:     "\n",
  12:     "- Artist\n",
  13:     "- Title\n",
  14:     "- Label\n",
  15:     "- Album/Release\n",
  16:     "- Release Date\n",
  17:     "- Comments\n",
  18:     "\n",
  19:     "The data is then organized and analyzed to provide insights into the music collection."
  20:    ]
  21:   },
  22:   {
  23:    "cell_type": "markdown",
  24:    "id": "5e3550fc",
  25:    "metadata": {},
  26:    "source": [
  27:     "## 1. Import Required Libraries\n",
  28:     "\n",
  29:     "In this section, we'll import the necessary libraries for working with XML data and creating DataFrames."
  30:    ]
  31:   },
  32:   {
  33:    "cell_type": "code",
  34:    "execution_count": 1,
  35:    "id": "1bd15fbb",
  36:    "metadata": {},
  37:    "outputs": [
  38:     {
  39:      "name": "stdout",
  40:      "output_type": "stream",
  41:      "text": [
  42:       "File exists: True\n",
  43:       "File size: 4.02 MB\n"
  44:      ]
  45:     }
  46:    ],
  47:    "source": [
  48:     "# Import required libraries\n",
  49:     "import xml.etree.ElementTree as ET\n",
  50:     "import pandas as pd\n",
  51:     "import matplotlib.pyplot as plt\n",
  52:     "import seaborn as sns\n",
  53:     "from datetime import datetime\n",
  54:     "import os\n",
  55:     "\n",
  56:     "# Set some display options for pandas\n",
  57:     "pd.set_option('display.max_columns', None)  # Show all columns\n",
  58:     "pd.set_option('display.max_rows', 20)       # Limit rows for better readability\n",
  59:     "pd.set_option('display.width', 1000)        # Wider display for better viewing\n",
  60:     "pd.set_option('display.colheader_justify', 'left')  # Left-align column headers\n",
  61:     "\n",
  62:     "# Check for the collection.nml file\n",
  63:     "file_path = '../data/collection.nml'\n",
  64:     "print(f\"File exists: {os.path.exists(file_path)}\")\n",
  65:     "print(f\"File size: {os.path.getsize(file_path) / (1024*1024):.2f} MB\")"
  66:    ]
  67:   },
  68:   {
  69:    "cell_type": "markdown",
  70:    "id": "a2f3e030",
  71:    "metadata": {},
  72:    "source": [
  73:     "## 2. Define Song Data Structure\n",
  74:     "\n",
  75:     "Before we parse the XML file, let's define the structure for our song data. We'll create a function to extract the detailed information from each entry in the collection."
  76:    ]
  77:   },
  78:   {
  79:    "cell_type": "code",
  80:    "execution_count": 2,
  81:    "id": "1c76eba7",
  82:    "metadata": {},
  83:    "outputs": [],
  84:    "source": [
  85:     "def extract_song_data(entry):\n",
  86:     "    \"\"\"\n",
  87:     "    Extract detailed song information from an XML entry element\n",
  88:     "    \n",
  89:     "    Parameters:\n",
  90:     "    entry (Element): XML Element representing a song entry\n",
  91:     "    \n",
  92:     "    Returns:\n",
  93:     "    dict: Dictionary containing the extracted song information\n",
  94:     "    \"\"\"\n",
  95:     "    # Initialize song data with empty values\n",
  96:     "    song_data = {\n",
  97:     "        'Artist': '',\n",
  98:     "        'Title': '',\n",
  99:     "        'Album': '',\n",
 100:     "        'Label': '',\n",
 101:     "        'Release Date': '',\n",
 102:     "        'Comment': '',\n",
 103:     "        'Genre': '',\n",
 104:     "        'BPM': '',\n",
 105:     "        'Key': '',\n",
 106:     "        'Play Count': 0,\n",
 107:     "        'File Path': '',\n",
 108:     "        'Duration': 0,\n",
 109:     "        'File Size': 0,\n",
 110:     "        'Last Played': ''\n",
 111:     "    }\n",
 112:     "    \n",
 113:     "    # Extract basic information from the entry attributes\n",
 114:     "    song_data['Artist'] = entry.get('ARTIST', '')\n",
 115:     "    song_data['Title'] = entry.get('TITLE', '')\n",
 116:     "    \n",
 117:     "    # Extract album information\n",
 118:     "    album_elem = entry.find('./ALBUM')\n",
 119:     "    if album_elem is not None:\n",
 120:     "        song_data['Album'] = album_elem.get('TITLE', '')\n",
 121:     "    \n",
 122:     "    # Extract detailed information from INFO element\n",
 123:     "    info_elem = entry.find('./INFO')\n",
 124:     "    if info_elem is not None:\n",
 125:     "        song_data['Label'] = info_elem.get('LABEL', '')\n",
 126:     "        song_data['Comment'] = info_elem.get('COMMENT', '')\n",
 127:     "        song_data['Genre'] = info_elem.get('GENRE', '')\n",
 128:     "        song_data['Key'] = info_elem.get('KEY', '')\n",
 129:     "        song_data['Play Count'] = int(info_elem.get('PLAYCOUNT', 0))\n",
 130:     "        song_data['Duration'] = float(info_elem.get('PLAYTIME', 0))\n",
 131:     "        song_data['File Size'] = float(info_elem.get('FILESIZE', 0))\n",
 132:     "        \n",
 133:     "        # Convert release date from string to datetime if available\n",
 134:     "        release_date = info_elem.get('RELEASE_DATE', '')\n",
 135:     "        if release_date:\n",
 136:     "            # Format is typically YYYY/M/D\n",
 137:     "            try:\n",
 138:     "                year, month, day = release_date.split('/')\n",
 139:     "                song_data['Release Date'] = f\"{year}-{month.zfill(2)}-{day.zfill(2)}\"\n",
 140:     "            except:\n",
 141:     "                song_data['Release Date'] = release_date\n",
 142:     "        \n",
 143:     "        # Get last played date if available\n",
 144:     "        last_played = info_elem.get('LAST_PLAYED', '')\n",
 145:     "        if last_played:\n",
 146:     "            song_data['Last Played'] = last_played\n",
 147:     "    \n",
 148:     "    # Extract BPM information\n",
 149:     "    tempo_elem = entry.find('./TEMPO')\n",
 150:     "    if tempo_elem is not None:\n",
 151:     "        song_data['BPM'] = tempo_elem.get('BPM', '')\n",
 152:     "    \n",
 153:     "    # Extract file path information\n",
 154:     "    location_elem = entry.find('./LOCATION')\n",
 155:     "    if location_elem is not None:\n",
 156:     "        dir_path = location_elem.get('DIR', '').replace('/', os.sep)\n",
 157:     "        file_name = location_elem.get('FILE', '')\n",
 158:     "        song_data['File Path'] = os.path.join(dir_path, file_name)\n",
 159:     "    \n",
 160:     "    return song_data\n",
 161:     "\n",
 162:     "# Create a simple function to check if any of the required fields are missing\n",
 163:     "def has_required_fields(song_data):\n",
 164:     "    \"\"\"\n",
 165:     "    Check if a song has the required fields\n",
 166:     "    \n",
 167:     "    Parameters:\n",
 168:     "    song_data (dict): Dictionary containing song information\n",
 169:     "    \n",
 170:     "    Returns:\n",
 171:     "    bool: True if all required fields are present, False otherwise\n",
 172:     "    \"\"\"\n",
 173:     "    required_fields = ['Artist', 'Title']\n",
 174:     "    return all(song_data[field] for field in required_fields)"
 175:    ]
 176:   },
 177:   {
 178:    "cell_type": "markdown",
 179:    "id": "95e8e02c",
 180:    "metadata": {},
 181:    "source": [
 182:     "## 3. Extract Song Information\n",
 183:     "\n",
 184:     "Now, let's parse the XML file and extract the song information using the functions we defined."
 185:    ]
 186:   },
 187:   {
 188:    "cell_type": "code",
 189:    "execution_count": 3,
 190:    "id": "f4f1bd21",
 191:    "metadata": {},
 192:    "outputs": [
 193:     {
 194:      "name": "stdout",
 195:      "output_type": "stream",
 196:      "text": [
 197:       "Successfully parsed XML file\n",
 198:       "Root tag: NML\n",
 199:       "Root attributes: {'VERSION': '19'}\n",
 200:       "Number of entries found: 4151\n",
 201:       "Successfully extracted data for 3003 songs\n",
 202:       "\n",
 203:       "Example of extracted song data:\n",
 204:       "Artist: aaliyah\n",
 205:       "Title: try again (a cappella)\n",
 206:       "Album: try again (vinyl single)\n",
 207:       "Label: \n",
 208:       "Release Date: \n",
 209:       "Comment: \n",
 210:       "Genre: A Cappella A capella\n",
 211:       "BPM: 96.212700\n",
 212:       "Key: 1m\n",
 213:       "Play Count: 0\n",
 214:       "File Path: /:Users/:roel4ez/:Music/:iTunes-Traktor/:music/:Music/:aaliyah/:try again (vinyl single)/:/try again (a cappella).mp3\n",
 215:       "Duration: 268.0\n",
 216:       "File Size: 6540.0\n",
 217:       "Last Played: \n"
 218:      ]
 219:     }
 220:    ],
 221:    "source": [
 222:     "# Parse the XML file\n",
 223:     "try:\n",
 224:     "    tree = ET.parse(file_path)\n",
 225:     "    root = tree.getroot()\n",
 226:     "    print(f\"Successfully parsed XML file\")\n",
 227:     "    print(f\"Root tag: {root.tag}\")\n",
 228:     "    print(f\"Root attributes: {root.attrib}\")\n",
 229:     "except Exception as e:\n",
 230:     "    print(f\"Error parsing XML file: {e}\")\n",
 231:     "    raise\n",
 232:     "\n",
 233:     "# Find all ENTRY elements\n",
 234:     "entries = root.findall('.//ENTRY')\n",
 235:     "print(f\"Number of entries found: {len(entries)}\")\n",
 236:     "\n",
 237:     "# Extract song data from each entry\n",
 238:     "songs = []\n",
 239:     "for entry in entries:\n",
 240:     "    song_data = extract_song_data(entry)\n",
 241:     "    if has_required_fields(song_data):\n",
 242:     "        songs.append(song_data)\n",
 243:     "\n",
 244:     "print(f\"Successfully extracted data for {len(songs)} songs\")\n",
 245:     "\n",
 246:     "# Display the first entry as an example\n",
 247:     "if songs:\n",
 248:     "    print(\"\\nExample of extracted song data:\")\n",
 249:     "    for key, value in songs[0].items():\n",
 250:     "        print(f\"{key}: {value}\")"
 251:    ]
 252:   },
 253:   {
 254:    "cell_type": "markdown",
 255:    "id": "9c8bc52b",
 256:    "metadata": {},
 257:    "source": [
 258:     "## 4. Create DataFrame\n",
 259:     "\n",
 260:     "Now, let's convert our list of song dictionaries into a pandas DataFrame for easier manipulation and analysis."
 261:    ]
 262:   },
 263:   {
 264:    "cell_type": "code",
 265:    "execution_count": 4,
 266:    "id": "6b053d19",
 267:    "metadata": {},
 268:    "outputs": [
 269:     {
 270:      "name": "stdout",
 271:      "output_type": "stream",
 272:      "text": [
 273:       "DataFrame shape: (3003, 14)\n",
 274:       "\n",
 275:       "DataFrame Information:\n",
 276:       "<class 'pandas.core.frame.DataFrame'>\n",
 277:       "RangeIndex: 3003 entries, 0 to 3002\n",
 278:       "Data columns (total 14 columns):\n",
 279:       " #   Column        Non-Null Count  Dtype         \n",
 280:       "---  ------        --------------  -----         \n",
 281:       " 0   Artist        3003 non-null   object        \n",
 282:       " 1   Title         3003 non-null   object        \n",
 283:       " 2   Album         3003 non-null   object        \n",
 284:       " 3   Label         3003 non-null   object        \n",
 285:       " 4   Release Date  2483 non-null   datetime64[ns]\n",
 286:       " 5   Comment       3003 non-null   object        \n",
 287:       " 6   Genre         3003 non-null   object        \n",
 288:       " 7   BPM           2828 non-null   float64       \n",
 289:       " 8   Key           3003 non-null   object        \n",
 290:       " 9   Play Count    3003 non-null   int64         \n",
 291:       " 10  File Path     3003 non-null   object        \n",
 292:       " 11  Duration      3003 non-null   float64       \n",
 293:       " 12  File Size     3003 non-null   float64       \n",
 294:       " 13  Last Played   1876 non-null   datetime64[ns]\n",
 295:       "dtypes: datetime64[ns](2), float64(3), int64(1), object(8)\n",
 296:       "memory usage: 328.6+ KB\n",
 297:       "\n",
 298:       "Numeric Column Statistics:\n",
 299:       "      Release Date                    BPM          Play Count   Duration     File Size     Last Played                   \n",
 300:       "count                           2483  2828.000000  3003.000000  3003.000000    3003.000000                           1876\n",
 301:       "mean   2011-04-24 16:58:22.859444224   170.713421     2.704296   349.296370   21881.610390  2023-04-06 21:52:34.797441536\n",
 302:       "min              1992-01-01 00:00:00    70.000038     0.000000     0.000000       0.000000            2017-02-05 00:00:00\n",
 303:       "25%              2003-01-01 00:00:00   172.000259     0.000000   299.000000   12316.000000            2021-05-14 18:00:00\n",
 304:       "50%              2013-01-01 00:00:00   173.999680     1.000000   353.000000   14452.000000            2024-10-04 00:00:00\n",
 305:       "75%              2019-01-01 00:00:00   174.963886     3.000000   400.000000   17514.000000            2025-03-11 00:00:00\n",
 306:       "max              2025-05-16 00:00:00   233.333649    68.000000  1119.000000  140708.000000            2025-06-08 00:00:00\n",
 307:       "std                              NaN    14.848389     4.638907    79.523318   18467.607933                            NaN\n"
 308:      ]
 309:     }
 310:    ],
 311:    "source": [
 312:     "# Create a DataFrame from the extracted song data\n",
 313:     "songs_df = pd.DataFrame(songs)\n",
 314:     "\n",
 315:     "# Check the shape of the DataFrame\n",
 316:     "print(f\"DataFrame shape: {songs_df.shape}\")\n",
 317:     "\n",
 318:     "# Convert numeric columns to appropriate types\n",
 319:     "numeric_columns = ['Play Count', 'Duration', 'File Size']\n",
 320:     "for col in numeric_columns:\n",
 321:     "    if col in songs_df.columns:\n",
 322:     "        songs_df[col] = pd.to_numeric(songs_df[col], errors='coerce')\n",
 323:     "\n",
 324:     "# Convert date columns to datetime type\n",
 325:     "date_columns = ['Release Date', 'Last Played']\n",
 326:     "for col in date_columns:\n",
 327:     "    if col in songs_df.columns:\n",
 328:     "        songs_df[col] = pd.to_datetime(songs_df[col], errors='coerce')\n",
 329:     "\n",
 330:     "# Convert BPM to float\n",
 331:     "if 'BPM' in songs_df.columns:\n",
 332:     "    songs_df['BPM'] = pd.to_numeric(songs_df['BPM'], errors='coerce')\n",
 333:     "\n",
 334:     "# Display DataFrame information\n",
 335:     "print(\"\\nDataFrame Information:\")\n",
 336:     "songs_df.info()\n",
 337:     "\n",
 338:     "# Display DataFrame column statistics\n",
 339:     "print(\"\\nNumeric Column Statistics:\")\n",
 340:     "print(songs_df.describe())"
 341:    ]
 342:   },
 343:   {
 344:    "cell_type": "markdown",
 345:    "id": "2ff908a8",
 346:    "metadata": {},
 347:    "source": [
 348:     "## 5. Display and Analyze Data\n",
 349:     "\n",
 350:     "Now that we have our data in a DataFrame, let's explore and analyze it."
 351:    ]
 352:   },
 353:   {
 354:    "cell_type": "code",
 355:    "execution_count": 5,
 356:    "id": "fd1d7d69",
 357:    "metadata": {},
 358:    "outputs": [
 359:     {
 360:      "name": "stdout",
 361:      "output_type": "stream",
 362:      "text": [
 363:       "First few rows of the DataFrame:\n"
 364:      ]
 365:     },
 366:     {
 367:      "data": {
 368:       "text/html": [
 369:        "<div>\n",
 370:        "<style scoped>\n",
 371:        "    .dataframe tbody tr th:only-of-type {\n",
 372:        "        vertical-align: middle;\n",
 373:        "    }\n",
 374:        "\n",
 375:        "    .dataframe tbody tr th {\n",
 376:        "        vertical-align: top;\n",
 377:        "    }\n",
 378:        "\n",
 379:        "    .dataframe thead th {\n",
 380:        "        text-align: right;\n",
 381:        "    }\n",
 382:        "</style>\n",
 383:        "<table border=\"1\" class=\"dataframe\">\n",
 384:        "  <thead>\n",
 385:        "    <tr style=\"text-align: left;\">\n",
 386:        "      <th></th>\n",
 387:        "      <th>Artist</th>\n",
 388:        "      <th>Title</th>\n",
 389:        "      <th>Album</th>\n",
 390:        "      <th>Label</th>\n",
 391:        "      <th>Release Date</th>\n",
 392:        "      <th>Comment</th>\n",
 393:        "      <th>Genre</th>\n",
 394:        "      <th>BPM</th>\n",
 395:        "      <th>Key</th>\n",
 396:        "      <th>Play Count</th>\n",
 397:        "      <th>File Path</th>\n",
 398:        "      <th>Duration</th>\n",
 399:        "      <th>File Size</th>\n",
 400:        "      <th>Last Played</th>\n",
 401:        "    </tr>\n",
 402:        "  </thead>\n",
 403:        "  <tbody>\n",
 404:        "    <tr>\n",
 405:        "      <th>0</th>\n",
 406:        "      <td>aaliyah</td>\n",
 407:        "      <td>try again (a cappella)</td>\n",
 408:        "      <td>try again (vinyl single)</td>\n",
 409:        "      <td></td>\n",
 410:        "      <td>NaT</td>\n",
 411:        "      <td></td>\n",
 412:        "      <td>A Cappella A capella</td>\n",
 413:        "      <td>96.212700</td>\n",
 414:        "      <td>1m</td>\n",
 415:        "      <td>0</td>\n",
 416:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 417:        "      <td>268.0</td>\n",
 418:        "      <td>6540.0</td>\n",
 419:        "      <td>NaT</td>\n",
 420:        "    </tr>\n",
 421:        "    <tr>\n",
 422:        "      <th>1</th>\n",
 423:        "      <td>Absolute Zero &amp; Subphonics</td>\n",
 424:        "      <td>The Code</td>\n",
 425:        "      <td>Hardware XV - History Of Hardware CD1</td>\n",
 426:        "      <td>Renegade Hardware</td>\n",
 427:        "      <td>1999-01-01</td>\n",
 428:        "      <td>Vinyl</td>\n",
 429:        "      <td>Drum &amp; Bass</td>\n",
 430:        "      <td>170.192902</td>\n",
 431:        "      <td>4m</td>\n",
 432:        "      <td>3</td>\n",
 433:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 434:        "      <td>436.0</td>\n",
 435:        "      <td>17016.0</td>\n",
 436:        "      <td>2025-03-21</td>\n",
 437:        "    </tr>\n",
 438:        "    <tr>\n",
 439:        "      <th>2</th>\n",
 440:        "      <td>Adam F</td>\n",
 441:        "      <td>Brand New Funk</td>\n",
 442:        "      <td>Drum &amp; Bass Arena - 20 Years of Drum N Bass 19...</td>\n",
 443:        "      <td></td>\n",
 444:        "      <td>2016-01-01</td>\n",
 445:        "      <td>Vinyl</td>\n",
 446:        "      <td>Drum &amp; Bass</td>\n",
 447:        "      <td>171.291626</td>\n",
 448:        "      <td>5m</td>\n",
 449:        "      <td>5</td>\n",
 450:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 451:        "      <td>466.0</td>\n",
 452:        "      <td>18216.0</td>\n",
 453:        "      <td>2021-02-27</td>\n",
 454:        "    </tr>\n",
 455:        "    <tr>\n",
 456:        "      <th>3</th>\n",
 457:        "      <td>Alix Perez</td>\n",
 458:        "      <td>Down The Line (feat. MC Fats)</td>\n",
 459:        "      <td>10 Years Of Shogun Audio</td>\n",
 460:        "      <td>Shogun Audio</td>\n",
 461:        "      <td>2014-01-01</td>\n",
 462:        "      <td></td>\n",
 463:        "      <td>Drum &amp; Bass</td>\n",
 464:        "      <td>171.996994</td>\n",
 465:        "      <td>10m</td>\n",
 466:        "      <td>4</td>\n",
 467:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 468:        "      <td>372.0</td>\n",
 469:        "      <td>14780.0</td>\n",
 470:        "      <td>2020-01-03</td>\n",
 471:        "    </tr>\n",
 472:        "    <tr>\n",
 473:        "      <th>4</th>\n",
 474:        "      <td>Alix Perez</td>\n",
 475:        "      <td>Fade Away</td>\n",
 476:        "      <td>1984</td>\n",
 477:        "      <td></td>\n",
 478:        "      <td>2009-01-01</td>\n",
 479:        "      <td></td>\n",
 480:        "      <td>Drum &amp; Bass</td>\n",
 481:        "      <td>172.999741</td>\n",
 482:        "      <td>1m</td>\n",
 483:        "      <td>6</td>\n",
 484:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 485:        "      <td>339.0</td>\n",
 486:        "      <td>13436.0</td>\n",
 487:        "      <td>2022-06-24</td>\n",
 488:        "    </tr>\n",
 489:        "  </tbody>\n",
 490:        "</table>\n",
 491:        "</div>"
 492:       ],
 493:       "text/plain": [
 494:        "  Artist                      Title                          Album                                              Label              Release Date Comment Genre                  BPM        Key   Play Count File Path                                           Duration  File Size Last Played\n",
 495:        "0                     aaliyah         try again (a cappella)                           try again (vinyl single)                           NaT            A Cappella A capella   96.212700   1m  0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  268.0      6540.0          NaT \n",
 496:        "1  Absolute Zero & Subphonics                       The Code              Hardware XV - History Of Hardware CD1  Renegade Hardware 1999-01-01    Vinyl            Drum & Bass  170.192902   4m  3           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  436.0     17016.0   2025-03-21 \n",
 497:        "2                      Adam F                 Brand New Funk  Drum & Bass Arena - 20 Years of Drum N Bass 19...                    2016-01-01    Vinyl            Drum & Bass  171.291626   5m  5           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  466.0     18216.0   2021-02-27 \n",
 498:        "3                  Alix Perez  Down The Line (feat. MC Fats)                           10 Years Of Shogun Audio       Shogun Audio 2014-01-01                     Drum & Bass  171.996994  10m  4           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  372.0     14780.0   2020-01-03 \n",
 499:        "4                  Alix Perez                      Fade Away                                               1984                    2009-01-01                     Drum & Bass  172.999741   1m  6           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  339.0     13436.0   2022-06-24 "
 500:       ]
 501:      },
 502:      "metadata": {},
 503:      "output_type": "display_data"
 504:     },
 505:     {
 506:      "name": "stdout",
 507:      "output_type": "stream",
 508:      "text": [
 509:       "\n",
 510:       "Last few rows of the DataFrame:\n"
 511:      ]
 512:     },
 513:     {
 514:      "data": {
 515:       "text/html": [
 516:        "<div>\n",
 517:        "<style scoped>\n",
 518:        "    .dataframe tbody tr th:only-of-type {\n",
 519:        "        vertical-align: middle;\n",
 520:        "    }\n",
 521:        "\n",
 522:        "    .dataframe tbody tr th {\n",
 523:        "        vertical-align: top;\n",
 524:        "    }\n",
 525:        "\n",
 526:        "    .dataframe thead th {\n",
 527:        "        text-align: right;\n",
 528:        "    }\n",
 529:        "</style>\n",
 530:        "<table border=\"1\" class=\"dataframe\">\n",
 531:        "  <thead>\n",
 532:        "    <tr style=\"text-align: left;\">\n",
 533:        "      <th></th>\n",
 534:        "      <th>Artist</th>\n",
 535:        "      <th>Title</th>\n",
 536:        "      <th>Album</th>\n",
 537:        "      <th>Label</th>\n",
 538:        "      <th>Release Date</th>\n",
 539:        "      <th>Comment</th>\n",
 540:        "      <th>Genre</th>\n",
 541:        "      <th>BPM</th>\n",
 542:        "      <th>Key</th>\n",
 543:        "      <th>Play Count</th>\n",
 544:        "      <th>File Path</th>\n",
 545:        "      <th>Duration</th>\n",
 546:        "      <th>File Size</th>\n",
 547:        "      <th>Last Played</th>\n",
 548:        "    </tr>\n",
 549:        "  </thead>\n",
 550:        "  <tbody>\n",
 551:        "    <tr>\n",
 552:        "      <th>2998</th>\n",
 553:        "      <td>Surge</td>\n",
 554:        "      <td>Cisco</td>\n",
 555:        "      <td>Through The Eyes</td>\n",
 556:        "      <td>Full Cycle</td>\n",
 557:        "      <td>2000-01-01</td>\n",
 558:        "      <td></td>\n",
 559:        "      <td>Drum &amp; Bass</td>\n",
 560:        "      <td>NaN</td>\n",
 561:        "      <td></td>\n",
 562:        "      <td>0</td>\n",
 563:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 564:        "      <td>347.0</td>\n",
 565:        "      <td>13636.0</td>\n",
 566:        "      <td>NaT</td>\n",
 567:        "    </tr>\n",
 568:        "    <tr>\n",
 569:        "      <th>2999</th>\n",
 570:        "      <td>Die</td>\n",
 571:        "      <td>Jitta Bug (Remix)</td>\n",
 572:        "      <td>Through The Eyes</td>\n",
 573:        "      <td>Full Cycle</td>\n",
 574:        "      <td>2000-01-01</td>\n",
 575:        "      <td></td>\n",
 576:        "      <td>Drum &amp; Bass</td>\n",
 577:        "      <td>NaN</td>\n",
 578:        "      <td></td>\n",
 579:        "      <td>0</td>\n",
 580:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 581:        "      <td>398.0</td>\n",
 582:        "      <td>15620.0</td>\n",
 583:        "      <td>NaT</td>\n",
 584:        "    </tr>\n",
 585:        "    <tr>\n",
 586:        "      <th>3000</th>\n",
 587:        "      <td>Suv</td>\n",
 588:        "      <td>Dark Angel</td>\n",
 589:        "      <td>Through The Eyes</td>\n",
 590:        "      <td>Full Cycle</td>\n",
 591:        "      <td>2000-01-01</td>\n",
 592:        "      <td></td>\n",
 593:        "      <td>Drum &amp; Bass</td>\n",
 594:        "      <td>NaN</td>\n",
 595:        "      <td></td>\n",
 596:        "      <td>0</td>\n",
 597:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 598:        "      <td>381.0</td>\n",
 599:        "      <td>14956.0</td>\n",
 600:        "      <td>NaT</td>\n",
 601:        "    </tr>\n",
 602:        "    <tr>\n",
 603:        "      <th>3001</th>\n",
 604:        "      <td>Roni Size</td>\n",
 605:        "      <td>Breaks</td>\n",
 606:        "      <td>Through The Eyes</td>\n",
 607:        "      <td>Full Cycle</td>\n",
 608:        "      <td>2000-01-01</td>\n",
 609:        "      <td></td>\n",
 610:        "      <td>Drum &amp; Bass</td>\n",
 611:        "      <td>NaN</td>\n",
 612:        "      <td></td>\n",
 613:        "      <td>0</td>\n",
 614:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 615:        "      <td>354.0</td>\n",
 616:        "      <td>13900.0</td>\n",
 617:        "      <td>NaT</td>\n",
 618:        "    </tr>\n",
 619:        "    <tr>\n",
 620:        "      <th>3002</th>\n",
 621:        "      <td>Calyx</td>\n",
 622:        "      <td>Follow The Leader (featuring Teebee) (ASHADOW3...</td>\n",
 623:        "      <td></td>\n",
 624:        "      <td>Moving Shadow</td>\n",
 625:        "      <td>NaT</td>\n",
 626:        "      <td>www.mediahuman.com</td>\n",
 627:        "      <td></td>\n",
 628:        "      <td>171.986618</td>\n",
 629:        "      <td>7m</td>\n",
 630:        "      <td>0</td>\n",
 631:        "      <td>/:Users/:roel4ez/:Music/:iTunes-Traktor/:music...</td>\n",
 632:        "      <td>384.0</td>\n",
 633:        "      <td>15208.0</td>\n",
 634:        "      <td>NaT</td>\n",
 635:        "    </tr>\n",
 636:        "  </tbody>\n",
 637:        "</table>\n",
 638:        "</div>"
 639:       ],
 640:       "text/plain": [
 641:        "     Artist     Title                                              Album             Label          Release Date Comment             Genre         BPM        Key  Play Count File Path                                           Duration  File Size Last Played\n",
 642:        "2998      Surge                                              Cisco  Through The Eyes     Full Cycle 2000-01-01                        Drum & Bass         NaN      0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  347.0     13636.0   NaT        \n",
 643:        "2999        Die                                  Jitta Bug (Remix)  Through The Eyes     Full Cycle 2000-01-01                        Drum & Bass         NaN      0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  398.0     15620.0   NaT        \n",
 644:        "3000        Suv                                         Dark Angel  Through The Eyes     Full Cycle 2000-01-01                        Drum & Bass         NaN      0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  381.0     14956.0   NaT        \n",
 645:        "3001  Roni Size                                             Breaks  Through The Eyes     Full Cycle 2000-01-01                        Drum & Bass         NaN      0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  354.0     13900.0   NaT        \n",
 646:        "3002      Calyx  Follow The Leader (featuring Teebee) (ASHADOW3...                    Moving Shadow        NaT    www.mediahuman.com               171.986618  7m  0           /:Users/:roel4ez/:Music/:iTunes-Traktor/:music...  384.0     15208.0   NaT        "
 647:       ]
 648:      },
 649:      "metadata": {},
 650:      "output_type": "display_data"
 651:     },
 652:     {
 653:      "name": "stdout",
 654:      "output_type": "stream",
 655:      "text": [
 656:       "\n",
 657:       "Missing values per column:\n",
 658:       "Release Date     520\n",
 659:       "BPM              175\n",
 660:       "Last Played     1127\n",
 661:       "dtype: int64\n",
 662:       "\n",
 663:       "Sample of core columns (Artist, Title, Album, Label, Release Date, Comment, Genre):\n"
 664:      ]
 665:     },
 666:     {
 667:      "data": {
 668:       "text/html": [
 669:        "<div>\n",
 670:        "<style scoped>\n",
 671:        "    .dataframe tbody tr th:only-of-type {\n",
 672:        "        vertical-align: middle;\n",
 673:        "    }\n",
 674:        "\n",
 675:        "    .dataframe tbody tr th {\n",
 676:        "        vertical-align: top;\n",
 677:        "    }\n",
 678:        "\n",
 679:        "    .dataframe thead th {\n",
 680:        "        text-align: right;\n",
 681:        "    }\n",
 682:        "</style>\n",
 683:        "<table border=\"1\" class=\"dataframe\">\n",
 684:        "  <thead>\n",
 685:        "    <tr style=\"text-align: left;\">\n",
 686:        "      <th></th>\n",
 687:        "      <th>Artist</th>\n",
 688:        "      <th>Title</th>\n",
 689:        "      <th>Album</th>\n",
 690:        "      <th>Label</th>\n",
 691:        "      <th>Release Date</th>\n",
 692:        "      <th>Comment</th>\n",
 693:        "      <th>Genre</th>\n",
 694:        "    </tr>\n",
 695:        "  </thead>\n",
 696:        "  <tbody>\n",
 697:        "    <tr>\n",
 698:        "      <th>0</th>\n",
 699:        "      <td>aaliyah</td>\n",
 700:        "      <td>try again (a cappella)</td>\n",
 701:        "      <td>try again (vinyl single)</td>\n",
 702:        "      <td></td>\n",
 703:        "      <td>NaT</td>\n",
 704:        "      <td></td>\n",
 705:        "      <td>A Cappella A capella</td>\n",
 706:        "    </tr>\n",
 707:        "    <tr>\n",
 708:        "      <th>1</th>\n",
 709:        "      <td>Absolute Zero &amp; Subphonics</td>\n",
 710:        "      <td>The Code</td>\n",
 711:        "      <td>Hardware XV - History Of Hardware CD1</td>\n",
 712:        "      <td>Renegade Hardware</td>\n",
 713:        "      <td>1999-01-01</td>\n",
 714:        "      <td>Vinyl</td>\n",
 715:        "      <td>Drum &amp; Bass</td>\n",
 716:        "    </tr>\n",
 717:        "    <tr>\n",
 718:        "      <th>2</th>\n",
 719:        "      <td>Adam F</td>\n",
 720:        "      <td>Brand New Funk</td>\n",
 721:        "      <td>Drum &amp; Bass Arena - 20 Years of Drum N Bass 19...</td>\n",
 722:        "      <td></td>\n",
 723:        "      <td>2016-01-01</td>\n",
 724:        "      <td>Vinyl</td>\n",
 725:        "      <td>Drum &amp; Bass</td>\n",
 726:        "    </tr>\n",
 727:        "    <tr>\n",
 728:        "      <th>3</th>\n",
 729:        "      <td>Alix Perez</td>\n",
 730:        "      <td>Down The Line (feat. MC Fats)</td>\n",
 731:        "      <td>10 Years Of Shogun Audio</td>\n",
 732:        "      <td>Shogun Audio</td>\n",
 733:        "      <td>2014-01-01</td>\n",
 734:        "      <td></td>\n",
 735:        "      <td>Drum &amp; Bass</td>\n",
 736:        "    </tr>\n",
 737:        "    <tr>\n",
 738:        "      <th>4</th>\n",
 739:        "      <td>Alix Perez</td>\n",
 740:        "      <td>Fade Away</td>\n",
 741:        "      <td>1984</td>\n",
 742:        "      <td></td>\n",
 743:        "      <td>2009-01-01</td>\n",
 744:        "      <td></td>\n",
 745:        "      <td>Drum &amp; Bass</td>\n",
 746:        "    </tr>\n",
 747:        "    <tr>\n",
 748:        "      <th>5</th>\n",
 749:        "      <td>Alix Perez</td>\n",
 750:        "      <td>Forsaken feat. Peven Everett &amp; SpectraSoul</td>\n",
 751:        "      <td>1984</td>\n",
 752:        "      <td></td>\n",
 753:        "      <td>2009-01-01</td>\n",
 754:        "      <td></td>\n",
 755:        "      <td>Drum &amp; Bass</td>\n",
 756:        "    </tr>\n",
 757:        "    <tr>\n",
 758:        "      <th>6</th>\n",
 759:        "      <td>Alix Perez</td>\n",
 760:        "      <td>Revolve-Her</td>\n",
 761:        "      <td>Recall and Reflect EP-(EXIT059)</td>\n",
 762:        "      <td></td>\n",
 763:        "      <td>2015-01-01</td>\n",
 764:        "      <td></td>\n",
 765:        "      <td>Drum &amp; Bass</td>\n",
 766:        "    </tr>\n",
 767:        "    <tr>\n",
 768:        "      <th>7</th>\n",
 769:        "      <td>Alix Perez</td>\n",
 770:        "      <td>Never Left</td>\n",
 771:        "      <td>Recall and Reflect EP-(EXIT059)</td>\n",
 772:        "      <td></td>\n",
 773:        "      <td>2015-01-01</td>\n",
 774:        "      <td></td>\n",
 775:        "      <td>Drum &amp; Bass</td>\n",
 776:        "    </tr>\n",
 777:        "    <tr>\n",
 778:        "      <th>8</th>\n",
 779:        "      <td>Alix Perez</td>\n",
 780:        "      <td>The Cut  Deepens ft. Foreign Beggars</td>\n",
 781:        "      <td></td>\n",
 782:        "      <td></td>\n",
 783:        "      <td>NaT</td>\n",
 784:        "      <td></td>\n",
 785:        "      <td>Drum &amp; Bass</td>\n",
 786:        "    </tr>\n",
 787:        "    <tr>\n",
 788:        "      <th>9</th>\n",
 789:        "      <td>Ancronix</td>\n",
 790:        "      <td>Skin it Back</td>\n",
 791:        "      <td>320 DUB</td>\n",
 792:        "      <td></td>\n",
 793:        "      <td>2004-01-01</td>\n",
 794:        "      <td>00000E1B 00000DDB 00004323 00003E79 0005093E 0...</td>\n",
 795:        "      <td>Drum &amp; Bass</td>\n",
 796:        "    </tr>\n",
 797:        "  </tbody>\n",
 798:        "</table>\n",
 799:        "</div>"
 800:       ],
 801:       "text/plain": [
 802:        "  Artist                      Title                                       Album                                              Label              Release Date Comment                                            Genre                \n",
 803:        "0                     aaliyah                      try again (a cappella)                           try again (vinyl single)                           NaT                                                       A Cappella A capella\n",
 804:        "1  Absolute Zero & Subphonics                                    The Code              Hardware XV - History Of Hardware CD1  Renegade Hardware 1999-01-01                                                Vinyl           Drum & Bass\n",
 805:        "2                      Adam F                              Brand New Funk  Drum & Bass Arena - 20 Years of Drum N Bass 19...                    2016-01-01                                                Vinyl           Drum & Bass\n",
 806:        "3                  Alix Perez               Down The Line (feat. MC Fats)                           10 Years Of Shogun Audio       Shogun Audio 2014-01-01                                                                Drum & Bass\n",
 807:        "4                  Alix Perez                                   Fade Away                                               1984                    2009-01-01                                                                Drum & Bass\n",
 808:        "5                  Alix Perez  Forsaken feat. Peven Everett & SpectraSoul                                               1984                    2009-01-01                                                                Drum & Bass\n",
 809:        "6                  Alix Perez                                 Revolve-Her                    Recall and Reflect EP-(EXIT059)                    2015-01-01                                                                Drum & Bass\n",
 810:        "7                  Alix Perez                                  Never Left                    Recall and Reflect EP-(EXIT059)                    2015-01-01                                                                Drum & Bass\n",
 811:        "8                  Alix Perez        The Cut  Deepens ft. Foreign Beggars                                                                              NaT                                                                Drum & Bass\n",
 812:        "9                    Ancronix                                Skin it Back                                            320 DUB                    2004-01-01    00000E1B 00000DDB 00004323 00003E79 0005093E 0...           Drum & Bass"
 813:       ]
 814:      },
 815:      "metadata": {},
 816:      "output_type": "display_data"
 817:     }
 818:    ],
 819:    "source": [
 820:     "# Display the first few rows of the DataFrame\n",
 821:     "print(\"First few rows of the DataFrame:\")\n",
 822:     "display(songs_df.head())\n",
 823:     "\n",
 824:     "# Display the last few rows of the DataFrame\n",
 825:     "print(\"\\nLast few rows of the DataFrame:\")\n",
 826:     "display(songs_df.tail())\n",
 827:     "\n",
 828:     "# Check for missing values\n",
 829:     "print(\"\\nMissing values per column:\")\n",
 830:     "missing_values = songs_df.isnull().sum()\n",
 831:     "print(missing_values[missing_values > 0])\n",
 832:     "\n",
 833:     "# Core information columns we're interested in\n",
 834:     "core_columns = ['Artist', 'Title', 'Album', 'Label', 'Release Date', 'Comment', 'Genre']\n",
 835:     "print(f\"\\nSample of core columns ({', '.join(core_columns)}):\")\n",
 836:     "display(songs_df[core_columns].head(10))"
 837:    ]
 838:   },
 839:   {
 840:    "cell_type": "code",
 841:    "execution_count": 8,
 842:    "id": "266c05cb",
 843:    "metadata": {},
 844:    "outputs": [
 845:     {
 846:      "name": "stdout",
 847:      "output_type": "stream",
 848:      "text": [
 849:       "Successfully loaded gazmazk4ez collection with 598 entries\n",
 850:       "\n",
 851:       "Columns in gazmazk4ez collection:\n",
 852:       "Index(['Catalog#', 'Artist', 'Title', 'Label', 'Format', 'Rating', 'Released', 'release_id', 'CollectionFolder', 'Date Added', 'Collection Media Condition', 'Collection Sleeve Condition', 'Collection Notes'], dtype='object')\n",
 853:       "\n",
 854:       "Updated DataFrame with Gazmazk4ez flag:\n"
 855:      ]
 856:     },
 857:     {
 858:      "data": {
 859:       "text/html": [
 860:        "<div>\n",
 861:        "<style scoped>\n",
 862:        "    .dataframe tbody tr th:only-of-type {\n",
 863:        "        vertical-align: middle;\n",
 864:        "    }\n",
 865:        "\n",
 866:        "    .dataframe tbody tr th {\n",
 867:        "        vertical-align: top;\n",
 868:        "    }\n",
 869:        "\n",
 870:        "    .dataframe thead th {\n",
 871:        "        text-align: right;\n",
 872:        "    }\n",
 873:        "</style>\n",
 874:        "<table border=\"1\" class=\"dataframe\">\n",
 875:        "  <thead>\n",
 876:        "    <tr style=\"text-align: left;\">\n",
 877:        "      <th></th>\n",
 878:        "      <th>Artist</th>\n",
 879:        "      <th>Title</th>\n",
 880:        "      <th>In Gazmazk4ez Collection</th>\n",
 881:        "    </tr>\n",
 882:        "  </thead>\n",
 883:        "  <tbody>\n",
 884:        "    <tr>\n",
 885:        "      <th>0</th>\n",
 886:        "      <td>aaliyah</td>\n",
 887:        "      <td>try again (a cappella)</td>\n",
 888:        "      <td>False</td>\n",
 889:        "    </tr>\n",
 890:        "    <tr>\n",
 891:        "      <th>1</th>\n",
 892:        "      <td>Absolute Zero &amp; Subphonics</td>\n",
 893:        "      <td>The Code</td>\n",
 894:        "      <td>False</td>\n",
 895:        "    </tr>\n",
 896:        "    <tr>\n",
 897:        "      <th>2</th>\n",
 898:        "      <td>Adam F</td>\n",
 899:        "      <td>Brand New Funk</td>\n",
 900:        "      <td>False</td>\n",
 901:        "    </tr>\n",
 902:        "    <tr>\n",
 903:        "      <th>3</th>\n",
 904:        "      <td>Alix Perez</td>\n",
 905:        "      <td>Down The Line (feat. MC Fats)</td>\n",
 906:        "      <td>False</td>\n",
 907:        "    </tr>\n",
 908:        "    <tr>\n",
 909:        "      <th>4</th>\n",
 910:        "      <td>Alix Perez</td>\n",
 911:        "      <td>Fade Away</td>\n",
 912:        "      <td>False</td>\n",
 913:        "    </tr>\n",
 914:        "    <tr>\n",
 915:        "      <th>5</th>\n",
 916:        "      <td>Alix Perez</td>\n",
 917:        "      <td>Forsaken feat. Peven Everett &amp; SpectraSoul</td>\n",
 918:        "      <td>False</td>\n",
 919:        "    </tr>\n",
 920:        "    <tr>\n",
 921:        "      <th>6</th>\n",
 922:        "      <td>Alix Perez</td>\n",
 923:        "      <td>Revolve-Her</td>\n",
 924:        "      <td>False</td>\n",
 925:        "    </tr>\n",
 926:        "    <tr>\n",
 927:        "      <th>7</th>\n",
 928:        "      <td>Alix Perez</td>\n",
 929:        "      <td>Never Left</td>\n",
 930:        "      <td>False</td>\n",
 931:        "    </tr>\n",
 932:        "    <tr>\n",
 933:        "      <th>8</th>\n",
 934:        "      <td>Alix Perez</td>\n",
 935:        "      <td>The Cut  Deepens ft. Foreign Beggars</td>\n",
 936:        "      <td>False</td>\n",
 937:        "    </tr>\n",
 938:        "    <tr>\n",
 939:        "      <th>9</th>\n",
 940:        "      <td>Ancronix</td>\n",
 941:        "      <td>Skin it Back</td>\n",
 942:        "      <td>False</td>\n",
 943:        "    </tr>\n",
 944:        "  </tbody>\n",
 945:        "</table>\n",
 946:        "</div>"
 947:       ],
 948:       "text/plain": [
 949:        "  Artist                      Title                                        In Gazmazk4ez Collection\n",
 950:        "0                     aaliyah                      try again (a cappella)  False                   \n",
 951:        "1  Absolute Zero & Subphonics                                    The Code  False                   \n",
 952:        "2                      Adam F                              Brand New Funk  False                   \n",
 953:        "3                  Alix Perez               Down The Line (feat. MC Fats)  False                   \n",
 954:        "4                  Alix Perez                                   Fade Away  False                   \n",
 955:        "5                  Alix Perez  Forsaken feat. Peven Everett & SpectraSoul  False                   \n",
 956:        "6                  Alix Perez                                 Revolve-Her  False                   \n",
 957:        "7                  Alix Perez                                  Never Left  False                   \n",
 958:        "8                  Alix Perez        The Cut  Deepens ft. Foreign Beggars  False                   \n",
 959:        "9                    Ancronix                                Skin it Back  False                   "
 960:       ]
 961:      },
 962:      "metadata": {},
 963:      "output_type": "display_data"
 964:     }
 965:    ],
 966:    "source": [
 967:     "# Load the gazmazk4ez collection\n",
 968:     "collection_path = '../data/gazmazk4ez-collection-20250608-1029.csv'\n",
 969:     "try:\n",
 970:     "    gazmazk4ez_df = pd.read_csv(collection_path)\n",
 971:     "    print(f\"Successfully loaded gazmazk4ez collection with {len(gazmazk4ez_df)} entries\")\n",
 972:     "except Exception as e:\n",
 973:     "    print(f\"Error loading gazmazk4ez collection: {e}\")\n",
 974:     "    raise\n",
 975:     "\n",
 976:     "# Inspect the gazmazk4ez collection\n",
 977:     "print(\"\\nColumns in gazmazk4ez collection:\")\n",
 978:     "print(gazmazk4ez_df.columns)\n",
 979:     "\n",
 980:     "# Add a flag to the songs DataFrame\n",
 981:     "songs_df['In Gazmazk4ez Collection'] = songs_df['Title'].isin(gazmazk4ez_df['Title'])\n",
 982:     "\n",
 983:     "# Display the updated DataFrame\n",
 984:     "print(\"\\nUpdated DataFrame with Gazmazk4ez flag:\")\n",
 985:     "display(songs_df[['Artist', 'Title', 'In Gazmazk4ez Collection']].head(10))"
 986:    ]
 987:   },
 988:   {
 989:    "cell_type": "code",
 990:    "execution_count": 10,
 991:    "id": "162a612d",
 992:    "metadata": {},
 993:    "outputs": [
 994:     {
 995:      "name": "stdout",
 996:      "output_type": "stream",
 997:      "text": [
 998:       "\n",
 999:       "Updated DataFrame with improved Gazmazk4ez flag:\n"
1000:      ]
1001:     },
1002:     {
1003:      "data": {
1004:       "text/html": [
1005:        "<div>\n",
1006:        "<style scoped>\n",
1007:        "    .dataframe tbody tr th:only-of-type {\n",
1008:        "        vertical-align: middle;\n",
1009:        "    }\n",
1010:        "\n",
1011:        "    .dataframe tbody tr th {\n",
1012:        "        vertical-align: top;\n",
1013:        "    }\n",
1014:        "\n",
1015:        "    .dataframe thead th {\n",
1016:        "        text-align: right;\n",
1017:        "    }\n",
1018:        "</style>\n",
1019:        "<table border=\"1\" class=\"dataframe\">\n",
1020:        "  <thead>\n",
1021:        "    <tr style=\"text-align: left;\">\n",
1022:        "      <th></th>\n",
1023:        "      <th>Artist</th>\n",
1024:        "      <th>Title</th>\n",
1025:        "      <th>In Gazmazk4ez Collection</th>\n",
1026:        "    </tr>\n",
1027:        "  </thead>\n",
1028:        "  <tbody>\n",
1029:        "    <tr>\n",
1030:        "      <th>0</th>\n",
1031:        "      <td>aaliyah</td>\n",
1032:        "      <td>try again (a cappella)</td>\n",
1033:        "      <td>False</td>\n",
1034:        "    </tr>\n",
1035:        "    <tr>\n",
1036:        "      <th>1</th>\n",
1037:        "      <td>Absolute Zero &amp; Subphonics</td>\n",
1038:        "      <td>The Code</td>\n",
1039:        "      <td>False</td>\n",
1040:        "    </tr>\n",
1041:        "    <tr>\n",
1042:        "      <th>2</th>\n",
1043:        "      <td>Adam F</td>\n",
1044:        "      <td>Brand New Funk</td>\n",
1045:        "      <td>False</td>\n",
1046:        "    </tr>\n",
1047:        "    <tr>\n",
1048:        "      <th>3</th>\n",
1049:        "      <td>Alix Perez</td>\n",
1050:        "      <td>Down The Line (feat. MC Fats)</td>\n",
1051:        "      <td>False</td>\n",
1052:        "    </tr>\n",
1053:        "    <tr>\n",
1054:        "      <th>4</th>\n",
1055:        "      <td>Alix Perez</td>\n",
1056:        "      <td>Fade Away</td>\n",
1057:        "      <td>False</td>\n",
1058:        "    </tr>\n",
1059:        "    <tr>\n",
1060:        "      <th>5</th>\n",
1061:        "      <td>Alix Perez</td>\n",
1062:        "      <td>Forsaken feat. Peven Everett &amp; SpectraSoul</td>\n",
1063:        "      <td>False</td>\n",
1064:        "    </tr>\n",
1065:        "    <tr>\n",
1066:        "      <th>6</th>\n",
1067:        "      <td>Alix Perez</td>\n",
1068:        "      <td>Revolve-Her</td>\n",
1069:        "      <td>False</td>\n",
1070:        "    </tr>\n",
1071:        "    <tr>\n",
1072:        "      <th>7</th>\n",
1073:        "      <td>Alix Perez</td>\n",
1074:        "      <td>Never Left</td>\n",
1075:        "      <td>False</td>\n",
1076:        "    </tr>\n",
1077:        "    <tr>\n",
1078:        "      <th>8</th>\n",
1079:        "      <td>Alix Perez</td>\n",
1080:        "      <td>The Cut  Deepens ft. Foreign Beggars</td>\n",
1081:        "      <td>False</td>\n",
1082:        "    </tr>\n",
1083:        "    <tr>\n",
1084:        "      <th>9</th>\n",
1085:        "      <td>Ancronix</td>\n",
1086:        "      <td>Skin it Back</td>\n",
1087:        "      <td>False</td>\n",
1088:        "    </tr>\n",
1089:        "  </tbody>\n",
1090:        "</table>\n",
1091:        "</div>"
1092:       ],
1093:       "text/plain": [
1094:        "  Artist                      Title                                        In Gazmazk4ez Collection\n",
1095:        "0                     aaliyah                      try again (a cappella)  False                   \n",
1096:        "1  Absolute Zero & Subphonics                                    The Code  False                   \n",
1097:        "2                      Adam F                              Brand New Funk  False                   \n",
1098:        "3                  Alix Perez               Down The Line (feat. MC Fats)  False                   \n",
1099:        "4                  Alix Perez                                   Fade Away  False                   \n",
1100:        "5                  Alix Perez  Forsaken feat. Peven Everett & SpectraSoul  False                   \n",
1101:        "6                  Alix Perez                                 Revolve-Her  False                   \n",
1102:        "7                  Alix Perez                                  Never Left  False                   \n",
1103:        "8                  Alix Perez        The Cut  Deepens ft. Foreign Beggars  False                   \n",
1104:        "9                    Ancronix                                Skin it Back  False                   "
1105:       ]
1106:      },
1107:      "metadata": {},
1108:      "output_type": "display_data"
1109:     }
1110:    ],
1111:    "source": [
1112:     "# Update the matching logic to include both Artist and Title\n",
1113:     "songs_df['In Gazmazk4ez Collection'] = songs_df.apply(\n",
1114:     "    lambda row: any(\n",
1115:     "        (row['Title'].strip().lower() == gaz_title.strip().lower() and\n",
1116:     "         row['Artist'].strip().lower() == gaz_artist.strip().lower())\n",
1117:     "        for gaz_title, gaz_artist in zip(gazmazk4ez_df['Title'], gazmazk4ez_df['Artist'])\n",
1118:     "    ), axis=1\n",
1119:     ")\n",
1120:     "\n",
1121:     "# Display the updated DataFrame\n",
1122:     "print(\"\\nUpdated DataFrame with improved Gazmazk4ez flag:\")\n",
1123:     "display(songs_df[['Artist', 'Title', 'In Gazmazk4ez Collection']].head(10))"
1124:    ]
1125:   },
1126:   {
1127:    "cell_type": "code",
1128:    "execution_count": 9,
1129:    "id": "8dd52624",
1130:    "metadata": {},
1131:    "outputs": [
1132:     {
1133:      "name": "stdout",
1134:      "output_type": "stream",
1135:      "text": [
1136:       "Number of songs in Gazmazk4ez collection: 24\n",
1137:       "Number of songs not in Gazmazk4ez collection: 2979\n",
1138:       "Percentage of songs in Gazmazk4ez collection: 0.80%\n"
1139:      ]
1140:     }
1141:    ],
1142:    "source": [
1143:     "# Analyze the number of songs in the gazmazk4ez collection\n",
1144:     "num_in_collection = songs_df['In Gazmazk4ez Collection'].sum()\n",
1145:     "num_not_in_collection = len(songs_df) - num_in_collection\n",
1146:     "\n",
1147:     "print(f\"Number of songs in Gazmazk4ez collection: {num_in_collection}\")\n",
1148:     "print(f\"Number of songs not in Gazmazk4ez collection: {num_not_in_collection}\")\n",
1149:     "\n",
1150:     "# Display percentage\n",
1151:     "percentage_in_collection = (num_in_collection / len(songs_df)) * 100\n",
1152:     "print(f\"Percentage of songs in Gazmazk4ez collection: {percentage_in_collection:.2f}%\")"
1153:    ]
1154:   },
1155:   {
1156:    "cell_type": "code",
1157:    "execution_count": 4,
1158:    "id": "e223606a",
1159:    "metadata": {},
1160:    "outputs": [
1161:     {
1162:      "name": "stdout",
1163:      "output_type": "stream",
1164:      "text": [
1165:       "Search results for 'Revolve-Her' by 'Alix Perez':\n",
1166:       "- Title: Alix Perez - Recall And Reflect EP, Year: 2015, Label: ['Exit Records'], Format: ['Vinyl', '12\"', 'Test Pressing'], Catalog#: EXIT059\n",
1167:       "- Title: Alix Perez - Recall And Reflect EP, Year: 2015, Label: ['Exit Records', 'Exit Records', 'Exit Records', 'Ten Eight Seven Mastering', 'Optimal Media GmbH'], Format: ['Vinyl', '12\"', '45 RPM', 'EP'], Catalog#: EXIT 059\n"
1168:      ]
1169:     }
1170:    ],
1171:    "source": [
1172:     "# Set up Discogs API integration\n",
1173:     "import os\n",
1174:     "import requests\n",
1175:     "from dotenv import load_dotenv\n",
1176:     "\n",
1177:     "# Load the API key from the .env file\n",
1178:     "load_dotenv()\n",
1179:     "DISCOGS_API_KEY = os.getenv('DISCOGS_API_KEY')\n",
1180:     "if not DISCOGS_API_KEY:\n",
1181:     "    raise ValueError(\"Discogs API key not found in .env file\")\n",
1182:     "\n",
1183:     "# Define a function to search for songs by name using the Discogs API\n",
1184:     "def search_song_on_discogs(song_name, artist_name=None):\n",
1185:     "    \"\"\"\n",
1186:     "    Search for a song on Discogs by name and optionally by artist.\n",
1187:     "\n",
1188:     "    Parameters:\n",
1189:     "    song_name (str): The name of the song to search for.\n",
1190:     "    artist_name (str): The name of the artist (optional).\n",
1191:     "\n",
1192:     "    Returns:\n",
1193:     "    dict: The JSON response from the Discogs API.\n",
1194:     "    \"\"\"\n",
1195:     "    base_url = \"https://api.discogs.com/database/search\"\n",
1196:     "    headers = {\n",
1197:     "        'Authorization': f'Discogs token={DISCOGS_API_KEY}'\n",
1198:     "    }\n",
1199:     "    params = {\n",
1200:     "        'track': song_name,\n",
1201:     "        'type': 'release',\n",
1202:     "        'format': 'vinyl',  # Filter by format (vinyl)\n",
1203:     "        'per_page': 5,\n",
1204:     "        'page': 1\n",
1205:     "    }\n",
1206:     "    if artist_name:\n",
1207:     "        params['artist'] = artist_name\n",
1208:     "\n",
1209:     "    response = requests.get(base_url, headers=headers, params=params)\n",
1210:     "    if response.status_code != 200:\n",
1211:     "        raise Exception(f\"Discogs API request failed: {response.status_code} {response.text}\")\n",
1212:     "\n",
1213:     "    return response.json()\n",
1214:     "\n",
1215:     "# Example usage\n",
1216:     "song_to_search = \"Revolve-Her\"\n",
1217:     "artist_to_search = \"Alix Perez\"\n",
1218:     "result = search_song_on_discogs(song_to_search, artist_to_search)\n",
1219:     "\n",
1220:     "# Display the search results\n",
1221:     "print(f\"Search results for '{song_to_search}' by '{artist_to_search}':\")\n",
1222:     "for release in result.get('results', []):\n",
1223:     "    print(f\"- Title: {release.get('title')}, Year: {release.get('year')}, Label: {release.get('label')}, Format: {release.get('format')}, Catalog#: {release.get('catno')}\")"
1224:    ]
1225:   }
1226:  ],
1227:  "metadata": {
1228:   "kernelspec": {
1229:    "display_name": "Python 3",
1230:    "language": "python",
1231:    "name": "python3"
1232:   },
1233:   "language_info": {
1234:    "codemirror_mode": {
1235:     "name": "ipython",
1236:     "version": 3
1237:    },
1238:    "file_extension": ".py",
1239:    "mimetype": "text/x-python",
1240:    "name": "python",
1241:    "nbconvert_exporter": "python",
1242:    "pygments_lexer": "ipython3",
1243:    "version": "3.10.12"
1244:   }
1245:  },
1246:  "nbformat": 4,
1247:  "nbformat_minor": 5
1248: }
</file>

<file path="spikes/spike001.ipynb">
  1: {
  2:  "cells": [
  3:   {
  4:    "cell_type": "code",
  5:    "execution_count": 1,
  6:    "id": "4a1a395a",
  7:    "metadata": {},
  8:    "outputs": [
  9:     {
 10:      "name": "stdout",
 11:      "output_type": "stream",
 12:      "text": [
 13:       "Root tag: NML\n",
 14:       "Root attributes: {'VERSION': '19'}\n",
 15:       "Child tags: ['HEAD', 'MUSICFOLDERS', 'COLLECTION', 'SETS', 'PLAYLISTS', 'INDEXING']\n",
 16:       "Number of entries found: 4151\n",
 17:       "\n",
 18:       "First entry attributes: {'MODIFIED_DATE': '2024/12/11', 'MODIFIED_TIME': '28725', 'AUDIO_ID': 'AQ0BNUMjUjIiMjMjJmRDVTRDNTQ1RWR1RjdGVGVTd2Y3RlSVVGY4NIZqp1M1ZiV2dlRFiYZ2iGJYiIaYV3VjVkdIWWWGY1dHN4ZUg2dplXuXZVeHiXd3qXaHbMyZzKiMmZd3WranyIp9bKmYnLda3K67u/6ZmHUjZ1J2Z2RFWHVFaIUmZmZ5MREREkERERETIRERESMRARERREeIZ4aIrKef/LZ4d824m5iLy8dol2hmd4h3mXeGiHqp7pmJiHvsc3dDIzIjIzIiUyMlQzREQzM1ZXh2MlZjSHeFNEZ2RViGJGdnWIQyI0IzMzVjM0VUQzQkM0Q0IiQREREREQEREA==', 'TITLE': 'try again (a cappella)', 'ARTIST': 'aaliyah'}\n",
 19:       "First entry children: ['LOCATION', 'ALBUM', 'MODIFICATION_INFO', 'INFO', 'TEMPO', 'LOUDNESS', 'MUSICAL_KEY', 'CUE_V2']\n"
 20:      ]
 21:     }
 22:    ],
 23:    "source": [
 24:     "import xml.etree.ElementTree as ET\n",
 25:     "import pandas as pd\n",
 26:     "\n",
 27:     "# Path to the collection.nml file\n",
 28:     "file_path = '../data/collection.nml'\n",
 29:     "\n",
 30:     "# Parse the XML file\n",
 31:     "tree = ET.parse(file_path)\n",
 32:     "root = tree.getroot()\n",
 33:     "\n",
 34:     "# Print the structure of the XML to understand it better\n",
 35:     "print(f\"Root tag: {root.tag}\")\n",
 36:     "print(f\"Root attributes: {root.attrib}\")\n",
 37:     "print(f\"Child tags: {[child.tag for child in root]}\")\n",
 38:     "\n",
 39:     "# Find all ENTRY elements\n",
 40:     "entries = root.findall('.//ENTRY')\n",
 41:     "print(f\"Number of entries found: {len(entries)}\")\n",
 42:     "\n",
 43:     "# If entries are found, print the first one to see its structure\n",
 44:     "if entries:\n",
 45:     "    first_entry = entries[0]\n",
 46:     "    print(\"\\nFirst entry attributes:\", first_entry.attrib)\n",
 47:     "    print(\"First entry children:\", [child.tag for child in first_entry])"
 48:    ]
 49:   },
 50:   {
 51:    "cell_type": "code",
 52:    "execution_count": 2,
 53:    "id": "4d035859",
 54:    "metadata": {},
 55:    "outputs": [
 56:     {
 57:      "name": "stdout",
 58:      "output_type": "stream",
 59:      "text": [
 60:       "Found 3003 songs with both artist and title information\n"
 61:      ]
 62:     },
 63:     {
 64:      "data": {
 65:       "text/html": [
 66:        "<div>\n",
 67:        "<style scoped>\n",
 68:        "    .dataframe tbody tr th:only-of-type {\n",
 69:        "        vertical-align: middle;\n",
 70:        "    }\n",
 71:        "\n",
 72:        "    .dataframe tbody tr th {\n",
 73:        "        vertical-align: top;\n",
 74:        "    }\n",
 75:        "\n",
 76:        "    .dataframe thead th {\n",
 77:        "        text-align: right;\n",
 78:        "    }\n",
 79:        "</style>\n",
 80:        "<table border=\"1\" class=\"dataframe\">\n",
 81:        "  <thead>\n",
 82:        "    <tr style=\"text-align: right;\">\n",
 83:        "      <th></th>\n",
 84:        "      <th>Artist</th>\n",
 85:        "      <th>Title</th>\n",
 86:        "    </tr>\n",
 87:        "  </thead>\n",
 88:        "  <tbody>\n",
 89:        "    <tr>\n",
 90:        "      <th>0</th>\n",
 91:        "      <td>aaliyah</td>\n",
 92:        "      <td>try again (a cappella)</td>\n",
 93:        "    </tr>\n",
 94:        "    <tr>\n",
 95:        "      <th>1</th>\n",
 96:        "      <td>Absolute Zero &amp; Subphonics</td>\n",
 97:        "      <td>The Code</td>\n",
 98:        "    </tr>\n",
 99:        "    <tr>\n",
100:        "      <th>2</th>\n",
101:        "      <td>Adam F</td>\n",
102:        "      <td>Brand New Funk</td>\n",
103:        "    </tr>\n",
104:        "    <tr>\n",
105:        "      <th>3</th>\n",
106:        "      <td>Alix Perez</td>\n",
107:        "      <td>Down The Line (feat. MC Fats)</td>\n",
108:        "    </tr>\n",
109:        "    <tr>\n",
110:        "      <th>4</th>\n",
111:        "      <td>Alix Perez</td>\n",
112:        "      <td>Fade Away</td>\n",
113:        "    </tr>\n",
114:        "    <tr>\n",
115:        "      <th>5</th>\n",
116:        "      <td>Alix Perez</td>\n",
117:        "      <td>Forsaken feat. Peven Everett &amp; SpectraSoul</td>\n",
118:        "    </tr>\n",
119:        "    <tr>\n",
120:        "      <th>6</th>\n",
121:        "      <td>Alix Perez</td>\n",
122:        "      <td>Revolve-Her</td>\n",
123:        "    </tr>\n",
124:        "    <tr>\n",
125:        "      <th>7</th>\n",
126:        "      <td>Alix Perez</td>\n",
127:        "      <td>Never Left</td>\n",
128:        "    </tr>\n",
129:        "    <tr>\n",
130:        "      <th>8</th>\n",
131:        "      <td>Alix Perez</td>\n",
132:        "      <td>The Cut  Deepens ft. Foreign Beggars</td>\n",
133:        "    </tr>\n",
134:        "    <tr>\n",
135:        "      <th>9</th>\n",
136:        "      <td>Ancronix</td>\n",
137:        "      <td>Skin it Back</td>\n",
138:        "    </tr>\n",
139:        "  </tbody>\n",
140:        "</table>\n",
141:        "</div>"
142:       ],
143:       "text/plain": [
144:        "                       Artist                                       Title\n",
145:        "0                     aaliyah                      try again (a cappella)\n",
146:        "1  Absolute Zero & Subphonics                                    The Code\n",
147:        "2                      Adam F                              Brand New Funk\n",
148:        "3                  Alix Perez               Down The Line (feat. MC Fats)\n",
149:        "4                  Alix Perez                                   Fade Away\n",
150:        "5                  Alix Perez  Forsaken feat. Peven Everett & SpectraSoul\n",
151:        "6                  Alix Perez                                 Revolve-Her\n",
152:        "7                  Alix Perez                                  Never Left\n",
153:        "8                  Alix Perez        The Cut  Deepens ft. Foreign Beggars\n",
154:        "9                    Ancronix                                Skin it Back"
155:       ]
156:      },
157:      "execution_count": 2,
158:      "metadata": {},
159:      "output_type": "execute_result"
160:     }
161:    ],
162:    "source": [
163:     "# Extract artist and title from all entries\n",
164:     "songs = []\n",
165:     "for entry in entries:\n",
166:     "    artist = entry.get('ARTIST', '')\n",
167:     "    title = entry.get('TITLE', '')\n",
168:     "    if artist and title:  # Only include entries that have both artist and title\n",
169:     "        songs.append({'Artist': artist, 'Title': title})\n",
170:     "\n",
171:     "# Create a DataFrame for better visualization\n",
172:     "songs_df = pd.DataFrame(songs)\n",
173:     "\n",
174:     "# Display the number of songs found\n",
175:     "print(f\"Found {len(songs_df)} songs with both artist and title information\")\n",
176:     "\n",
177:     "# Display the first 10 songs\n",
178:     "songs_df.head(10)"
179:    ]
180:   },
181:   {
182:    "cell_type": "code",
183:    "execution_count": 3,
184:    "id": "4b7d7438",
185:    "metadata": {},
186:    "outputs": [
187:     {
188:      "name": "stdout",
189:      "output_type": "stream",
190:      "text": [
191:       "Found 27 songs by Alix Perez\n"
192:      ]
193:     },
194:     {
195:      "data": {
196:       "text/html": [
197:        "<div>\n",
198:        "<style scoped>\n",
199:        "    .dataframe tbody tr th:only-of-type {\n",
200:        "        vertical-align: middle;\n",
201:        "    }\n",
202:        "\n",
203:        "    .dataframe tbody tr th {\n",
204:        "        vertical-align: top;\n",
205:        "    }\n",
206:        "\n",
207:        "    .dataframe thead th {\n",
208:        "        text-align: right;\n",
209:        "    }\n",
210:        "</style>\n",
211:        "<table border=\"1\" class=\"dataframe\">\n",
212:        "  <thead>\n",
213:        "    <tr style=\"text-align: right;\">\n",
214:        "      <th></th>\n",
215:        "      <th>Artist</th>\n",
216:        "      <th>Title</th>\n",
217:        "    </tr>\n",
218:        "  </thead>\n",
219:        "  <tbody>\n",
220:        "    <tr>\n",
221:        "      <th>3</th>\n",
222:        "      <td>Alix Perez</td>\n",
223:        "      <td>Down The Line (feat. MC Fats)</td>\n",
224:        "    </tr>\n",
225:        "    <tr>\n",
226:        "      <th>4</th>\n",
227:        "      <td>Alix Perez</td>\n",
228:        "      <td>Fade Away</td>\n",
229:        "    </tr>\n",
230:        "    <tr>\n",
231:        "      <th>5</th>\n",
232:        "      <td>Alix Perez</td>\n",
233:        "      <td>Forsaken feat. Peven Everett &amp; SpectraSoul</td>\n",
234:        "    </tr>\n",
235:        "    <tr>\n",
236:        "      <th>6</th>\n",
237:        "      <td>Alix Perez</td>\n",
238:        "      <td>Revolve-Her</td>\n",
239:        "    </tr>\n",
240:        "    <tr>\n",
241:        "      <th>7</th>\n",
242:        "      <td>Alix Perez</td>\n",
243:        "      <td>Never Left</td>\n",
244:        "    </tr>\n",
245:        "    <tr>\n",
246:        "      <th>8</th>\n",
247:        "      <td>Alix Perez</td>\n",
248:        "      <td>The Cut  Deepens ft. Foreign Beggars</td>\n",
249:        "    </tr>\n",
250:        "    <tr>\n",
251:        "      <th>94</th>\n",
252:        "      <td>Alix Perez</td>\n",
253:        "      <td>Annie's Song (S.P.Y remix)</td>\n",
254:        "    </tr>\n",
255:        "    <tr>\n",
256:        "      <th>546</th>\n",
257:        "      <td>Alix Perez Feat. Foreign Beggars</td>\n",
258:        "      <td>Dark Days Feat. Foreign Beggars (Original Mix)</td>\n",
259:        "    </tr>\n",
260:        "    <tr>\n",
261:        "      <th>764</th>\n",
262:        "      <td>Alix Perez, Skeptical</td>\n",
263:        "      <td>Without a Trace (Original Mix)</td>\n",
264:        "    </tr>\n",
265:        "    <tr>\n",
266:        "      <th>792</th>\n",
267:        "      <td>Alix Perez</td>\n",
268:        "      <td>Myriads (Jubei Remix)</td>\n",
269:        "    </tr>\n",
270:        "  </tbody>\n",
271:        "</table>\n",
272:        "</div>"
273:       ],
274:       "text/plain": [
275:        "                               Artist  \\\n",
276:        "3                          Alix Perez   \n",
277:        "4                          Alix Perez   \n",
278:        "5                          Alix Perez   \n",
279:        "6                          Alix Perez   \n",
280:        "7                          Alix Perez   \n",
281:        "8                          Alix Perez   \n",
282:        "94                         Alix Perez   \n",
283:        "546  Alix Perez Feat. Foreign Beggars   \n",
284:        "764             Alix Perez, Skeptical   \n",
285:        "792                        Alix Perez   \n",
286:        "\n",
287:        "                                              Title  \n",
288:        "3                     Down The Line (feat. MC Fats)  \n",
289:        "4                                         Fade Away  \n",
290:        "5        Forsaken feat. Peven Everett & SpectraSoul  \n",
291:        "6                                       Revolve-Her  \n",
292:        "7                                        Never Left  \n",
293:        "8              The Cut  Deepens ft. Foreign Beggars  \n",
294:        "94                       Annie's Song (S.P.Y remix)  \n",
295:        "546  Dark Days Feat. Foreign Beggars (Original Mix)  \n",
296:        "764                  Without a Trace (Original Mix)  \n",
297:        "792                           Myriads (Jubei Remix)  "
298:       ]
299:      },
300:      "execution_count": 3,
301:      "metadata": {},
302:      "output_type": "execute_result"
303:     }
304:    ],
305:    "source": [
306:     "# Function to filter songs by artist or title\n",
307:     "def filter_songs(df, artist=None, title=None):\n",
308:     "    \"\"\"\n",
309:     "    Filter songs by artist or title (case-insensitive).\n",
310:     "    \n",
311:     "    Parameters:\n",
312:     "    df (DataFrame): DataFrame containing songs\n",
313:     "    artist (str): Artist name to filter by (optional)\n",
314:     "    title (str): Title to filter by (optional)\n",
315:     "    \n",
316:     "    Returns:\n",
317:     "    DataFrame: Filtered DataFrame\n",
318:     "    \"\"\"\n",
319:     "    filtered_df = df.copy()\n",
320:     "    \n",
321:     "    if artist:\n",
322:     "        filtered_df = filtered_df[filtered_df['Artist'].str.lower().str.contains(artist.lower())]\n",
323:     "    \n",
324:     "    if title:\n",
325:     "        filtered_df = filtered_df[filtered_df['Title'].str.lower().str.contains(title.lower())]\n",
326:     "    \n",
327:     "    return filtered_df\n",
328:     "\n",
329:     "# Example: Filter songs by artist\n",
330:     "artist_search = \"Alix Perez\"  # Replace with any artist you want to search for\n",
331:     "filtered_by_artist = filter_songs(songs_df, artist=artist_search)\n",
332:     "print(f\"Found {len(filtered_by_artist)} songs by {artist_search}\")\n",
333:     "filtered_by_artist.head(10)"
334:    ]
335:   },
336:   {
337:    "cell_type": "code",
338:    "execution_count": 4,
339:    "id": "112bb00b",
340:    "metadata": {},
341:    "outputs": [
342:     {
343:      "name": "stdout",
344:      "output_type": "stream",
345:      "text": [
346:       "Found 3 songs with 'Fade' in the title\n"
347:      ]
348:     },
349:     {
350:      "data": {
351:       "text/html": [
352:        "<div>\n",
353:        "<style scoped>\n",
354:        "    .dataframe tbody tr th:only-of-type {\n",
355:        "        vertical-align: middle;\n",
356:        "    }\n",
357:        "\n",
358:        "    .dataframe tbody tr th {\n",
359:        "        vertical-align: top;\n",
360:        "    }\n",
361:        "\n",
362:        "    .dataframe thead th {\n",
363:        "        text-align: right;\n",
364:        "    }\n",
365:        "</style>\n",
366:        "<table border=\"1\" class=\"dataframe\">\n",
367:        "  <thead>\n",
368:        "    <tr style=\"text-align: right;\">\n",
369:        "      <th></th>\n",
370:        "      <th>Artist</th>\n",
371:        "      <th>Title</th>\n",
372:        "    </tr>\n",
373:        "  </thead>\n",
374:        "  <tbody>\n",
375:        "    <tr>\n",
376:        "      <th>4</th>\n",
377:        "      <td>Alix Perez</td>\n",
378:        "      <td>Fade Away</td>\n",
379:        "    </tr>\n",
380:        "    <tr>\n",
381:        "      <th>924</th>\n",
382:        "      <td>Enei, Charli Brix</td>\n",
383:        "      <td>Faded feat. Charli Brix (Original Mix)</td>\n",
384:        "    </tr>\n",
385:        "    <tr>\n",
386:        "      <th>1455</th>\n",
387:        "      <td>LSB, Drs</td>\n",
388:        "      <td>Faded (Workforce Remix)</td>\n",
389:        "    </tr>\n",
390:        "  </tbody>\n",
391:        "</table>\n",
392:        "</div>"
393:       ],
394:       "text/plain": [
395:        "                 Artist                                   Title\n",
396:        "4            Alix Perez                               Fade Away\n",
397:        "924   Enei, Charli Brix  Faded feat. Charli Brix (Original Mix)\n",
398:        "1455           LSB, Drs                 Faded (Workforce Remix)"
399:       ]
400:      },
401:      "execution_count": 4,
402:      "metadata": {},
403:      "output_type": "execute_result"
404:     }
405:    ],
406:    "source": [
407:     "# Example: Filter songs by title\n",
408:     "title_search = \"Fade\"  # Replace with any title you want to search for\n",
409:     "filtered_by_title = filter_songs(songs_df, title=title_search)\n",
410:     "print(f\"Found {len(filtered_by_title)} songs with '{title_search}' in the title\")\n",
411:     "filtered_by_title.head(10)"
412:    ]
413:   },
414:   {
415:    "cell_type": "code",
416:    "execution_count": 5,
417:    "id": "dd2f18b3",
418:    "metadata": {},
419:    "outputs": [
420:     {
421:      "name": "stdout",
422:      "output_type": "stream",
423:      "text": [
424:       "Found 1 songs by 'Alix' with 'Fade' in the title\n",
425:       "Saved full list of 3003 songs to '../data/songs_list.csv'\n"
426:      ]
427:     }
428:    ],
429:    "source": [
430:     "# Example: Filter songs by both artist and title\n",
431:     "artist_search = \"Alix\"\n",
432:     "title_search = \"Fade\"\n",
433:     "filtered_combined = filter_songs(songs_df, artist=artist_search, title=title_search)\n",
434:     "print(f\"Found {len(filtered_combined)} songs by '{artist_search}' with '{title_search}' in the title\")\n",
435:     "filtered_combined\n",
436:     "\n",
437:     "# Save the full list to a CSV file\n",
438:     "songs_df.to_csv('../data/songs_list.csv', index=False)\n",
439:     "print(f\"Saved full list of {len(songs_df)} songs to '../data/songs_list.csv'\")"
440:    ]
441:   },
442:   {
443:    "cell_type": "code",
444:    "execution_count": 6,
445:    "id": "89b6dabb",
446:    "metadata": {},
447:    "outputs": [
448:     {
449:      "name": "stdout",
450:      "output_type": "stream",
451:      "text": [
452:       "Found 3003 songs with detailed information\n",
453:       "Saved detailed list of 3003 songs to '../data/detailed_songs_list.csv'\n"
454:      ]
455:     }
456:    ],
457:    "source": [
458:     "# Extract more detailed information including genre and album\n",
459:     "detailed_songs = []\n",
460:     "for entry in entries:\n",
461:     "    artist = entry.get('ARTIST', '')\n",
462:     "    title = entry.get('TITLE', '')\n",
463:     "    \n",
464:     "    # Initialize with empty values\n",
465:     "    album = \"\"\n",
466:     "    genre = \"\"\n",
467:     "    \n",
468:     "    # Get album information\n",
469:     "    album_elem = entry.find('./ALBUM')\n",
470:     "    if album_elem is not None and 'TITLE' in album_elem.attrib:\n",
471:     "        album = album_elem.attrib['TITLE']\n",
472:     "    \n",
473:     "    # Get genre information\n",
474:     "    info_elem = entry.find('./INFO')\n",
475:     "    if info_elem is not None and 'GENRE' in info_elem.attrib:\n",
476:     "        genre = info_elem.attrib['GENRE']\n",
477:     "    \n",
478:     "    if artist and title:  # Only include entries that have both artist and title\n",
479:     "        detailed_songs.append({\n",
480:     "            'Artist': artist, \n",
481:     "            'Title': title,\n",
482:     "            'Album': album,\n",
483:     "            'Genre': genre\n",
484:     "        })\n",
485:     "\n",
486:     "# Create a DataFrame for better visualization\n",
487:     "detailed_df = pd.DataFrame(detailed_songs)\n",
488:     "\n",
489:     "# Display the number of songs found\n",
490:     "print(f\"Found {len(detailed_df)} songs with detailed information\")\n",
491:     "\n",
492:     "# Display the first 10 songs with detailed information\n",
493:     "detailed_df.head(10)\n",
494:     "\n",
495:     "# Save the detailed list to a CSV file\n",
496:     "detailed_df.to_csv('../data/detailed_songs_list.csv', index=False)\n",
497:     "print(f\"Saved detailed list of {len(detailed_df)} songs to '../data/detailed_songs_list.csv'\")"
498:    ]
499:   },
500:   {
501:    "cell_type": "code",
502:    "execution_count": 7,
503:    "id": "1883dcfa",
504:    "metadata": {},
505:    "outputs": [
506:     {
507:      "name": "stdout",
508:      "output_type": "stream",
509:      "text": [
510:       "Number of unique artists: 1060\n",
511:       "\n",
512:       "Top 10 artists by number of songs:\n",
513:       "Artist\n",
514:       "Total Science              74\n",
515:       "Ed Rush & Optical          66\n",
516:       "Bad Company                59\n",
517:       "S.P.Y                      56\n",
518:       "Spirit                     37\n",
519:       "Calibre                    37\n",
520:       "Cause 4 Concern            36\n",
521:       "Dillinja                   35\n",
522:       "Break                      34\n",
523:       "Artificial Intelligence    32\n",
524:       "Name: count, dtype: int64\n",
525:       "\n",
526:       "Top 10 genres:\n",
527:       "Genre\n",
528:       "Drum & Bass          2080\n",
529:       "                      589\n",
530:       "Electronic            129\n",
531:       "Electro                33\n",
532:       "Jungle                 28\n",
533:       "Dance                  26\n",
534:       "Drum And Bass          26\n",
535:       "Drum n Bass            12\n",
536:       "Drum and Bass          10\n",
537:       "Drum & Bass Other      10\n",
538:       "Name: count, dtype: int64\n",
539:       "\n",
540:       "Matplotlib not available for visualization\n",
541:       "\n",
542:       "Summary of the music collection:\n",
543:       "Total songs: 3003\n",
544:       "Unique artists: 1060\n",
545:       "Unique albums: 1280\n",
546:       "Unique genres: 38\n"
547:      ]
548:     }
549:    ],
550:    "source": [
551:     "# Basic statistics and analysis\n",
552:     "\n",
553:     "# Count unique artists\n",
554:     "unique_artists = detailed_df['Artist'].nunique()\n",
555:     "print(f\"Number of unique artists: {unique_artists}\")\n",
556:     "\n",
557:     "# Top 10 artists by number of songs\n",
558:     "top_artists = detailed_df['Artist'].value_counts().head(10)\n",
559:     "print(\"\\nTop 10 artists by number of songs:\")\n",
560:     "print(top_artists)\n",
561:     "\n",
562:     "# Top 10 genres\n",
563:     "if 'Genre' in detailed_df.columns:\n",
564:     "    genre_counts = detailed_df['Genre'].value_counts().head(10)\n",
565:     "    print(\"\\nTop 10 genres:\")\n",
566:     "    print(genre_counts)\n",
567:     "\n",
568:     "# Visualize top artists (if matplotlib is available)\n",
569:     "try:\n",
570:     "    import matplotlib.pyplot as plt\n",
571:     "    \n",
572:     "    plt.figure(figsize=(12, 6))\n",
573:     "    top_artists.plot(kind='bar')\n",
574:     "    plt.title('Top 10 Artists by Number of Songs')\n",
575:     "    plt.xlabel('Artist')\n",
576:     "    plt.ylabel('Number of Songs')\n",
577:     "    plt.xticks(rotation=45, ha='right')\n",
578:     "    plt.tight_layout()\n",
579:     "    plt.show()\n",
580:     "except ImportError:\n",
581:     "    print(\"\\nMatplotlib not available for visualization\")\n",
582:     "\n",
583:     "# Summary of the collection\n",
584:     "print(\"\\nSummary of the music collection:\")\n",
585:     "print(f\"Total songs: {len(detailed_df)}\")\n",
586:     "print(f\"Unique artists: {unique_artists}\")\n",
587:     "print(f\"Unique albums: {detailed_df['Album'].nunique()}\")\n",
588:     "if 'Genre' in detailed_df.columns:\n",
589:     "    print(f\"Unique genres: {detailed_df['Genre'].nunique()}\")"
590:    ]
591:   },
592:   {
593:    "cell_type": "code",
594:    "execution_count": null,
595:    "id": "637fce4c",
596:    "metadata": {},
597:    "outputs": [],
598:    "source": []
599:   }
600:  ],
601:  "metadata": {
602:   "kernelspec": {
603:    "display_name": "Python 3",
604:    "language": "python",
605:    "name": "python3"
606:   },
607:   "language_info": {
608:    "codemirror_mode": {
609:     "name": "ipython",
610:     "version": 3
611:    },
612:    "file_extension": ".py",
613:    "mimetype": "text/x-python",
614:    "name": "python",
615:    "nbconvert_exporter": "python",
616:    "pygments_lexer": "ipython3",
617:    "version": "3.10.12"
618:   }
619:  },
620:  "nbformat": 4,
621:  "nbformat_minor": 5
622: }
</file>

<file path="src/python/core/__init__.py">
1: __version__ = "0.1.0"
</file>

<file path="src/python/core/collection_expander.py">
  1: import pandas as pd
  2: import sqlite3
  3: import json
  4: from pathlib import Path
  5: from typing import List, Dict, Optional, Any
  6: import logging
  7: from datetime import datetime
  8: from .discogs_client import DiscogsAPIClient, load_api_key_from_env
  9: from .discogs_parser import DiscogsCSVParser
 10: logger = logging.getLogger(__name__)
 11: class CollectionExpander:
 12:     def __init__(self, api_key: str, db_path: str = "./data/musictool.db"):
 13:         self.api_client = DiscogsAPIClient(api_key)
 14:         self.db_path = Path(db_path)
 15:         self.db_path.parent.mkdir(parents=True, exist_ok=True)
 16:         self._init_database()
 17:         logger.info(f"Collection Expander initialized with database: {db_path}")
 18:     def expand_collection(self, discogs_csv_path: str, max_releases: int = None, skip_existing: bool = True) -> pd.DataFrame:
 19:         logger.info(f"Starting collection expansion")
 20:         parser = DiscogsCSVParser(discogs_csv_path)
 21:         releases_df = parser.parse()
 22:         total_releases = len(releases_df)
 23:         logger.info(f"Found {total_releases} releases in CSV")
 24:         if skip_existing:
 25:             already_expanded = self._get_expanded_release_ids()
 26:             releases_to_process = releases_df[~releases_df['release_id'].isin(already_expanded)]
 27:             logger.info(f"Skipping {len(already_expanded)} already expanded releases")
 28:         else:
 29:             releases_to_process = releases_df
 30:         if max_releases:
 31:             releases_to_process = releases_to_process.head(max_releases)
 32:             logger.info(f"Limited to {max_releases} releases")
 33:         release_ids = releases_to_process['release_id'].tolist()
 34:         logger.info(f"Processing {len(release_ids)} releases")
 35:         expanded_tracks = []
 36:         processed = 0
 37:         errors = 0
 38:         for i, release_id in enumerate(release_ids):
 39:             try:
 40:                 logger.info(f"Processing release {i+1}/{len(release_ids)}: {release_id}")
 41:                 release_info = releases_df[releases_df['release_id'] == release_id].iloc[0]
 42:                 logger.info(f"  ‚Üí {release_info['artist_clean']} - {release_info['title']}")
 43:                 tracklist = self.api_client.get_release_tracklist(release_id)
 44:                 if tracklist:
 45:                     track_count = len([t for t in tracklist if t['type_'] == 'track'])
 46:                     if track_count > 0:
 47:                         release_tracks = []
 48:                         for track in tracklist:
 49:                             if track['type_'] == 'track':
 50:                                 expanded_track = self._create_expanded_track(track, release_info)
 51:                                 release_tracks.append(expanded_track)
 52:                         release_df = pd.DataFrame(release_tracks)
 53:                         self._save_expanded_collection(release_df)
 54:                         expanded_tracks.extend(release_tracks)
 55:                         processed += 1
 56:                         logger.info(f"  ‚úÖ {track_count} tracks added and saved to database")
 57:                     else:
 58:                         logger.warning(f"  ‚ö†Ô∏è No actual tracks found (only headings/metadata)")
 59:                         processed += 1
 60:                 else:
 61:                     logger.warning(f"  ‚ùå No tracklist found for release {release_id}")
 62:                     errors += 1
 63:                 if (i + 1) % 10 == 0:
 64:                     logger.info(f"üìä Progress: {i+1}/{len(release_ids)} releases processed ({processed} successful, {errors} errors)")
 65:             except KeyboardInterrupt:
 66:                 logger.info("‚èπÔ∏è Interrupted by user")
 67:                 break
 68:             except Exception as e:
 69:                 logger.error(f"  ‚ùå Error processing release {release_id}: {e}")
 70:                 errors += 1
 71:                 continue
 72:         logger.info(f"üéâ Expansion complete!")
 73:         logger.info(f"üìä Final stats: {processed} successful, {errors} errors out of {len(release_ids)} total")
 74:         return self.load_physical_collection()
 75:     def _create_expanded_track(self, track: Dict[str, Any], release_info: pd.Series) -> Dict[str, Any]:
 76:         track_artists = track.get('artists', [])
 77:         if track_artists:
 78:             artist = ', '.join(track_artists)
 79:         else:
 80:             artist = release_info['artist_clean']
 81:         expanded_track = {
 82:             'artist': artist,
 83:             'title': track['title'],
 84:             'position': track['position'],
 85:             'duration': track['duration'],
 86:             'album': release_info['title'],
 87:             'album_artist': release_info['artist_clean'],
 88:             'label': release_info['label'],
 89:             'catalog_number': release_info['catalog_number_clean'],
 90:             'release_year': release_info['release_year'],
 91:             'format_type': release_info['format_type'],
 92:             'format_description': release_info['format_description'],
 93:             'collection_folder': release_info['collection_folder'],
 94:             'date_added': release_info['date_added'],
 95:             'media_condition': release_info['media_condition'],
 96:             'sleeve_condition': release_info['sleeve_condition'],
 97:             'notes': release_info['notes'],
 98:             'discogs_release_id': release_info['release_id'],
 99:             'source': 'discogs_physical',
100:             'expanded_date': datetime.now().isoformat()
101:         }
102:         return expanded_track
103:     def _init_database(self):
104:         with sqlite3.connect(self.db_path) as conn:
105:             cursor = conn.cursor()
106:             cursor.execute('''
107:                 CREATE TABLE IF NOT EXISTS expanded_tracks (
108:                     id INTEGER PRIMARY KEY AUTOINCREMENT,
109:                     artist TEXT NOT NULL,
110:                     title TEXT NOT NULL,
111:                     position TEXT,
112:                     duration TEXT,
113:                     album TEXT,
114:                     album_artist TEXT,
115:                     label TEXT,
116:                     catalog_number TEXT,
117:                     release_year INTEGER,
118:                     format_type TEXT,
119:                     format_description TEXT,
120:                     collection_folder TEXT,
121:                     date_added TEXT,
122:                     media_condition TEXT,
123:                     sleeve_condition TEXT,
124:                     notes TEXT,
125:                     discogs_release_id INTEGER,
126:                     source TEXT,
127:                     expanded_date TEXT,
128:                     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
129:                 )
130:             ''')
131:             cursor.execute('''
132:                 CREATE INDEX IF NOT EXISTS idx_artist_title
133:                 ON expanded_tracks(artist, title)
134:             ''')
135:             cursor.execute('''
136:                 CREATE INDEX IF NOT EXISTS idx_release_id
137:                 ON expanded_tracks(discogs_release_id)
138:             ''')
139:             conn.commit()
140:             logger.info("Database initialized successfully")
141:     def _get_expanded_release_ids(self) -> List[int]:
142:         try:
143:             with sqlite3.connect(self.db_path) as conn:
144:                 cursor = conn.cursor()
145:                 cursor.execute('SELECT DISTINCT discogs_release_id FROM expanded_tracks')
146:                 release_ids = [row[0] for row in cursor.fetchall()]
147:                 logger.info(f"Found {len(release_ids)} already expanded releases")
148:                 return release_ids
149:         except Exception as e:
150:             logger.warning(f"Error getting expanded release IDs: {e}")
151:             return []
152:     def _save_expanded_collection(self, df: pd.DataFrame):
153:         if df.empty:
154:             logger.warning("No tracks to save")
155:             return
156:         with sqlite3.connect(self.db_path) as conn:
157:             release_ids = df['discogs_release_id'].unique().tolist()
158:             placeholders = ','.join(['?' for _ in release_ids])
159:             cursor = conn.cursor()
160:             cursor.execute(f'''
161:                 DELETE FROM expanded_tracks
162:                 WHERE discogs_release_id IN ({placeholders})
163:             ''', release_ids)
164:             df.to_sql('expanded_tracks', conn, if_exists='append', index=False)
165:             logger.info(f"Saved {len(df)} tracks to database")
166:     def load_physical_collection(self) -> pd.DataFrame:
167:         try:
168:             with sqlite3.connect(self.db_path) as conn:
169:                 df = pd.read_sql_query('SELECT * FROM expanded_tracks', conn)
170:                 logger.info(f"Loaded {len(df)} tracks from physical collection database")
171:                 return df
172:         except Exception as e:
173:             logger.error(f"Error loading physical collection: {e}")
174:             return pd.DataFrame()
175:     def get_expansion_stats(self) -> Dict[str, Any]:
176:         df = self.load_physical_collection()
177:         if df.empty:
178:             return {'total_tracks': 0, 'total_releases': 0}
179:         stats = {
180:             'total_tracks': len(df),
181:             'total_releases': df['discogs_release_id'].nunique(),
182:             'unique_artists': df['artist'].nunique(),
183:             'unique_albums': df['album'].nunique(),
184:             'format_breakdown': df['format_type'].value_counts().to_dict(),
185:             'collection_folders': df['collection_folder'].value_counts().to_dict(),
186:             'labels': df['label'].nunique(),
187:             'year_range': {
188:                 'earliest': int(df['release_year'].min()) if df['release_year'].min() > 0 else None,
189:                 'latest': int(df['release_year'].max()) if df['release_year'].max() > 0 else None
190:             }
191:         }
192:         return stats
193: def expand_collection_cli(csv_path: str, max_releases: int = 10):
194:     api_key = load_api_key_from_env()
195:     if not api_key:
196:         print("‚ùå No Discogs API key found in .env file")
197:         return None
198:     expander = CollectionExpander(api_key)
199:     print(f"üöÄ Starting collection expansion (max {max_releases} releases)")
200:     expanded_df = expander.expand_collection(csv_path, max_releases)
201:     if not expanded_df.empty:
202:         print(f"\n‚úÖ Expansion complete!")
203:         stats = expander.get_expansion_stats()
204:         print(f"üìä Results: {stats['total_tracks']} tracks from {stats['total_releases']} releases")
205:         print(f"üéµ Artists: {stats['unique_artists']}, Albums: {stats['unique_albums']}")
206:         return expanded_df
207:     else:
208:         print("‚ùå No tracks expanded")
209:         return None
210: if __name__ == "__main__":
211:     import sys
212:     max_releases = int(sys.argv[1]) if len(sys.argv) > 1 else 5
213:     csv_path = "./data/gazmazk4ez-collection-20250608-1029.csv"
214:     expand_collection_cli(csv_path, max_releases)
</file>

<file path="src/python/core/discogs_client.py">
  1: import requests
  2: import time
  3: import json
  4: import os
  5: from pathlib import Path
  6: from typing import Dict, List, Optional, Any
  7: import logging
  8: from requests.adapters import HTTPAdapter
  9: from urllib3.util.retry import Retry
 10: logger = logging.getLogger(__name__)
 11: class DiscogsAPIClient:
 12:     def __init__(self, api_key: str, cache_dir: str = "./data/cache"):
 13:         self.api_key = api_key
 14:         self.base_url = "https://api.discogs.com"
 15:         self.cache_dir = Path(cache_dir)
 16:         self.cache_dir.mkdir(parents=True, exist_ok=True)
 17:         self.requests_per_minute = 60
 18:         self.min_delay = 60 / self.requests_per_minute
 19:         self.last_request_time = 0
 20:         self.session = requests.Session()
 21:         retry_strategy = Retry(
 22:             total=3,
 23:             backoff_factor=1,
 24:             status_forcelist=[429, 500, 502, 503, 504],
 25:         )
 26:         adapter = HTTPAdapter(max_retries=retry_strategy)
 27:         self.session.mount("http://", adapter)
 28:         self.session.mount("https://", adapter)
 29:         self.session.headers.update({
 30:             'User-Agent': 'MusicTool/1.0',
 31:             'Authorization': f'Discogs token={api_key}'
 32:         })
 33:         logger.info(f"Discogs API client initialized with cache dir: {cache_dir}")
 34:     def get_release(self, release_id: int, use_cache: bool = True) -> Optional[Dict[str, Any]]:
 35:         cache_file = self.cache_dir / f"release_{release_id}.json"
 36:         if use_cache and cache_file.exists():
 37:             try:
 38:                 with open(cache_file, 'r', encoding='utf-8') as f:
 39:                     logger.debug(f"Using cached data for release {release_id}")
 40:                     return json.load(f)
 41:             except Exception as e:
 42:                 logger.warning(f"Error reading cache for release {release_id}: {e}")
 43:         try:
 44:             url = f"{self.base_url}/releases/{release_id}"
 45:             self._wait_for_rate_limit()
 46:             logger.info(f"Fetching release {release_id} from Discogs API")
 47:             response = self.session.get(url)
 48:             if response.status_code == 200:
 49:                 release_data = response.json()
 50:                 try:
 51:                     with open(cache_file, 'w', encoding='utf-8') as f:
 52:                         json.dump(release_data, f, indent=2, ensure_ascii=False)
 53:                     logger.debug(f"Cached release {release_id} data")
 54:                 except Exception as e:
 55:                     logger.warning(f"Error caching release {release_id}: {e}")
 56:                 return release_data
 57:             elif response.status_code == 404:
 58:                 logger.warning(f"Release {release_id} not found")
 59:                 return None
 60:             elif response.status_code == 429:
 61:                 logger.warning(f"Rate limit hit for release {release_id}")
 62:                 time.sleep(60)
 63:                 return self.get_release(release_id, use_cache=False)
 64:             else:
 65:                 logger.error(f"API error for release {release_id}: {response.status_code}")
 66:                 return None
 67:         except Exception as e:
 68:             logger.error(f"Error fetching release {release_id}: {e}")
 69:             return None
 70:     def get_release_tracklist(self, release_id: int) -> List[Dict[str, Any]]:
 71:         release_data = self.get_release(release_id)
 72:         if not release_data:
 73:             return []
 74:         tracks = []
 75:         tracklist = release_data.get('tracklist', [])
 76:         for track in tracklist:
 77:             track_info = {
 78:                 'position': track.get('position', ''),
 79:                 'title': track.get('title', ''),
 80:                 'duration': track.get('duration', ''),
 81:                 'type_': track.get('type_', 'track'),
 82:                 'artists': self._extract_track_artists(track),
 83:                 'release_id': release_id
 84:             }
 85:             tracks.append(track_info)
 86:         return tracks
 87:     def batch_get_releases(self, release_ids: List[int], max_requests: int = 5) -> Dict[int, Dict[str, Any]]:
 88:         results = {}
 89:         processed = 0
 90:         logger.info(f"Fetching {min(len(release_ids), max_requests)} releases (limit: {max_requests})")
 91:         for release_id in release_ids[:max_requests]:
 92:             try:
 93:                 release_data = self.get_release(release_id)
 94:                 if release_data:
 95:                     results[release_id] = release_data
 96:                     processed += 1
 97:                     logger.info(f"Progress: {processed}/{min(len(release_ids), max_requests)} releases fetched")
 98:                 else:
 99:                     logger.warning(f"Failed to fetch release {release_id}")
100:             except KeyboardInterrupt:
101:                 logger.info("Interrupted by user")
102:                 break
103:             except Exception as e:
104:                 logger.error(f"Error processing release {release_id}: {e}")
105:                 continue
106:         logger.info(f"Batch completed: {len(results)} releases fetched successfully")
107:         return results
108:     def _wait_for_rate_limit(self):
109:         current_time = time.time()
110:         time_since_last = current_time - self.last_request_time
111:         if time_since_last < self.min_delay:
112:             sleep_time = self.min_delay - time_since_last
113:             logger.debug(f"Rate limiting: sleeping {sleep_time:.2f} seconds")
114:             time.sleep(sleep_time)
115:         self.last_request_time = time.time()
116:     def _extract_track_artists(self, track: Dict[str, Any]) -> List[str]:
117:         artists = []
118:         if 'artists' in track:
119:             for artist in track['artists']:
120:                 if isinstance(artist, dict):
121:                     artists.append(artist.get('name', ''))
122:                 else:
123:                     artists.append(str(artist))
124:         return artists
125:     def get_cache_stats(self) -> Dict[str, int]:
126:         cache_files = list(self.cache_dir.glob("release_*.json"))
127:         return {
128:             'cached_releases': len(cache_files),
129:             'cache_size_mb': sum(f.stat().st_size for f in cache_files) / (1024 * 1024)
130:         }
131: def load_api_key_from_env() -> Optional[str]:
132:     from dotenv import load_dotenv
133:     load_dotenv()
134:     api_key = os.getenv('DISCOGS_API_KEY')
135:     if not api_key:
136:         logger.error("DISCOGS_API_KEY not found in environment variables")
137:         return None
138:     return api_key
139: if __name__ == "__main__":
140:     api_key = load_api_key_from_env()
141:     if api_key:
142:         client = DiscogsAPIClient(api_key)
143:         test_release = client.get_release(23143)
144:         if test_release:
145:             print(f"Test successful: {test_release.get('title', 'Unknown')}")
146:             tracklist = client.get_release_tracklist(23143)
147:             print(f"Tracks found: {len(tracklist)}")
148:         else:
149:             print("Test failed")
150:     else:
151:         print("No API key found")
</file>

<file path="src/python/core/discogs_parser.py">
  1: import pandas as pd
  2: from pathlib import Path
  3: from typing import List, Dict, Optional
  4: import logging
  5: logger = logging.getLogger(__name__)
  6: class DiscogsCSVParser:
  7:     def __init__(self, csv_file_path: str):
  8:         self.csv_file_path = Path(csv_file_path)
  9:         if not self.csv_file_path.exists():
 10:             raise FileNotFoundError(f"Discogs CSV file not found: {csv_file_path}")
 11:     def parse(self) -> pd.DataFrame:
 12:         logger.info(f"Parsing Discogs CSV file: {self.csv_file_path}")
 13:         try:
 14:             df = pd.read_csv(str(self.csv_file_path))
 15:             logger.info(f"Found {len(df)} releases in Discogs CSV")
 16:             df = self._clean_dataframe(df)
 17:             df = self._normalize_data(df)
 18:             logger.info(f"Successfully parsed {len(df)} releases")
 19:             return df
 20:         except Exception as e:
 21:             logger.error(f"Error parsing Discogs CSV file: {e}")
 22:             raise
 23:     def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
 24:         column_mapping = {
 25:             'Catalog#': 'catalog_number',
 26:             'Artist': 'artist',
 27:             'Title': 'title',
 28:             'Label': 'label',
 29:             'Format': 'format',
 30:             'Rating': 'rating',
 31:             'Released': 'released',
 32:             'release_id': 'release_id',
 33:             'CollectionFolder': 'collection_folder',
 34:             'Date Added': 'date_added',
 35:             'Collection Media Condition': 'media_condition',
 36:             'Collection Sleeve Condition': 'sleeve_condition',
 37:             'Collection Notes': 'notes'
 38:         }
 39:         df = df.rename(columns=column_mapping)
 40:         text_columns = ['catalog_number', 'artist', 'title', 'label', 'format',
 41:                        'collection_folder', 'media_condition', 'sleeve_condition', 'notes']
 42:         for col in text_columns:
 43:             if col in df.columns:
 44:                 df[col] = df[col].fillna('').astype(str)
 45:         return df
 46:     def _normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:
 47:         if 'catalog_number' in df.columns:
 48:             df['catalog_number_clean'] = df['catalog_number'].apply(self._clean_catalog_number)
 49:         if 'released' in df.columns:
 50:             df['release_year'] = df['released'].apply(self._parse_release_year)
 51:         if 'artist' in df.columns:
 52:             df['artist_clean'] = df['artist'].apply(self._clean_artist_name)
 53:         if 'format' in df.columns:
 54:             df['format_type'] = df['format'].apply(self._extract_format_type)
 55:             df['format_description'] = df['format'].apply(self._extract_format_description)
 56:         if 'date_added' in df.columns:
 57:             df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')
 58:         return df
 59:     def _clean_catalog_number(self, catalog_num: str) -> str:
 60:         if not catalog_num or catalog_num == 'nan':
 61:             return ''
 62:         catalog_clean = str(catalog_num).strip().strip('"')
 63:         if ',' in catalog_clean:
 64:             catalog_clean = catalog_clean.split(',')[0].strip()
 65:         return catalog_clean
 66:     def _parse_release_year(self, released: str) -> int:
 67:         if not released or str(released) == 'nan':
 68:             return 0
 69:         try:
 70:             year_str = str(released).strip()
 71:             if year_str.isdigit() and len(year_str) == 4:
 72:                 return int(year_str)
 73:             else:
 74:                 import re
 75:                 year_match = re.search(r'\b(19|20)\d{2}\b', year_str)
 76:                 if year_match:
 77:                     return int(year_match.group())
 78:         except (ValueError, AttributeError):
 79:             pass
 80:         return 0
 81:     def _clean_artist_name(self, artist: str) -> str:
 82:         if not artist or artist == 'nan':
 83:             return ''
 84:         artist_clean = str(artist).strip()
 85:         if artist_clean.lower() in ['various', 'various artists']:
 86:             return 'Various'
 87:         artist_clean = ' '.join(artist_clean.split())
 88:         return artist_clean
 89:     def _extract_format_type(self, format_str: str) -> str:
 90:         if not format_str or format_str == 'nan':
 91:             return ''
 92:         format_lower = str(format_str).lower()
 93:         if 'lp' in format_lower:
 94:             return 'LP'
 95:         elif '12"' in format_str or '12 inch' in format_lower:
 96:             return '12"'
 97:         elif '7"' in format_str or '7 inch' in format_lower:
 98:             return '7"'
 99:         elif 'cd' in format_lower:
100:             return 'CD'
101:         elif 'cassette' in format_lower or 'tape' in format_lower:
102:             return 'Cassette'
103:         else:
104:             return 'Other'
105:     def _extract_format_description(self, format_str: str) -> str:
106:         if not format_str or format_str == 'nan':
107:             return ''
108:         desc = str(format_str).strip().strip('"')
109:         return desc
110: if __name__ == "__main__":
111:     import sys
112:     if len(sys.argv) > 1:
113:         parser = DiscogsCSVParser(sys.argv[1])
114:         df = parser.parse()
115:         print(f"Parsed {len(df)} releases")
116:         print(df.head())
</file>

<file path="src/python/core/duplicate_finder.py">
  1: import pandas as pd
  2: from typing import Dict, List, Tuple, Optional, Set
  3: import logging
  4: from fuzzywuzzy import fuzz
  5: import re
  6: from collections import defaultdict
  7: import time
  8: from .nml_parser import NMLParser
  9: logger = logging.getLogger(__name__)
 10: class DuplicateFinder:
 11:     def __init__(self, nml_path: str):
 12:         self.nml_path = nml_path
 13:         self.digital_collection = self._load_digital_collection()
 14:         logger.info(f"Duplicate Finder initialized with {len(self.digital_collection)} tracks")
 15:     def find_duplicates(self, similarity_threshold: int = 85, group_by: str = "artist_title") -> pd.DataFrame:
 16:         logger.info(f"Starting duplicate search (threshold: {similarity_threshold}%, method: {group_by})")
 17:         start_time = time.time()
 18:         if self.digital_collection.empty:
 19:             logger.warning("No digital collection data found")
 20:             return pd.DataFrame()
 21:         duplicate_groups = []
 22:         processed_tracks = set()
 23:         for idx, track in self.digital_collection.iterrows():
 24:             if idx in processed_tracks:
 25:                 continue
 26:             similar_tracks = self._find_similar_tracks(
 27:                 track, idx, similarity_threshold, group_by, processed_tracks
 28:             )
 29:             if similar_tracks:
 30:                 group_id = len(duplicate_groups) + 1
 31:                 duplicate_groups.append(self._create_duplicate_record(track, idx, group_id, 1, 100.0, "original"))
 32:                 for similar_track, similar_idx, similarity_score in similar_tracks:
 33:                     duplicate_groups.append(
 34:                         self._create_duplicate_record(
 35:                             similar_track, similar_idx, group_id,
 36:                             len([t for t in similar_tracks if t[2] >= similarity_score]) + 2,
 37:                             similarity_score, "duplicate"
 38:                         )
 39:                     )
 40:                     processed_tracks.add(similar_idx)
 41:                 processed_tracks.add(idx)
 42:         duplicates_df = pd.DataFrame(duplicate_groups)
 43:         elapsed_time = time.time() - start_time
 44:         total_groups = duplicates_df['group_id'].nunique() if not duplicates_df.empty else 0
 45:         total_duplicates = len(duplicates_df) if not duplicates_df.empty else 0
 46:         logger.info(f"üîç Duplicate search complete:")
 47:         logger.info(f"  Processing time: {elapsed_time:.2f}s")
 48:         logger.info(f"  Duplicate groups found: {total_groups}")
 49:         logger.info(f"  Total duplicate tracks: {total_duplicates}")
 50:         return duplicates_df
 51:     def _find_similar_tracks(self, reference_track: pd.Series, ref_idx: int,
 52:                            threshold: int, method: str, processed: Set[int]) -> List[Tuple]:
 53:         similar_tracks = []
 54:         ref_artist = self._normalize_text(reference_track['artist'])
 55:         ref_title = self._normalize_text(reference_track['title'])
 56:         ref_filename = self._normalize_filename(reference_track.get('location', ''))
 57:         ref_duration = reference_track.get('totaltime', 0)
 58:         for idx, track in self.digital_collection.iterrows():
 59:             if idx <= ref_idx or idx in processed:
 60:                 continue
 61:             similarity_score = self._calculate_similarity(
 62:                 reference_track, track, method,
 63:                 ref_artist, ref_title, ref_filename, ref_duration
 64:             )
 65:             if similarity_score >= threshold:
 66:                 similar_tracks.append((track, idx, similarity_score))
 67:         similar_tracks.sort(key=lambda x: x[2], reverse=True)
 68:         return similar_tracks
 69:     def _calculate_similarity(self, track1: pd.Series, track2: pd.Series, method: str,
 70:                             ref_artist: str, ref_title: str, ref_filename: str, ref_duration: float) -> float:
 71:         artist2 = self._normalize_text(track2['artist'])
 72:         title2 = self._normalize_text(track2['title'])
 73:         filename2 = self._normalize_filename(track2.get('location', ''))
 74:         duration2 = track2.get('totaltime', 0)
 75:         if method == "artist_title":
 76:             artist_sim = fuzz.ratio(ref_artist, artist2)
 77:             title_sim = fuzz.ratio(ref_title, title2)
 78:             combined_sim = fuzz.ratio(f"{ref_artist} {ref_title}", f"{artist2} {title2}")
 79:             return (title_sim * 0.5) + (artist_sim * 0.3) + (combined_sim * 0.2)
 80:         elif method == "title_only":
 81:             return fuzz.ratio(ref_title, title2)
 82:         elif method == "filename":
 83:             return fuzz.ratio(ref_filename, filename2)
 84:         elif method == "duration":
 85:             title_sim = fuzz.ratio(ref_title, title2)
 86:             if ref_duration > 0 and duration2 > 0:
 87:                 duration_diff = abs(ref_duration - duration2) / 1000
 88:                 duration_sim = max(0, 100 - (duration_diff * 5))
 89:                 duration_sim = min(100, duration_sim)
 90:             else:
 91:                 duration_sim = 0
 92:             return (title_sim * 0.7) + (duration_sim * 0.3)
 93:         return 0
 94:     def _create_duplicate_record(self, track: pd.Series, track_idx: int, group_id: int,
 95:                                rank: int, similarity: float, status: str) -> Dict:
 96:         return {
 97:             'group_id': group_id,
 98:             'rank': rank,
 99:             'status': status,
100:             'similarity': similarity,
101:             'artist': track['artist'],
102:             'title': track['title'],
103:             'album': track['album'],
104:             'genre': track.get('genre', ''),
105:             'bpm': track.get('bpm', 0),
106:             'duration': track.get('totaltime', 0),
107:             'bitrate': track.get('bitrate', 0),
108:             'filetype': track.get('filetype', ''),
109:             'filesize': track.get('filesize', 0),
110:             'location': track.get('location', ''),
111:             'date_added': track.get('dateadded', ''),
112:             'play_count': track.get('playcount', 0),
113:             'track_index': track_idx
114:         }
115:     def _normalize_text(self, text: str) -> str:
116:         if not text or pd.isna(text):
117:             return ''
118:         text = str(text).lower()
119:         text = re.sub(r'\b(the|a|an)\b', '', text)
120:         text = re.sub(r'\(.*?\)', '', text)
121:         text = re.sub(r'\[.*?\]', '', text)
122:         text = re.sub(r'\s*-\s*(remix|edit|mix|version|remaster|remastered)\b.*', '', text)
123:         text = re.sub(r'[^\w\s]', ' ', text)
124:         text = re.sub(r'\s+', ' ', text)
125:         text = text.strip()
126:         return text
127:     def _normalize_filename(self, filepath: str) -> str:
128:         if not filepath or pd.isna(filepath):
129:             return ''
130:         filename = filepath.split('/')[-1].split('\\')[-1]
131:         filename = re.sub(r'\.[^.]*$', '', filename)
132:         return self._normalize_text(filename)
133:     def _load_digital_collection(self) -> pd.DataFrame:
134:         try:
135:             parser = NMLParser(self.nml_path)
136:             df = parser.parse()
137:             logger.info(f"Loaded {len(df)} digital tracks from NML")
138:             return df
139:         except Exception as e:
140:             logger.error(f"Error loading digital collection: {e}")
141:             return pd.DataFrame()
142:     def get_duplicate_stats(self, duplicates_df: pd.DataFrame) -> Dict:
143:         if duplicates_df.empty:
144:             return {}
145:         total_groups = duplicates_df['group_id'].nunique()
146:         total_duplicates = len(duplicates_df)
147:         potential_space_saved = 0
148:         for group_id in duplicates_df['group_id'].unique():
149:             group_tracks = duplicates_df[duplicates_df['group_id'] == group_id]
150:             if len(group_tracks) > 1:
151:                 filesizes = group_tracks['filesize'].astype(float)
152:                 potential_space_saved += filesizes.sum() - filesizes.max()
153:         duplicate_tracks = duplicates_df[duplicates_df['status'] == 'duplicate']
154:         stats = {
155:             'total_groups': total_groups,
156:             'total_duplicate_tracks': len(duplicate_tracks),
157:             'original_tracks': len(duplicates_df[duplicates_df['status'] == 'original']),
158:             'potential_space_saved_mb': potential_space_saved / (1024 * 1024),
159:             'average_similarity': duplicate_tracks['similarity'].mean() if not duplicate_tracks.empty else 0,
160:             'top_duplicate_artists': duplicate_tracks['artist'].value_counts().head().to_dict(),
161:             'top_duplicate_albums': duplicate_tracks['album'].value_counts().head().to_dict(),
162:             'duplicate_formats': duplicate_tracks['filetype'].value_counts().to_dict(),
163:             'largest_groups': duplicates_df.groupby('group_id').size().sort_values(ascending=False).head().to_dict()
164:         }
165:         return stats
166: def find_duplicates_cli(nml_path: str = "./data/collection.nml",
167:                        threshold: int = 85, method: str = "artist_title"):
168:     print("üîç Starting Duplicate Search...")
169:     print(f"   Similarity threshold: {threshold}%")
170:     print(f"   Method: {method}")
171:     try:
172:         finder = DuplicateFinder(nml_path)
173:         duplicates = finder.find_duplicates(threshold, method)
174:         if not duplicates.empty:
175:             print(f"\nüìä Duplicate Search Results:")
176:             stats = finder.get_duplicate_stats(duplicates)
177:             print(f"   Duplicate groups: {stats['total_groups']}")
178:             print(f"   Duplicate tracks: {stats['total_duplicate_tracks']}")
179:             print(f"   Potential space saved: {stats['potential_space_saved_mb']:.1f} MB")
180:             print(f"   Average similarity: {stats['average_similarity']:.1f}%")
181:             print(f"\nüéµ Sample Duplicate Groups:")
182:             for group_id in duplicates['group_id'].unique()[:3]:
183:                 group = duplicates[duplicates['group_id'] == group_id]
184:                 print(f"\n   Group {group_id}:")
185:                 for _, track in group.iterrows():
186:                     status_icon = "üü¢" if track['status'] == 'original' else "üîÑ"
187:                     print(f"     {status_icon} {track['artist']} - {track['title']} ({track['similarity']:.1f}%)")
188:             return duplicates
189:         else:
190:             print("‚úÖ No duplicates found")
191:             return None
192:     except Exception as e:
193:         print(f"‚ùå Error during duplicate search: {e}")
194:         import traceback
195:         traceback.print_exc()
196:         return None
197: if __name__ == "__main__":
198:     import sys
199:     threshold = int(sys.argv[1]) if len(sys.argv) > 1 else 85
200:     method = sys.argv[2] if len(sys.argv) > 2 else "artist_title"
201:     find_duplicates_cli(threshold=threshold, method=method)
</file>

<file path="src/python/core/gap_analyzer_fast.py">
  1: import pandas as pd
  2: from typing import Dict, List, Tuple, Optional
  3: import logging
  4: from fuzzywuzzy import fuzz
  5: from fuzzywuzzy import process
  6: import re
  7: from collections import defaultdict
  8: import time
  9: from .nml_parser import NMLParser
 10: from .collection_expander import CollectionExpander, load_api_key_from_env
 11: logger = logging.getLogger(__name__)
 12: class FastGapAnalyzer:
 13:     def __init__(self, nml_path: str, db_path: str = "./data/musictool.db"):
 14:         self.nml_path = nml_path
 15:         self.db_path = db_path
 16:         self.digital_collection = self._load_digital_collection()
 17:         self.physical_collection = self._load_physical_collection()
 18:         self.digital_index = self._create_search_index(self.digital_collection)
 19:         logger.info(f"Fast Gap Analyzer initialized:")
 20:         logger.info(f"  Digital tracks: {len(self.digital_collection)}")
 21:         logger.info(f"  Physical tracks: {len(self.physical_collection)}")
 22:         logger.info(f"  Digital index entries: {len(self.digital_index)}")
 23:     def find_gaps(self, confidence_threshold: int = 80, batch_size: int = 100) -> pd.DataFrame:
 24:         logger.info(f"Starting fast gap analysis (confidence threshold: {confidence_threshold}%)")
 25:         start_time = time.time()
 26:         if self.physical_collection.empty:
 27:             logger.warning("No physical collection data found")
 28:             return pd.DataFrame()
 29:         if self.digital_collection.empty:
 30:             logger.warning("No digital collection data found")
 31:             return pd.DataFrame()
 32:         gap_results = []
 33:         total_tracks = len(self.physical_collection)
 34:         for i in range(0, total_tracks, batch_size):
 35:             batch_end = min(i + batch_size, total_tracks)
 36:             batch = self.physical_collection.iloc[i:batch_end]
 37:             logger.info(f"Processing batch {i//batch_size + 1}: tracks {i+1}-{batch_end} of {total_tracks}")
 38:             for idx, physical_track in batch.iterrows():
 39:                 result = self._find_track_in_digital_fast(physical_track, confidence_threshold)
 40:                 gap_results.append(result)
 41:         gap_df = pd.DataFrame(gap_results)
 42:         elapsed_time = time.time() - start_time
 43:         tracks_per_second = total_tracks / elapsed_time if elapsed_time > 0 else 0
 44:         total_tracks = len(gap_df)
 45:         found_tracks = len(gap_df[gap_df['status'] == 'found'])
 46:         missing_tracks = len(gap_df[gap_df['status'] == 'missing'])
 47:         logger.info(f"üéØ Fast gap analysis complete:")
 48:         logger.info(f"  Total physical tracks: {total_tracks}")
 49:         logger.info(f"  Found in digital: {found_tracks} ({found_tracks/total_tracks*100:.1f}%)")
 50:         logger.info(f"  Missing from digital: {missing_tracks} ({missing_tracks/total_tracks*100:.1f}%)")
 51:         logger.info(f"  Processing time: {elapsed_time:.2f}s ({tracks_per_second:.1f} tracks/sec)")
 52:         return gap_df
 53:     def _create_search_index(self, digital_df: pd.DataFrame) -> Dict[str, List[int]]:
 54:         index = defaultdict(list)
 55:         for idx, track in digital_df.iterrows():
 56:             artist = self._normalize_text(track['artist'])
 57:             title = self._normalize_text(track['title'])
 58:             if artist:
 59:                 artist_prefix = artist[:3] if len(artist) >= 3 else artist
 60:                 index[f"artist:{artist_prefix}"].append(idx)
 61:             if title:
 62:                 title_prefix = title[:3] if len(title) >= 3 else title
 63:                 index[f"title:{title_prefix}"].append(idx)
 64:             if artist and title:
 65:                 combined_prefix = f"{artist} {title}"[:6]
 66:                 index[f"combined:{combined_prefix}"].append(idx)
 67:         return dict(index)
 68:     def _find_track_in_digital_fast(self, physical_track: pd.Series, confidence_threshold: int) -> Dict:
 69:         phys_artist = self._normalize_text(physical_track['artist'])
 70:         phys_title = self._normalize_text(physical_track['title'])
 71:         candidates = set()
 72:         if phys_artist:
 73:             artist_prefix = phys_artist[:3] if len(phys_artist) >= 3 else phys_artist
 74:             candidates.update(self.digital_index.get(f"artist:{artist_prefix}", []))
 75:         if phys_title:
 76:             title_prefix = phys_title[:3] if len(phys_title) >= 3 else phys_title
 77:             candidates.update(self.digital_index.get(f"title:{title_prefix}", []))
 78:         if phys_artist and phys_title:
 79:             combined_prefix = f"{phys_artist} {phys_title}"[:6]
 80:             candidates.update(self.digital_index.get(f"combined:{combined_prefix}", []))
 81:         if not candidates:
 82:             sample_size = min(100, len(self.digital_collection))
 83:             candidates = set(self.digital_collection.sample(n=sample_size).index)
 84:         if len(candidates) > 200:
 85:             candidates = set(list(candidates)[:200])
 86:         logger.debug(f"Checking {len(candidates)} candidates for: {phys_artist} - {phys_title}")
 87:         best_match = None
 88:         best_confidence = 0
 89:         best_match_data = None
 90:         phys_artist_title = f"{phys_artist} {phys_title}"
 91:         for idx in candidates:
 92:             digital_track = self.digital_collection.iloc[idx]
 93:             dig_artist = self._normalize_text(digital_track['artist'])
 94:             dig_title = self._normalize_text(digital_track['title'])
 95:             dig_artist_title = f"{dig_artist} {dig_title}"
 96:             artist_score = fuzz.ratio(phys_artist, dig_artist)
 97:             title_score = fuzz.ratio(phys_title, dig_title)
 98:             combined_score = fuzz.ratio(phys_artist_title, dig_artist_title)
 99:             weighted_score = (title_score * 0.6) + (artist_score * 0.3) + (combined_score * 0.1)
100:             if weighted_score > best_confidence:
101:                 best_confidence = weighted_score
102:                 best_match_data = digital_track
103:                 best_match = {
104:                     'digital_artist': digital_track['artist'],
105:                     'digital_title': digital_track['title'],
106:                     'digital_album': digital_track['album'],
107:                     'digital_genre': digital_track['genre'],
108:                     'digital_bpm': digital_track['bpm'],
109:                     'artist_score': artist_score,
110:                     'title_score': title_score,
111:                     'combined_score': combined_score
112:                 }
113:                 if weighted_score >= 95:
114:                     logger.debug(f"Near-perfect match found ({weighted_score:.1f}%), stopping search")
115:                     break
116:         if best_confidence >= confidence_threshold:
117:             status = 'found'
118:             status_reason = f"Matched with {best_confidence:.1f}% confidence"
119:         else:
120:             status = 'missing'
121:             if best_confidence > 50:
122:                 status_reason = f"Possible match at {best_confidence:.1f}% confidence (below threshold)"
123:             else:
124:                 status_reason = "No good matches found"
125:         result = {
126:             'physical_artist': physical_track['artist'],
127:             'physical_title': physical_track['title'],
128:             'physical_album': physical_track['album'],
129:             'physical_label': physical_track['label'],
130:             'physical_format': physical_track['format_type'],
131:             'physical_year': physical_track['release_year'],
132:             'physical_catalog': physical_track['catalog_number'],
133:             'status': status,
134:             'confidence': best_confidence,
135:             'status_reason': status_reason,
136:             'digital_artist': best_match['digital_artist'] if best_match else '',
137:             'digital_title': best_match['digital_title'] if best_match else '',
138:             'digital_album': best_match['digital_album'] if best_match else '',
139:             'digital_genre': best_match['digital_genre'] if best_match else '',
140:             'digital_bpm': best_match['digital_bpm'] if best_match else 0,
141:             'artist_score': best_match['artist_score'] if best_match else 0,
142:             'title_score': best_match['title_score'] if best_match else 0,
143:             'combined_score': best_match['combined_score'] if best_match else 0
144:         }
145:         return result
146:     def _normalize_text(self, text: str) -> str:
147:         if not text or pd.isna(text):
148:             return ''
149:         text = str(text).lower()
150:         text = re.sub(r'\b(the|a|an)\b', '', text)
151:         text = re.sub(r'\(.*?\)', '', text)
152:         text = re.sub(r'\[.*?\]', '', text)
153:         text = re.sub(r'[^\w\s]', ' ', text)
154:         text = re.sub(r'\s+', ' ', text)
155:         text = text.strip()
156:         return text
157:     def _load_digital_collection(self) -> pd.DataFrame:
158:         try:
159:             parser = NMLParser(self.nml_path)
160:             df = parser.parse()
161:             logger.info(f"Loaded {len(df)} digital tracks from NML")
162:             return df
163:         except Exception as e:
164:             logger.error(f"Error loading digital collection: {e}")
165:             return pd.DataFrame()
166:     def _load_physical_collection(self) -> pd.DataFrame:
167:         try:
168:             api_key = load_api_key_from_env()
169:             if not api_key:
170:                 logger.error("No API key found for loading physical collection")
171:                 return pd.DataFrame()
172:             expander = CollectionExpander(api_key, self.db_path)
173:             df = expander.load_physical_collection()
174:             logger.info(f"Loaded {len(df)} physical tracks from database")
175:             return df
176:         except Exception as e:
177:             logger.error(f"Error loading physical collection: {e}")
178:             return pd.DataFrame()
</file>

<file path="src/python/core/gap_analyzer.py">
  1: import pandas as pd
  2: from typing import Dict, List, Tuple, Optional
  3: import logging
  4: from fuzzywuzzy import fuzz
  5: from fuzzywuzzy import process
  6: import re
  7: from .nml_parser import NMLParser
  8: from .collection_expander import CollectionExpander, load_api_key_from_env
  9: logger = logging.getLogger(__name__)
 10: class GapAnalyzer:
 11:     def __init__(self, nml_path: str, db_path: str = "./data/musictool.db"):
 12:         self.nml_path = nml_path
 13:         self.db_path = db_path
 14:         self.digital_collection = self._load_digital_collection()
 15:         self.physical_collection = self._load_physical_collection()
 16:         logger.info(f"Gap Analyzer initialized:")
 17:         logger.info(f"  Digital tracks: {len(self.digital_collection)}")
 18:         logger.info(f"  Physical tracks: {len(self.physical_collection)}")
 19:     def find_gaps(self, confidence_threshold: int = 80) -> pd.DataFrame:
 20:         logger.info(f"Starting gap analysis (confidence threshold: {confidence_threshold}%)")
 21:         if self.physical_collection.empty:
 22:             logger.warning("No physical collection data found")
 23:             return pd.DataFrame()
 24:         if self.digital_collection.empty:
 25:             logger.warning("No digital collection data found")
 26:             return pd.DataFrame()
 27:         gap_results = []
 28:         for idx, physical_track in self.physical_collection.iterrows():
 29:             result = self._find_track_in_digital(physical_track, confidence_threshold)
 30:             gap_results.append(result)
 31:         gap_df = pd.DataFrame(gap_results)
 32:         total_tracks = len(gap_df)
 33:         found_tracks = len(gap_df[gap_df['status'] == 'found'])
 34:         missing_tracks = len(gap_df[gap_df['status'] == 'missing'])
 35:         logger.info(f"üéØ Gap analysis complete:")
 36:         logger.info(f"  Total physical tracks: {total_tracks}")
 37:         logger.info(f"  Found in digital: {found_tracks} ({found_tracks/total_tracks*100:.1f}%)")
 38:         logger.info(f"  Missing from digital: {missing_tracks} ({missing_tracks/total_tracks*100:.1f}%)")
 39:         return gap_df
 40:     def _find_track_in_digital(self, physical_track: pd.Series, confidence_threshold: int) -> Dict:
 41:         phys_artist = self._normalize_text(physical_track['artist'])
 42:         phys_title = self._normalize_text(physical_track['title'])
 43:         phys_artist_title = f"{phys_artist} {phys_title}"
 44:         best_match = None
 45:         best_confidence = 0
 46:         best_match_data = None
 47:         for idx, digital_track in self.digital_collection.iterrows():
 48:             dig_artist = self._normalize_text(digital_track['artist'])
 49:             dig_title = self._normalize_text(digital_track['title'])
 50:             dig_artist_title = f"{dig_artist} {dig_title}"
 51:             artist_score = fuzz.ratio(phys_artist, dig_artist)
 52:             title_score = fuzz.ratio(phys_title, dig_title)
 53:             combined_score = fuzz.ratio(phys_artist_title, dig_artist_title)
 54:             weighted_score = (title_score * 0.6) + (artist_score * 0.3) + (combined_score * 0.1)
 55:             if weighted_score > best_confidence:
 56:                 best_confidence = weighted_score
 57:                 best_match_data = digital_track
 58:                 best_match = {
 59:                     'digital_artist': digital_track['artist'],
 60:                     'digital_title': digital_track['title'],
 61:                     'digital_album': digital_track['album'],
 62:                     'digital_genre': digital_track['genre'],
 63:                     'digital_bpm': digital_track['bpm'],
 64:                     'artist_score': artist_score,
 65:                     'title_score': title_score,
 66:                     'combined_score': combined_score
 67:                 }
 68:         if best_confidence >= confidence_threshold:
 69:             status = 'found'
 70:             status_reason = f"Matched with {best_confidence:.1f}% confidence"
 71:         else:
 72:             status = 'missing'
 73:             if best_confidence > 50:
 74:                 status_reason = f"Possible match at {best_confidence:.1f}% confidence (below threshold)"
 75:             else:
 76:                 status_reason = "No good matches found"
 77:         result = {
 78:             'physical_artist': physical_track['artist'],
 79:             'physical_title': physical_track['title'],
 80:             'physical_album': physical_track['album'],
 81:             'physical_label': physical_track['label'],
 82:             'physical_format': physical_track['format_type'],
 83:             'physical_year': physical_track['release_year'],
 84:             'physical_catalog': physical_track['catalog_number'],
 85:             'status': status,
 86:             'confidence': best_confidence,
 87:             'status_reason': status_reason,
 88:             'digital_artist': best_match['digital_artist'] if best_match else '',
 89:             'digital_title': best_match['digital_title'] if best_match else '',
 90:             'digital_album': best_match['digital_album'] if best_match else '',
 91:             'digital_genre': best_match['digital_genre'] if best_match else '',
 92:             'digital_bpm': best_match['digital_bpm'] if best_match else 0,
 93:             'artist_score': best_match['artist_score'] if best_match else 0,
 94:             'title_score': best_match['title_score'] if best_match else 0,
 95:             'combined_score': best_match['combined_score'] if best_match else 0
 96:         }
 97:         return result
 98:     def _normalize_text(self, text: str) -> str:
 99:         if not text or pd.isna(text):
100:             return ''
101:         text = str(text).lower()
102:         text = re.sub(r'\b(the|a|an)\b', '', text)
103:         text = re.sub(r'\(.*?\)', '', text)
104:         text = re.sub(r'\[.*?\]', '', text)
105:         text = re.sub(r'[^\w\s]', ' ', text)
106:         text = re.sub(r'\s+', ' ', text)
107:         text = text.strip()
108:         return text
109:     def _load_digital_collection(self) -> pd.DataFrame:
110:         try:
111:             parser = NMLParser(self.nml_path)
112:             df = parser.parse()
113:             logger.info(f"Loaded {len(df)} digital tracks from NML")
114:             return df
115:         except Exception as e:
116:             logger.error(f"Error loading digital collection: {e}")
117:             return pd.DataFrame()
118:     def _load_physical_collection(self) -> pd.DataFrame:
119:         try:
120:             api_key = load_api_key_from_env()
121:             if not api_key:
122:                 logger.error("No API key found for loading physical collection")
123:                 return pd.DataFrame()
124:             expander = CollectionExpander(api_key, self.db_path)
125:             df = expander.load_physical_collection()
126:             logger.info(f"Loaded {len(df)} physical tracks from database")
127:             return df
128:         except Exception as e:
129:             logger.error(f"Error loading physical collection: {e}")
130:             return pd.DataFrame()
131:     def get_summary_stats(self, gap_df: pd.DataFrame) -> Dict:
132:         if gap_df.empty:
133:             return {}
134:         total = len(gap_df)
135:         found = len(gap_df[gap_df['status'] == 'found'])
136:         missing = len(gap_df[gap_df['status'] == 'missing'])
137:         missing_tracks = gap_df[gap_df['status'] == 'missing']
138:         stats = {
139:             'total_physical_tracks': total,
140:             'found_in_digital': found,
141:             'missing_from_digital': missing,
142:             'found_percentage': (found / total * 100) if total > 0 else 0,
143:             'missing_percentage': (missing / total * 100) if total > 0 else 0,
144:             'average_confidence': gap_df['confidence'].mean(),
145:             'top_missing_artists': missing_tracks['physical_artist'].value_counts().head().to_dict(),
146:             'top_missing_labels': missing_tracks['physical_label'].value_counts().head().to_dict(),
147:             'missing_by_format': missing_tracks['physical_format'].value_counts().to_dict(),
148:             'missing_by_decade': self._group_by_decade(missing_tracks['physical_year']).to_dict()
149:         }
150:         return stats
151:     def _group_by_decade(self, years: pd.Series) -> pd.Series:
152:         decades = (years // 10) * 10
153:         return decades.value_counts().sort_index()
154: def analyze_gaps_cli(nml_path: str = "./data/collection.nml", confidence: int = 80):
155:     print("üéØ Starting Gap Analysis...")
156:     print(f"   Confidence threshold: {confidence}%")
157:     try:
158:         analyzer = GapAnalyzer(nml_path)
159:         gap_results = analyzer.find_gaps(confidence)
160:         if not gap_results.empty:
161:             print(f"\nüìä Gap Analysis Results:")
162:             stats = analyzer.get_summary_stats(gap_results)
163:             print(f"   Found: {stats['found_in_digital']}/{stats['total_physical_tracks']} ({stats['found_percentage']:.1f}%)")
164:             print(f"   Missing: {stats['missing_from_digital']}/{stats['total_physical_tracks']} ({stats['missing_percentage']:.1f}%)")
165:             print(f"   Average confidence: {stats['average_confidence']:.1f}%")
166:             print(f"\nüéµ Sample Results:")
167:             display_cols = ['physical_artist', 'physical_title', 'status', 'confidence', 'digital_title']
168:             print(gap_results[display_cols].head(10).to_string(index=False))
169:             missing = gap_results[gap_results['status'] == 'missing']
170:             if not missing.empty:
171:                 print(f"\n‚ùå Missing Tracks ({len(missing)}):")
172:                 missing_display = missing[['physical_artist', 'physical_title', 'physical_album']].head(10)
173:                 print(missing_display.to_string(index=False))
174:             return gap_results
175:         else:
176:             print("‚ùå No gap analysis results generated")
177:             return None
178:     except Exception as e:
179:         print(f"‚ùå Error during gap analysis: {e}")
180:         import traceback
181:         traceback.print_exc()
182:         return None
183: if __name__ == "__main__":
184:     import sys
185:     confidence = int(sys.argv[1]) if len(sys.argv) > 1 else 80
186:     analyze_gaps_cli(confidence=confidence)
</file>

<file path="src/python/core/nml_parser.py">
  1: import pandas as pd
  2: from lxml import etree
  3: from pathlib import Path
  4: from typing import List, Dict, Optional
  5: import logging
  6: logger = logging.getLogger(__name__)
  7: class NMLParser:
  8:     def __init__(self, nml_file_path: str):
  9:         self.nml_file_path = Path(nml_file_path)
 10:         if not self.nml_file_path.exists():
 11:             raise FileNotFoundError(f"NML file not found: {nml_file_path}")
 12:     def parse(self) -> pd.DataFrame:
 13:         logger.info(f"Parsing NML file: {self.nml_file_path}")
 14:         try:
 15:             tree = etree.parse(str(self.nml_file_path))
 16:             root = tree.getroot()
 17:             entries = root.xpath("//ENTRY")
 18:             logger.info(f"Found {len(entries)} tracks in NML file")
 19:             tracks = []
 20:             for entry in entries:
 21:                 track_data = self._extract_track_data(entry)
 22:                 if track_data:
 23:                     tracks.append(track_data)
 24:             df = pd.DataFrame(tracks)
 25:             logger.info(f"Successfully parsed {len(df)} tracks")
 26:             return df
 27:         except Exception as e:
 28:             logger.error(f"Error parsing NML file: {e}")
 29:             raise
 30:     def _extract_track_data(self, entry) -> Optional[Dict]:
 31:         try:
 32:             artist = entry.get('ARTIST', '').strip()
 33:             title = entry.get('TITLE', '').strip()
 34:             if not artist or not title:
 35:                 return None
 36:             album_elem = entry.find('ALBUM')
 37:             album = album_elem.get('TITLE', '') if album_elem is not None else ''
 38:             album_track = album_elem.get('TRACK', '') if album_elem is not None else ''
 39:             location_elem = entry.find('LOCATION')
 40:             file_path = ''
 41:             if location_elem is not None:
 42:                 directory = location_elem.get('DIR', '').replace('/:', '/')
 43:                 filename = location_elem.get('FILE', '')
 44:                 file_path = f"{directory}{filename}" if directory and filename else ''
 45:             info_elem = entry.find('INFO')
 46:             genre = ''
 47:             label = ''
 48:             playtime = 0
 49:             release_date = ''
 50:             play_count = 0
 51:             last_played = ''
 52:             filesize = 0
 53:             if info_elem is not None:
 54:                 genre = info_elem.get('GENRE', '')
 55:                 label = info_elem.get('LABEL', '')
 56:                 playtime = int(info_elem.get('PLAYTIME', '0'))
 57:                 release_date = info_elem.get('RELEASE_DATE', '')
 58:                 play_count = int(info_elem.get('PLAYCOUNT', '0'))
 59:                 last_played = info_elem.get('LAST_PLAYED', '')
 60:                 filesize = int(info_elem.get('FILESIZE', '0'))
 61:             tempo_elem = entry.find('TEMPO')
 62:             bpm = 0.0
 63:             if tempo_elem is not None:
 64:                 bpm = float(tempo_elem.get('BPM', '0'))
 65:             key_elem = entry.find('MUSICAL_KEY')
 66:             key = ''
 67:             if key_elem is not None:
 68:                 key_value = key_elem.get('VALUE', '')
 69:                 key = self._convert_key_value(key_value)
 70:             filetype = ''
 71:             if file_path:
 72:                 filetype = self._extract_filetype(file_path)
 73:             return {
 74:                 'artist': artist,
 75:                 'title': title,
 76:                 'album': album,
 77:                 'album_track': album_track,
 78:                 'genre': genre,
 79:                 'label': label,
 80:                 'bpm': bpm,
 81:                 'key': key,
 82:                 'playtime': playtime,
 83:                 'file_path': file_path,
 84:                 'release_date': release_date,
 85:                 'play_count': play_count,
 86:                 'last_played': last_played,
 87:                 'filesize': filesize,
 88:                 'filetype': filetype
 89:             }
 90:         except Exception as e:
 91:             logger.warning(f"Error extracting track data: {e}")
 92:             return None
 93:     def _convert_key_value(self, key_value: str) -> str:
 94:         if not key_value:
 95:             return ''
 96:         try:
 97:             key_num = int(key_value)
 98:             keys = ['C', 'C
 99:             if 0 <= key_num <= 11:
100:                 return f"{keys[key_num]}maj"
101:             elif 12 <= key_num <= 23:
102:                 return f"{keys[key_num - 12]}min"
103:             else:
104:                 return f"Key{key_value}"
105:         except ValueError:
106:             return key_value
107:     def _extract_filetype(self, file_path: str) -> str:
108:         if not file_path:
109:             return ''
110:         # Get the file extension (everything after the last dot)
111:         if '.' in file_path:
112:             filetype = file_path.split('.')[-1].lower()
113:             return filetype
114:         return ''
115: if __name__ == "__main__":
116:     import sys
117:     if len(sys.argv) > 1:
118:         parser = NMLParser(sys.argv[1])
119:         df = parser.parse()
120:         print(f"Parsed {len(df)} tracks")
121:         print(df.head())
</file>

<file path="src/python/ui/__init__.py">
1: 
</file>

<file path="src/python/ui/streamlit_app.py">
  1: import streamlit as st
  2: import pandas as pd
  3: import plotly.express as px
  4: import plotly.graph_objects as go
  5: from pathlib import Path
  6: import os
  7: import sys
  8: project_root = Path(__file__).parent.parent.parent.parent
  9: sys.path.insert(0, str(project_root))
 10: from src.python.core.nml_parser import NMLParser
 11: from src.python.core.discogs_parser import DiscogsCSVParser
 12: from src.python.core.discogs_client import load_api_key_from_env
 13: from src.python.core.collection_expander import CollectionExpander
 14: from src.python.core.gap_analyzer_fast import FastGapAnalyzer
 15: from src.python.core.duplicate_finder import DuplicateFinder
 16: st.set_page_config(
 17:     page_title="MusicTool - Collection Manager",
 18:     page_icon="üéµ",
 19:     layout="wide",
 20:     initial_sidebar_state="expanded"
 21: )
 22: st.markdown("""
 23: <style>
 24:     .main-header {
 25:         font-size: 3rem;
 26:         font-weight: bold;
 27:         text-align: center;
 28:         background: linear-gradient(90deg, #ff6b6b, #4ecdc4, #45b7d1);
 29:         -webkit-background-clip: text;
 30:         -webkit-text-fill-color: transparent;
 31:         margin-bottom: 2rem;
 32:     }
 33:     .metric-card {
 34:         background-color: #f0f2f6;
 35:         padding: 1rem;
 36:         border-radius: 0.5rem;
 37:         border-left: 4px solid #ff6b6b;
 38:         margin: 0.5rem 0;
 39:     }
 40:     .success-metric {
 41:         border-left-color: #28a745;
 42:     }
 43:     .warning-metric {
 44:         border-left-color: #ffc107;
 45:     }
 46:     .danger-metric {
 47:         border-left-color: #dc3545;
 48:     }
 49:     .stDataFrame {
 50:         border: 1px solid #e0e0e0;
 51:         border-radius: 0.5rem;
 52:     }
 53: </style>
 54: """, unsafe_allow_html=True)
 55: def load_collections():
 56:     @st.cache_data
 57:     def load_digital_collection():
 58:         nml_path = "./data/collection.nml"
 59:         if Path(nml_path).exists():
 60:             parser = NMLParser(nml_path)
 61:             return parser.parse()
 62:         return pd.DataFrame()
 63:     @st.cache_data
 64:     def load_physical_collection():
 65:         api_key = load_api_key_from_env()
 66:         if api_key:
 67:             expander = CollectionExpander(api_key)
 68:             return expander.load_physical_collection()
 69:         return pd.DataFrame()
 70:     return load_digital_collection(), load_physical_collection()
 71: def main():
 72:     st.markdown('<h1 class="main-header">üéµ MusicTool Collection Manager</h1>', unsafe_allow_html=True)
 73:     st.sidebar.title("üéõÔ∏è Navigation")
 74:     page = st.sidebar.selectbox(
 75:         "Choose a page:",
 76:         ["üè† Dashboard", "üíø Digital Collection", "üìÄ Physical Collection", "üîç Gap Analysis", "üîÑ Duplicate Finder", "‚öôÔ∏è Tools"]
 77:     )
 78:     with st.spinner("Loading collections..."):
 79:         digital_df, physical_df = load_collections()
 80:     if page == "üè† Dashboard":
 81:         show_dashboard(digital_df, physical_df)
 82:     elif page == "üíø Digital Collection":
 83:         show_digital_collection(digital_df)
 84:     elif page == "üìÄ Physical Collection":
 85:         show_physical_collection(physical_df)
 86:     elif page == "üîç Gap Analysis":
 87:         show_gap_analysis(digital_df, physical_df)
 88:     elif page == "üîÑ Duplicate Finder":
 89:         show_duplicate_finder(digital_df)
 90:     elif page == "‚öôÔ∏è Tools":
 91:         show_tools()
 92: def show_dashboard(digital_df, physical_df):
 93:     st.header("üìä Collection Overview")
 94:     col1, col2, col3, col4 = st.columns(4)
 95:     with col1:
 96:         st.markdown('<div class="metric-card success-metric">', unsafe_allow_html=True)
 97:         st.metric("Digital Tracks", len(digital_df), delta=None)
 98:         st.markdown('</div>', unsafe_allow_html=True)
 99:     with col2:
100:         st.markdown('<div class="metric-card warning-metric">', unsafe_allow_html=True)
101:         st.metric("Physical Tracks", len(physical_df), delta=None)
102:         st.markdown('</div>', unsafe_allow_html=True)
103:     with col3:
104:         if not digital_df.empty:
105:             unique_artists = digital_df['artist'].nunique()
106:             st.markdown('<div class="metric-card">', unsafe_allow_html=True)
107:             st.metric("Digital Artists", unique_artists)
108:             st.markdown('</div>', unsafe_allow_html=True)
109:         else:
110:             st.metric("Digital Artists", "N/A")
111:     with col4:
112:         if not physical_df.empty:
113:             unique_releases = physical_df['discogs_release_id'].nunique()
114:             st.markdown('<div class="metric-card">', unsafe_allow_html=True)
115:             st.metric("Physical Releases", unique_releases)
116:             st.markdown('</div>', unsafe_allow_html=True)
117:         else:
118:             st.metric("Physical Releases", "N/A")
119:     if not digital_df.empty:
120:         col1, col2 = st.columns(2)
121:         with col1:
122:             st.subheader("üéµ Digital Collection by Genre")
123:             if 'genre' in digital_df.columns:
124:                 genre_counts = digital_df['genre'].value_counts().head(10)
125:                 fig = px.bar(
126:                     x=genre_counts.values,
127:                     y=genre_counts.index,
128:                     orientation='h',
129:                     title="Top 10 Genres",
130:                     color=genre_counts.values,
131:                     color_continuous_scale="viridis"
132:                 )
133:                 fig.update_layout(height=400, showlegend=False)
134:                 st.plotly_chart(fig, use_container_width=True)
135:         with col2:
136:             st.subheader("üìÄ File Format Distribution")
137:             if 'filetype' in digital_df.columns:
138:                 format_counts = digital_df['filetype'].value_counts()
139:                 fig = px.pie(
140:                     values=format_counts.values,
141:                     names=format_counts.index,
142:                     title="Audio Formats",
143:                     color_discrete_sequence=px.colors.qualitative.Set3
144:                 )
145:                 fig.update_layout(height=400)
146:                 st.plotly_chart(fig, use_container_width=True)
147:     if not physical_df.empty and 'date_added' in physical_df.columns:
148:         st.subheader("üìÖ Recent Physical Additions")
149:         recent_df = physical_df.sort_values('date_added', ascending=False).head(5)
150:         st.dataframe(
151:             recent_df[['artist', 'title', 'album', 'date_added', 'format_type']],
152:             use_container_width=True
153:         )
154: def show_digital_collection(digital_df):
155:     st.header("üíø Digital Collection")
156:     if digital_df.empty:
157:         st.warning("No digital collection data found. Please check your NML file.")
158:         return
159:     st.subheader("üîç Filters")
160:     col1, col2, col3 = st.columns(3)
161:     with col1:
162:         artists = ['All'] + sorted(digital_df['artist'].unique().tolist())
163:         selected_artist = st.selectbox("Artist:", artists)
164:     with col2:
165:         genres = ['All'] + sorted(digital_df['genre'].unique().tolist())
166:         selected_genre = st.selectbox("Genre:", genres)
167:     with col3:
168:         if digital_df['bpm'].max() > 0:
169:             min_bpm, max_bpm = st.slider(
170:                 "BPM Range:",
171:                 min_value=int(digital_df['bpm'].min()),
172:                 max_value=int(digital_df['bpm'].max()),
173:                 value=(int(digital_df['bpm'].min()), int(digital_df['bpm'].max()))
174:             )
175:         else:
176:             min_bpm, max_bpm = 0, 200
177:     filtered_df = digital_df.copy()
178:     if selected_artist != 'All':
179:         filtered_df = filtered_df[filtered_df['artist'] == selected_artist]
180:     if selected_genre != 'All':
181:         filtered_df = filtered_df[filtered_df['genre'] == selected_genre]
182:     filtered_df = filtered_df[
183:         (filtered_df['bpm'] >= min_bpm) & (filtered_df['bpm'] <= max_bpm)
184:     ]
185:     st.subheader(f"üìä Results ({len(filtered_df)} tracks)")
186:     display_columns = st.multiselect(
187:         "Choose columns to display:",
188:         options=['artist', 'title', 'album', 'genre', 'bpm', 'key', 'playtime', 'filetype'],
189:         default=['artist', 'title', 'album', 'genre', 'bpm']
190:     )
191:     if display_columns:
192:         column_config = {
193:             'bpm': st.column_config.NumberColumn("BPM", format="%.1f"),
194:             'playtime': st.column_config.NumberColumn("Duration (s)", format="%d"),
195:             'filetype': st.column_config.SelectboxColumn("Format", options=['mp3', 'flac', 'aiff', 'wav'])
196:         }
197:         st.dataframe(
198:             filtered_df[display_columns],
199:             column_config=column_config,
200:             use_container_width=True,
201:             height=400
202:         )
203:         csv = filtered_df[display_columns].to_csv(index=False)
204:         st.download_button(
205:             label="üì• Download as CSV",
206:             data=csv,
207:             file_name=f"digital_collection_{len(filtered_df)}_tracks.csv",
208:             mime="text/csv"
209:         )
210: def show_physical_collection(physical_df):
211:     st.header("üìÄ Physical Collection")
212:     if physical_df.empty:
213:         st.warning("No physical collection data found. Please expand your collection first.")
214:         return
215:     col1, col2, col3 = st.columns(3)
216:     with col1:
217:         st.metric("Total Tracks", len(physical_df))
218:     with col2:
219:         unique_albums = physical_df['album'].nunique()
220:         st.metric("Unique Albums", unique_albums)
221:     with col3:
222:         unique_labels = physical_df['label'].nunique()
223:         st.metric("Record Labels", unique_labels)
224:     st.subheader("üîç Filters")
225:     col1, col2, col3 = st.columns(3)
226:     with col1:
227:         formats = ['All'] + sorted(physical_df['format_type'].unique().tolist())
228:         selected_format = st.selectbox("Format:", formats)
229:     with col2:
230:         labels = ['All'] + sorted(physical_df['label'].unique().tolist())
231:         selected_label = st.selectbox("Label:", labels)
232:     with col3:
233:         if physical_df['release_year'].max() > 0:
234:             years = physical_df[physical_df['release_year'] > 0]['release_year']
235:             min_year, max_year = st.slider(
236:                 "Release Year:",
237:                 min_value=int(years.min()),
238:                 max_value=int(years.max()),
239:                 value=(int(years.min()), int(years.max()))
240:             )
241:         else:
242:             min_year, max_year = 1990, 2025
243:     filtered_df = physical_df.copy()
244:     if selected_format != 'All':
245:         filtered_df = filtered_df[filtered_df['format_type'] == selected_format]
246:     if selected_label != 'All':
247:         filtered_df = filtered_df[filtered_df['label'] == selected_label]
248:     filtered_df = filtered_df[
249:         (filtered_df['release_year'] >= min_year) & (filtered_df['release_year'] <= max_year)
250:     ]
251:     st.subheader(f"üìä Results ({len(filtered_df)} tracks)")
252:     display_columns = ['artist', 'title', 'album', 'label', 'format_type', 'release_year', 'catalog_number']
253:     st.dataframe(
254:         filtered_df[display_columns],
255:         use_container_width=True,
256:         height=400
257:     )
258:     csv = filtered_df[display_columns].to_csv(index=False)
259:     st.download_button(
260:         label="üì• Download as CSV",
261:         data=csv,
262:         file_name=f"physical_collection_{len(filtered_df)}_tracks.csv",
263:         mime="text/csv"
264:     )
265: def show_gap_analysis(digital_df, physical_df):
266:     st.header("üîç Gap Analysis")
267:     if digital_df.empty or physical_df.empty:
268:         st.warning("Both digital and physical collections are needed for gap analysis.")
269:         return
270:     st.subheader("‚öôÔ∏è Analysis Options")
271:     col1, col2, col3 = st.columns(3)
272:     with col1:
273:         confidence_threshold = st.slider("Confidence Threshold", 50, 95, 80, 5)
274:         st.caption("Minimum match confidence to consider a track 'found'")
275:     with col2:
276:         analysis_mode = st.selectbox(
277:             "Analysis Mode",
278:             ["üöÄ Fast Sample (100 tracks)", "üîç Complete Analysis", "üéØ Custom Range"]
279:         )
280:     with col3:
281:         if analysis_mode == "üéØ Custom Range":
282:             max_tracks = st.number_input("Max tracks to analyze", 50, len(physical_df), 500)
283:         else:
284:             max_tracks = 100 if "Sample" in analysis_mode else len(physical_df)
285:     estimated_time = max_tracks * 0.1
286:     st.info(f"üìä Will analyze {max_tracks:,} tracks (estimated time: {estimated_time:.1f}s)")
287:     if st.button(f"üöÄ Run {analysis_mode}", type="primary"):
288:         if max_tracks < len(physical_df):
289:             physical_subset = physical_df.head(max_tracks)
290:             st.info(f"Analyzing first {max_tracks} tracks out of {len(physical_df):,} total")
291:         else:
292:             physical_subset = physical_df
293:         progress_bar = st.progress(0)
294:         status_text = st.empty()
295:         try:
296:             status_text.text("üîÑ Initializing Fast Gap Analyzer...")
297:             analyzer = FastGapAnalyzer("./data/collection.nml")
298:             status_text.text("üîç Running gap analysis...")
299:             gap_results = analyzer.find_gaps(confidence_threshold=confidence_threshold)
300:             if max_tracks < len(physical_df):
301:                 # Get the track identifiers from our subset
302:                 subset_ids = set(physical_subset.index)
303:                 # This is simplified - in practice you'd need better matching
304:                 gap_results = gap_results.head(max_tracks)
305:             progress_bar.progress(1.0)
306:             status_text.text("‚úÖ Analysis complete!")
307:             st.session_state.gap_results = gap_results
308:             st.session_state.analysis_config = {
309:                 'confidence_threshold': confidence_threshold,
310:                 'tracks_analyzed': len(gap_results),
311:                 'mode': analysis_mode
312:             }
313:         except Exception as e:
314:             st.error(f"‚ùå Error during gap analysis: {e}")
315:             return
316:     if 'gap_results' in st.session_state:
317:         gap_results = st.session_state.gap_results
318:         config = st.session_state.analysis_config
319:         if not gap_results.empty:
320:                 total = len(gap_results)
321:                 found = len(gap_results[gap_results['status'] == 'found'])
322:                 missing = len(gap_results[gap_results['status'] == 'missing'])
323:                 col1, col2, col3 = st.columns(3)
324:                 with col1:
325:                     st.markdown('<div class="metric-card success-metric">', unsafe_allow_html=True)
326:                     st.metric("Found in Digital", f"{found}/{total}", f"{found/total*100:.1f}%")
327:                     st.markdown('</div>', unsafe_allow_html=True)
328:                 with col2:
329:                     st.markdown('<div class="metric-card danger-metric">', unsafe_allow_html=True)
330:                     st.metric("Missing from Digital", f"{missing}/{total}", f"{missing/total*100:.1f}%")
331:                     st.markdown('</div>', unsafe_allow_html=True)
332:                 with col3:
333:                     avg_confidence = gap_results['confidence'].mean()
334:                     st.markdown('<div class="metric-card">', unsafe_allow_html=True)
335:                     st.metric("Average Confidence", f"{avg_confidence:.1f}%")
336:                     st.markdown('</div>', unsafe_allow_html=True)
337:                 tab1, tab2, tab3 = st.tabs(["üìä All Results", "‚úÖ Found Tracks", "‚ùå Missing Tracks"])
338:                 with tab1:
339:                     st.subheader("Complete Gap Analysis Results")
340:                     def color_status(val):
341:                         if val == 'found':
342:                             return 'background-color: #d4edda'
343:                         else:
344:                             return 'background-color: #f8d7da'
345:                     display_cols = ['physical_artist', 'physical_title', 'physical_album',
346:                                   'status', 'confidence', 'digital_title']
347:                     styled_df = gap_results[display_cols].style.applymap(
348:                         color_status, subset=['status']
349:                     )
350:                     st.dataframe(styled_df, use_container_width=True, height=400)
351:                 with tab2:
352:                     found_tracks = gap_results[gap_results['status'] == 'found']
353:                     if not found_tracks.empty:
354:                         st.subheader(f"‚úÖ Found Tracks ({len(found_tracks)})")
355:                         for idx, track in found_tracks.iterrows():
356:                             with st.expander(f"üéµ {track['physical_artist']} - {track['physical_title']}"):
357:                                 col1, col2 = st.columns(2)
358:                                 with col1:
359:                                     st.write("**Physical:**")
360:                                     st.write(f"Artist: {track['physical_artist']}")
361:                                     st.write(f"Title: {track['physical_title']}")
362:                                     st.write(f"Album: {track['physical_album']}")
363:                                     st.write(f"Label: {track['physical_label']}")
364:                                 with col2:
365:                                     st.write("**Digital Match:**")
366:                                     st.write(f"Artist: {track['digital_artist']}")
367:                                     st.write(f"Title: {track['digital_title']}")
368:                                     st.write(f"Album: {track['digital_album']}")
369:                                     st.write(f"Confidence: {track['confidence']:.1f}%")
370:                     else:
371:                         st.info("No tracks found in digital collection.")
372:                 with tab3:
373:                     missing_tracks = gap_results[gap_results['status'] == 'missing']
374:                     if not missing_tracks.empty:
375:                         st.subheader(f"‚ùå Missing Tracks ({len(missing_tracks)})")
376:                         albums = missing_tracks['physical_album'].unique()
377:                         for album in albums:
378:                             album_tracks = missing_tracks[missing_tracks['physical_album'] == album]
379:                             with st.expander(f"üìÄ {album} ({len(album_tracks)} tracks missing)"):
380:                                 st.dataframe(
381:                                     album_tracks[['physical_artist', 'physical_title', 'physical_label', 'confidence']],
382:                                     use_container_width=True
383:                                 )
384:                         csv = missing_tracks[['physical_artist', 'physical_title', 'physical_album',
385:                                             'physical_label', 'physical_catalog']].to_csv(index=False)
386:                         st.download_button(
387:                             label="üì• Download Missing Tracks",
388:                             data=csv,
389:                             file_name=f"missing_tracks_{len(missing_tracks)}.csv",
390:                             mime="text/csv"
391:                         )
392:                     else:
393:                         st.success("üéâ All physical tracks found in digital collection!")
394:         else:
395:             st.info("Click 'Run Analysis' to start the gap analysis.")
396: def show_duplicate_finder(digital_df):
397:     st.header("üîÑ Duplicate Finder")
398:     if digital_df.empty:
399:         st.warning("Digital collection is needed for duplicate detection.")
400:         return
401:     st.info(f"üéµ Scanning {len(digital_df):,} tracks in your digital collection for potential duplicates")
402:     st.subheader("‚öôÔ∏è Detection Settings")
403:     col1, col2 = st.columns(2)
404:     with col1:
405:         similarity_threshold = st.slider(
406:             "Similarity Threshold",
407:             60, 95, 85, 5,
408:             help="Higher values = more strict matching (fewer false positives)"
409:         )
410:         st.caption(f"Tracks with {similarity_threshold}%+ similarity will be considered duplicates")
411:     with col2:
412:         detection_method = st.selectbox(
413:             "Detection Method",
414:             [
415:                 "artist_title",
416:                 "title_only",
417:                 "filename",
418:                 "duration"
419:             ],
420:             format_func=lambda x: {
421:                 "artist_title": "üéµ Artist + Title (recommended)",
422:                 "title_only": "üéº Title Only",
423:                 "filename": "üìÅ Filename Similarity",
424:                 "duration": "‚è±Ô∏è Duration + Title"
425:             }[x]
426:         )
427:     method_descriptions = {
428:         "artist_title": "Compares both artist and title using fuzzy matching. Most reliable for finding true duplicates.",
429:         "title_only": "Only compares track titles. Good for finding covers or different versions of the same song.",
430:         "filename": "Compares file names. Useful for finding files with similar naming patterns.",
431:         "duration": "Combines title similarity with track duration. Good for identifying identical recordings."
432:     }
433:     st.caption(f"‚ÑπÔ∏è {method_descriptions[detection_method]}")
434:     estimated_comparisons = (len(digital_df) * (len(digital_df) - 1)) // 2
435:     estimated_time = estimated_comparisons / 10000
436:     if estimated_time > 60:
437:         st.warning(f"‚ö†Ô∏è Large collection detected. Estimated processing time: {estimated_time/60:.1f} minutes")
438:     else:
439:         st.info(f"üìä Will perform ~{estimated_comparisons:,} comparisons (estimated time: {estimated_time:.1f}s)")
440:     if st.button(f"üîç Find Duplicates", type="primary"):
441:         progress_bar = st.progress(0)
442:         status_text = st.empty()
443:         try:
444:             status_text.text("üîÑ Initializing Duplicate Finder...")
445:             finder = DuplicateFinder("./data/collection.nml")
446:             status_text.text("üîç Analyzing tracks for duplicates...")
447:             progress_bar.progress(0.3)
448:             duplicates_df = finder.find_duplicates(
449:                 similarity_threshold=similarity_threshold,
450:                 group_by=detection_method
451:             )
452:             progress_bar.progress(1.0)
453:             status_text.text("‚úÖ Duplicate detection complete!")
454:             st.session_state.duplicates_results = duplicates_df
455:             st.session_state.duplicate_stats = finder.get_duplicate_stats(duplicates_df)
456:             st.session_state.duplicate_config = {
457:                 'threshold': similarity_threshold,
458:                 'method': detection_method,
459:                 'total_tracks': len(digital_df)
460:             }
461:         except Exception as e:
462:             st.error(f"‚ùå Error during duplicate detection: {e}")
463:             return
464:     if 'duplicates_results' in st.session_state:
465:         duplicates_df = st.session_state.duplicates_results
466:         stats = st.session_state.duplicate_stats
467:         config = st.session_state.duplicate_config
468:         if not duplicates_df.empty:
469:             st.success(f"üéØ Found {stats['total_groups']} duplicate groups with {stats['total_duplicate_tracks']} duplicate tracks!")
470:             col1, col2, col3, col4 = st.columns(4)
471:             with col1:
472:                 st.markdown('<div class="metric-card">', unsafe_allow_html=True)
473:                 st.metric("Duplicate Groups", stats['total_groups'])
474:                 st.markdown('</div>', unsafe_allow_html=True)
475:             with col2:
476:                 st.markdown('<div class="metric-card danger-metric">', unsafe_allow_html=True)
477:                 st.metric("Duplicate Tracks", stats['total_duplicate_tracks'])
478:                 st.markdown('</div>', unsafe_allow_html=True)
479:             with col3:
480:                 st.markdown('<div class="metric-card warning-metric">', unsafe_allow_html=True)
481:                 st.metric("Space to Save", f"{stats['potential_space_saved_mb']:.0f} MB")
482:                 st.markdown('</div>', unsafe_allow_html=True)
483:             with col4:
484:                 st.markdown('<div class="metric-card">', unsafe_allow_html=True)
485:                 st.metric("Avg Similarity", f"{stats['average_similarity']:.1f}%")
486:                 st.markdown('</div>', unsafe_allow_html=True)
487:             tab1, tab2, tab3 = st.tabs(["üìä All Duplicates", "üîç By Group", "üìà Statistics"])
488:             with tab1:
489:                 st.subheader("Complete Duplicate Results")
490:                 def color_by_group(row):
491:                     if row['status'] == 'original':
492:                         return ['background-color: #d4edda'] * len(row)
493:                     else:
494:                         colors = ['#f8d7da', '#fff3cd', '#d1ecf1', '#e2e3e5', '#f8f9fa']
495:                         color_idx = (row['group_id'] - 1) % len(colors)
496:                         return [f'background-color: {colors[color_idx]}'] * len(row)
497:                 display_cols = ['group_id', 'rank', 'status', 'similarity', 'artist', 'title', 'album', 'duration', 'filetype']
498:                 styled_df = duplicates_df[display_cols].style.apply(color_by_group, axis=1)
499:                 st.dataframe(styled_df, use_container_width=True, height=400)
500:                 col1, col2 = st.columns(2)
501:                 with col1:
502:                     duplicates_csv = duplicates_df.to_csv(index=False)
503:                     st.download_button(
504:                         label="üì• Download All Results",
505:                         data=duplicates_csv,
506:                         file_name=f"duplicates_all_{len(duplicates_df)}.csv",
507:                         mime="text/csv"
508:                     )
509:                 with col2:
510:                     duplicates_only = duplicates_df[duplicates_df['status'] == 'duplicate']
511:                     if not duplicates_only.empty:
512:                         duplicates_only_csv = duplicates_only.to_csv(index=False)
513:                         st.download_button(
514:                             label="üì• Download Duplicates Only",
515:                             data=duplicates_only_csv,
516:                             file_name=f"duplicates_only_{len(duplicates_only)}.csv",
517:                             mime="text/csv"
518:                         )
519:             with tab2:
520:                 st.subheader("Duplicate Groups")
521:                 group_ids = sorted(duplicates_df['group_id'].unique())
522:                 selected_group = st.selectbox("Select Group:", group_ids, format_func=lambda x: f"Group {x}")
523:                 group_tracks = duplicates_df[duplicates_df['group_id'] == selected_group]
524:                 st.write(f"**Group {selected_group}** - {len(group_tracks)} tracks")
525:                 for idx, track in group_tracks.iterrows():
526:                     status_icon = "üü¢" if track['status'] == 'original' else "üîÑ"
527:                     similarity_color = "green" if track['similarity'] >= 90 else "orange" if track['similarity'] >= 80 else "red"
528:                     with st.expander(f"{status_icon} {track['artist']} - {track['title']} ({track['similarity']:.1f}%)"):
529:                         col1, col2 = st.columns(2)
530:                         with col1:
531:                             st.write("**Track Information:**")
532:                             st.write(f"Artist: {track['artist']}")
533:                             st.write(f"Title: {track['title']}")
534:                             st.write(f"Album: {track['album']}")
535:                             st.write(f"Genre: {track['genre']}")
536:                         with col2:
537:                             st.write("**File Information:**")
538:                             st.write(f"Duration: {track['duration']/1000:.1f}s" if track['duration'] else "Unknown")
539:                             st.write(f"Format: {track['filetype']}")
540:                             st.write(f"Bitrate: {track['bitrate']}" if track['bitrate'] else "Unknown")
541:                             st.write(f"Size: {track['filesize']/1024/1024:.1f} MB" if track['filesize'] else "Unknown")
542:                         if track['location']:
543:                             st.caption(f"üìÅ {track['location']}")
544:             with tab3:
545:                 st.subheader("Duplicate Statistics")
546:                 if stats['top_duplicate_artists']:
547:                     st.write("**üé§ Artists with Most Duplicates:**")
548:                     for artist, count in stats['top_duplicate_artists'].items():
549:                         st.write(f"‚Ä¢ {artist}: {count} duplicates")
550:                 if stats['top_duplicate_albums']:
551:                     st.write("**üíø Albums with Most Duplicates:**")
552:                     for album, count in stats['top_duplicate_albums'].items():
553:                         st.write(f"‚Ä¢ {album}: {count} duplicates")
554:                 if stats['duplicate_formats']:
555:                     st.write("**üìÅ Duplicate File Formats:**")
556:                     format_df = pd.DataFrame(list(stats['duplicate_formats'].items()),
557:                                            columns=['Format', 'Count'])
558:                     fig = px.pie(
559:                         format_df,
560:                         values='Count',
561:                         names='Format',
562:                         title="Duplicate Files by Format"
563:                     )
564:                     st.plotly_chart(fig, use_container_width=True)
565:                 if stats['largest_groups']:
566:                     st.write("**üìä Largest Duplicate Groups:**")
567:                     for group_id, size in stats['largest_groups'].items():
568:                         st.write(f"‚Ä¢ Group {group_id}: {size} tracks")
569:         else:
570:             st.success("üéâ No duplicates found in your collection!")
571:             st.balloons()
572:     else:
573:         st.info("Click 'Find Duplicates' to start scanning your collection.")
574: def show_tools():
575:     st.header("‚öôÔ∏è Collection Tools")
576:     st.subheader("üìà Expand Physical Collection")
577:     st.write("Expand Discogs releases into individual tracks using the Discogs API.")
578:     col1, col2 = st.columns(2)
579:     with col1:
580:         max_releases = st.number_input(
581:             "Max releases to process:",
582:             min_value=1,
583:             max_value=50,
584:             value=10,
585:             help="Start small to test, increase for full expansion"
586:         )
587:     with col2:
588:         if st.button("üöÄ Start Expansion", type="primary"):
589:             api_key = load_api_key_from_env()
590:             if not api_key:
591:                 st.error("‚ùå No Discogs API key found in .env file")
592:             else:
593:                 with st.spinner(f"Expanding {max_releases} releases..."):
594:                     try:
595:                         from src.python.core.collection_expander import expand_collection_cli
596:                         result = expand_collection_cli("./data/gazmazk4ez-collection-20250608-1029.csv", max_releases)
597:                         if result is not None:
598:                             st.success(f"‚úÖ Successfully expanded {len(result)} tracks!")
599:                             st.balloons()
600:                         else:
601:                             st.error("‚ùå Expansion failed")
602:                     except Exception as e:
603:                         st.error(f"Error during expansion: {e}")
604:     st.subheader("‚öôÔ∏è Configuration")
605:     api_key = load_api_key_from_env()
606:     if api_key:
607:         st.success("‚úÖ Discogs API key configured")
608:     else:
609:         st.error("‚ùå Discogs API key not found")
610:         st.info("Add DISCOGS_API_KEY=your_key_here to .env file")
611:     nml_exists = Path("./data/collection.nml").exists()
612:     csv_exists = Path("./data/gazmazk4ez-collection-20250608-1029.csv").exists()
613:     db_exists = Path("./data/musictool.db").exists()
614:     col1, col2, col3 = st.columns(3)
615:     with col1:
616:         if nml_exists:
617:             st.success("‚úÖ NML file found")
618:         else:
619:             st.error("‚ùå NML file missing")
620:     with col2:
621:         if csv_exists:
622:             st.success("‚úÖ Discogs CSV found")
623:         else:
624:             st.error("‚ùå Discogs CSV missing")
625:     with col3:
626:         if db_exists:
627:             st.success("‚úÖ Database exists")
628:         else:
629:             st.warning("‚ö†Ô∏è Database not found")
630: if __name__ == "__main__":
631:     main()
</file>

<file path="src/python/__init__.py">
1: 
</file>

<file path=".env.example">
 1: # MusicTool Configuration Template
 2: # Copy this file to .env and fill in your values
 3: 
 4: # Discogs API Configuration
 5: DISCOGS_API_KEY=your_discogs_api_key_here
 6: DISCOGS_API_BASE_URL=https://api.discogs.com
 7: 
 8: # File Paths (optional - can be set via UI)
 9: # NML_FILE_PATH=/path/to/your/collection.nml
10: # DISCOGS_CSV_PATH=/path/to/your/discogs-export.csv
11: 
12: # Data Storage
13: DATABASE_PATH=./data/musictool.db
14: CACHE_PATH=./data/cache/
15: 
16: # API Rate Limiting
17: DISCOGS_REQUESTS_PER_MINUTE=60
18: 
19: # Logging
20: LOG_LEVEL=INFO
21: LOG_FILE=./logs/musictool.log
</file>

<file path=".gitignore">
1: logs/
2: cache/
3: __pycache__/
4: data/
</file>

<file path="LICENSE">
 1: MIT License
 2: 
 3: Copyright (c) 2025 Roel Fauconnier
 4: 
 5: Permission is hereby granted, free of charge, to any person obtaining a copy
 6: of this software and associated documentation files (the "Software"), to deal
 7: in the Software without restriction, including without limitation the rights
 8: to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 9: copies of the Software, and to permit persons to whom the Software is
10: furnished to do so, subject to the following conditions:
11: 
12: The above copyright notice and this permission notice shall be included in all
13: copies or substantial portions of the Software.
14: 
15: THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
16: IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
17: FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
18: AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
19: LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
20: OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
21: SOFTWARE.
</file>

<file path="monitor_expansion.py">
 1: import sqlite3
 2: import pandas as pd
 3: import time
 4: import sys
 5: from pathlib import Path
 6: from datetime import datetime
 7: sys.path.insert(0, str(Path(__file__).parent / "src" / "python"))
 8: from core.discogs_parser import DiscogsCSVParser
 9: def get_current_progress():
10:     try:
11:         conn = sqlite3.connect('./data/musictool.db')
12:         tracks_df = pd.read_sql('SELECT * FROM expanded_tracks', conn)
13:         conn.close()
14:         if len(tracks_df) == 0:
15:             return 0, 0, []
16:         unique_releases = tracks_df['discogs_release_id'].nunique()
17:         total_tracks = len(tracks_df)
18:         recent_releases = tracks_df.groupby('discogs_release_id').agg({
19:             'artist': 'first',
20:             'album': 'first',
21:             'created_at': 'first'
22:         }).sort_values('created_at', ascending=False).head(5)
23:         return unique_releases, total_tracks, recent_releases
24:     except Exception as e:
25:         print(f"Error reading database: {e}")
26:         return 0, 0, []
27: def get_total_releases():
28:     try:
29:         parser = DiscogsCSVParser('./data/gazmazk4ez-collection-20250608-1029.csv')
30:         all_releases = parser.parse()
31:         return len(all_releases)
32:     except Exception as e:
33:         print(f"Error reading CSV: {e}")
34:         return 598
35: def main():
36:     total_releases = get_total_releases()
37:     start_time = datetime.now()
38:     last_count = 0
39:     print("üéµ MusicTool Collection Expansion Monitor")
40:     print("=" * 50)
41:     print(f"üìä Total releases to process: {total_releases}")
42:     print(f"‚è∞ Started monitoring at: {start_time.strftime('%H:%M:%S')}")
43:     print("\nPress Ctrl+C to stop monitoring\n")
44:     try:
45:         while True:
46:             current_releases, current_tracks, recent = get_current_progress()
47:             progress_pct = (current_releases / total_releases) * 100
48:             remaining = total_releases - current_releases
49:             now = datetime.now()
50:             elapsed = (now - start_time).total_seconds()
51:             releases_added = current_releases - last_count if last_count > 0 else 0
52:             if elapsed > 0:
53:                 rate = current_releases / elapsed * 60
54:                 if rate > 0:
55:                     eta_minutes = remaining / (rate / 60)
56:                     eta_time = now + pd.Timedelta(minutes=eta_minutes)
57:                     eta_str = eta_time.strftime('%H:%M:%S')
58:                 else:
59:                     eta_str = "calculating..."
60:             else:
61:                 eta_str = "calculating..."
62:                 rate = 0
63:             print(f"\rüöÄ Progress: {current_releases:3d}/{total_releases} ({progress_pct:5.1f}%) | "
64:                   f"Tracks: {current_tracks:4d} | "
65:                   f"Rate: {rate:4.1f}/min | "
66:                   f"ETA: {eta_str}", end="")
67:             if int(elapsed) % 30 == 0 and len(recent) > 0:
68:                 print(f"\n\nüìÄ Recently expanded:")
69:                 for release_id, row in recent.iterrows():
70:                     print(f"   ‚Üí {release_id}: {row['artist']} - {row['album']}")
71:                 print()
72:             last_count = current_releases
73:             time.sleep(5)
74:     except KeyboardInterrupt:
75:         print(f"\n\n‚èπÔ∏è Monitoring stopped")
76:         print(f"üìä Final stats: {current_releases}/{total_releases} releases ({progress_pct:.1f}%)")
77:         print(f"üéµ Total tracks: {current_tracks}")
78: if __name__ == "__main__":
79:     main()
</file>

<file path="README.md">
  1: # MusicTool üéµ
  2: 
  3: **A comprehensive music collection management system for DJs, collectors, and music enthusiasts.**
  4: 
  5: MusicTool helps you analyze, organize, and optimize both your digital and physical music collections through intelligent gap analysis, duplicate detection, and collection insights.
  6: 
  7: ![Dashboard](https://img.shields.io/badge/Status-Active-green)
  8: ![Python](https://img.shields.io/badge/Python-3.8+-blue)
  9: ![Streamlit](https://img.shields.io/badge/UI-Streamlit-red)
 10: ![License](https://img.shields.io/badge/License-MIT-yellow)
 11: 
 12: ## ‚ú® Key Features
 13: 
 14: ### üîç **Intelligent Gap Analysis**
 15: - Compare physical and digital collections to find missing tracks
 16: - Advanced fuzzy matching algorithms for accurate identification
 17: - Performance-optimized for large collections (10,000+ tracks)
 18: - Configurable confidence thresholds and analysis modes
 19: 
 20: ### üîÑ **Smart Duplicate Detection**
 21: - Multiple detection methods (artist+title, filename, duration)
 22: - Identify space-saving opportunities in digital libraries
 23: - Quality-based recommendations for file management
 24: - Export capabilities for systematic cleanup
 25: 
 26: ### üìÄ **Collection Expansion**
 27: - Automatically expand Discogs releases into individual tracks
 28: - Rich metadata extraction (labels, catalog numbers, years)
 29: - Real-time progress monitoring with error handling
 30: - SQLite database for fast querying and analysis
 31: 
 32: ### üìä **Interactive Dashboard**
 33: - Visual analytics for collection insights
 34: - Genre distribution and format analysis
 35: - Collection growth tracking and statistics
 36: - Beautiful, responsive web interface
 37: 
 38: ## üöÄ Quick Start
 39: 
 40: ### Prerequisites
 41: - Python 3.8+
 42: - Discogs account with API access
 43: - Traktor NML export file
 44: - Discogs collection CSV export
 45: 
 46: ### Installation & Setup
 47: ```bash
 48: # Clone the repository
 49: git clone https://github.com/roel4ez/musictool.git
 50: cd musictool
 51: 
 52: # Install dependencies
 53: pip install -r requirements.txt
 54: 
 55: # Configure API access
 56: cp .env.example .env
 57: # Edit .env with your Discogs API key
 58: 
 59: # Launch the application
 60: streamlit run src/python/ui/streamlit_app.py
 61: ```
 62: 
 63: Open your browser to `http://localhost:8501` to access the MusicTool interface.
 64: 
 65: ## üìñ Documentation
 66: 
 67: ### User Guides
 68: - **[User Guide](docs/user-guide.md)** - Complete walkthrough for getting started
 69: - **[Feature Documentation](docs/features/README.md)** - Detailed feature descriptions and use cases
 70: - **[Technical Implementation](docs/technical-implementation.md)** - Architecture and algorithm details
 71: 
 72: ### Feature Docs
 73: - **[Gap Analysis Guide](docs/features/show-music-files.md)** - Find missing digital tracks
 74: - **[Collection Expansion](docs/adrs/2025-06-11-tracklist-database.md)** - Expand physical releases to track level
 75: 
 76: ## üèóÔ∏è Architecture
 77: 
 78: ```
 79: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
 80: ‚îÇ   Streamlit UI  ‚îÇ    ‚îÇ  Core Modules   ‚îÇ    ‚îÇ  Data Sources   ‚îÇ
 81: ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
 82: ‚îÇ ‚Ä¢ Dashboard     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ NML Parser    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ ‚Ä¢ Traktor NML   ‚îÇ
 83: ‚îÇ ‚Ä¢ Gap Analysis  ‚îÇ    ‚îÇ ‚Ä¢ Gap Analyzer  ‚îÇ    ‚îÇ ‚Ä¢ Discogs API   ‚îÇ
 84: ‚îÇ ‚Ä¢ Duplicate     ‚îÇ    ‚îÇ ‚Ä¢ Duplicate     ‚îÇ    ‚îÇ ‚Ä¢ SQLite DB     ‚îÇ
 85: ‚îÇ   Finder        ‚îÇ    ‚îÇ   Finder        ‚îÇ    ‚îÇ ‚Ä¢ CSV Files     ‚îÇ
 86: ‚îÇ ‚Ä¢ Tools         ‚îÇ    ‚îÇ ‚Ä¢ Collection    ‚îÇ    ‚îÇ                 ‚îÇ
 87: ‚îÇ                 ‚îÇ    ‚îÇ   Expander      ‚îÇ    ‚îÇ                 ‚îÇ
 88: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 89: ```
 90: 
 91: ### Core Components
 92: - **üéõÔ∏è Streamlit UI**: Beautiful, interactive web interface
 93: - **üîç Fast Gap Analyzer**: Performance-optimized matching algorithms
 94: - **üîÑ Duplicate Finder**: Multi-method duplicate detection
 95: - **üìÄ Collection Expander**: Discogs API integration for metadata expansion
 96: - **üíæ Data Layer**: SQLite database with optimized indexing
 97: 
 98: ## üéØ Use Cases
 99: 
100: ### For DJs
101: - **Ensure complete digital backup** of vinyl collection
102: - **Identify missing tracks** before gigs
103: - **Clean up digital libraries** by removing duplicates
104: - **Track collection growth** and acquisition priorities
105: 
106: ### For Collectors
107: - **Catalog and analyze** entire music collection
108: - **Discover collection patterns** and gaps
109: - **Optimize storage space** through duplicate management
110: - **Plan future acquisitions** based on data insights
111: 
112: ### For Music Libraries
113: - **Institutional collection management**
114: - **Data quality assurance** and cleanup
115: - **Collection development** planning
116: - **Digital preservation** gap analysis
117: 
118: ## üìä Performance Highlights
119: 
120: | Feature | Processing Speed | Accuracy | Scale |
121: |---------|-----------------|----------|-------|
122: | Gap Analysis | 10-50x faster than naive algorithms | 85%+ confidence matching | 10,000+ tracks |
123: | Duplicate Detection | 2-3 seconds per 1,000 tracks | Configurable precision | Any collection size |
124: | Collection Expansion | 2-3 tracks per second | 96%+ success rate | Unlimited releases |
125: 
126: ## üõ†Ô∏è Technology Stack
127: 
128: - **Backend**: Python 3.8+, Pandas, SQLite
129: - **UI**: Streamlit with custom CSS
130: - **APIs**: Discogs REST API, Traktor NML parsing
131: - **Algorithms**: FuzzyWuzzy string matching, optimized indexing
132: - **Storage**: Local SQLite database, file-based exports
133: 
134: ## üóÇÔ∏è Project Structure
135: 
136: ```
137: musictool/
138: ‚îú‚îÄ‚îÄ src/python/
139: ‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Core business logic
140: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nml_parser.py       # Traktor NML parsing
141: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gap_analyzer_fast.py # Performance-optimized gap analysis
142: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ duplicate_finder.py  # Fuzzy duplicate detection
143: ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ collection_expander.py # Discogs API integration
144: ‚îÇ   ‚îî‚îÄ‚îÄ ui/
145: ‚îÇ       ‚îî‚îÄ‚îÄ streamlit_app.py     # Web interface
146: ‚îú‚îÄ‚îÄ data/                        # Data files and database
147: ‚îÇ   ‚îú‚îÄ‚îÄ musictool.db            # SQLite database
148: ‚îÇ   ‚îú‚îÄ‚îÄ collection.nml          # Traktor export
149: ‚îÇ   ‚îî‚îÄ‚îÄ *.csv                   # Discogs exports
150: ‚îú‚îÄ‚îÄ docs/                        # Documentation
151: ‚îÇ   ‚îú‚îÄ‚îÄ features/               # Feature documentation
152: ‚îÇ   ‚îú‚îÄ‚îÄ adrs/                   # Architecture Decision Records
153: ‚îÇ   ‚îî‚îÄ‚îÄ *.md                    # User guides and technical docs
154: ‚îî‚îÄ‚îÄ tests/                       # Test suite
155:     ‚îú‚îÄ‚îÄ python/                 # Python tests
156:     ‚îî‚îÄ‚îÄ data/                   # Test data
157: ```
158: 
159: ## ü§ù Contributing
160: 
161: We welcome contributions! Please see our contributing guidelines for details on:
162: - Code style and standards
163: - Testing requirements
164: - Documentation updates
165: - Feature requests and bug reports
166: 
167: ### Development Setup
168: ```bash
169: # Clone and setup development environment
170: git clone https://github.com/roel4ez/musictool.git
171: cd musictool
172: 
173: # Create virtual environment
174: python -m venv venv
175: source venv/bin/activate  # or venv\Scripts\activate on Windows
176: 
177: # Install development dependencies
178: pip install -r requirements.txt
179: pip install -r requirements-dev.txt
180: 
181: # Run tests
182: pytest tests/
183: 
184: # Start development server
185: streamlit run src/python/ui/streamlit_app.py
186: ```
187: 
188: ## üìà Roadmap
189: 
190: ### Upcoming Features
191: - **ü§ñ AI-Powered Recommendations**: Machine learning for collection insights
192: - **‚òÅÔ∏è Cloud Integration**: Sync collections across devices
193: - **üéµ Additional Data Sources**: Spotify, Apple Music integration
194: - **üì± Mobile Interface**: Responsive design improvements
195: - **üîÑ Automated Workflows**: Scheduled analysis and reporting
196: 
197: ### Version History
198: - **v1.0** - Core gap analysis and collection management
199: - **v1.1** - Performance optimizations and duplicate detection
200: - **v1.2** - Enhanced UI and documentation (current)
201: 
202: ## üìù License
203: 
204: This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
205: 
206: ## üôè Acknowledgments
207: 
208: - **Discogs API** for comprehensive music metadata
209: - **Native Instruments** for Traktor NML format documentation
210: - **Streamlit** for the excellent web framework
211: - **FuzzyWuzzy** for fuzzy string matching capabilities
212: 
213: ## üìû Support
214: 
215: - **üìñ Documentation**: Check our comprehensive [user guide](docs/user-guide.md)
216: - **üêõ Bug Reports**: Open an issue on GitHub
217: - **üí° Feature Requests**: Start a discussion in GitHub Discussions
218: - **‚ùì Questions**: Check existing issues or start a new discussion
219: 
220: ---
221: 
222: **MusicTool** - Because your music collection deserves better organization! üéµ‚ú®
</file>

<file path="requirements.txt">
 1: # MusicTool MVP Dependencies
 2: # Core Data Processing
 3: pandas>=2.0.0
 4: numpy>=1.24.0
 5: 
 6: # UI Framework
 7: streamlit>=1.28.0
 8: plotly>=5.17.0
 9: 
10: # HTTP Requests & API
11: requests>=2.31.0
12: requests-ratelimiter>=0.4.0
13: 
14: # Environment & Configuration
15: python-dotenv>=1.0.0
16: 
17: # XML Parsing (for NML files)
18: lxml>=4.9.0
19: 
20: # String Matching & Fuzzy Search
21: fuzzywuzzy>=0.18.0
22: python-Levenshtein>=0.21.0
23: 
24: # Data Storage
25: # sqlite3 is built into Python - no need to install
26: 
27: # Development & Testing
28: pytest>=7.4.0
29: pytest-cov>=4.1.0
30: 
31: # Optional: Performance improvements
32: # pyarrow>=13.0.0  # For faster parquet I/O if we choose that format
</file>

<file path="run_full_expansion.py">
  1: import os
  2: import sys
  3: import logging
  4: from pathlib import Path
  5: from datetime import datetime
  6: sys.path.insert(0, str(Path(__file__).parent / "src" / "python"))
  7: from core.collection_expander import CollectionExpander
  8: from core.discogs_client import load_api_key_from_env
  9: def setup_logging():
 10:     log_dir = Path("./logs")
 11:     log_dir.mkdir(exist_ok=True)
 12:     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
 13:     log_file = log_dir / f"expansion_{timestamp}.log"
 14:     logging.basicConfig(
 15:         level=logging.INFO,
 16:         format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
 17:         handlers=[
 18:             logging.FileHandler(log_file),
 19:             logging.StreamHandler(sys.stdout)
 20:         ]
 21:     )
 22:     logger = logging.getLogger(__name__)
 23:     logger.info(f"üéµ Starting full collection expansion")
 24:     logger.info(f"üìù Logging to: {log_file}")
 25:     return logger
 26: def check_prerequisites():
 27:     logger = logging.getLogger(__name__)
 28:     csv_path = "./data/gazmazk4ez-collection-20250608-1029.csv"
 29:     if not Path(csv_path).exists():
 30:         logger.error(f"‚ùå Discogs CSV not found: {csv_path}")
 31:         return False
 32:     api_key = load_api_key_from_env()
 33:     if not api_key:
 34:         logger.error("‚ùå DISCOGS_API_KEY not found in environment or .env file")
 35:         return False
 36:     logger.info("‚úÖ All prerequisites met")
 37:     return True
 38: def get_current_status(expander):
 39:     logger = logging.getLogger(__name__)
 40:     try:
 41:         existing_tracks = expander.load_physical_collection()
 42:         if len(existing_tracks) > 0:
 43:             unique_releases = existing_tracks['discogs_release_id'].nunique()
 44:             total_tracks = len(existing_tracks)
 45:             logger.info(f"üìä Current database status:")
 46:             logger.info(f"   ‚Üí {unique_releases} releases already expanded")
 47:             logger.info(f"   ‚Üí {total_tracks} total tracks in database")
 48:             return unique_releases, total_tracks
 49:         else:
 50:             logger.info("üìä Starting with empty database")
 51:             return 0, 0
 52:     except Exception as e:
 53:         logger.info(f"üìä Starting with fresh database (error reading existing: {e})")
 54:         return 0, 0
 55: def main():
 56:     logger = setup_logging()
 57:     try:
 58:         if not check_prerequisites():
 59:             logger.error("‚ùå Prerequisites not met. Exiting.")
 60:             return 1
 61:         api_key = load_api_key_from_env()
 62:         expander = CollectionExpander(api_key)
 63:         existing_releases, existing_tracks = get_current_status(expander)
 64:         csv_path = "./data/gazmazk4ez-collection-20250608-1029.csv"
 65:         logger.info("üöÄ Starting full collection expansion...")
 66:         logger.info(f"üìÅ Source: {csv_path}")
 67:         logger.info(f"üóÑÔ∏è Database: ./data/musictool.db")
 68:         logger.info(f"‚öôÔ∏è Skip existing: True (idempotent)")
 69:         logger.info(f"üîÑ Rate limiting: Enabled (1 req/sec)")
 70:         start_time = datetime.now()
 71:         expanded_df = expander.expand_collection(
 72:             csv_path,
 73:             max_releases=None,
 74:             skip_existing=True
 75:         )
 76:         end_time = datetime.now()
 77:         duration = end_time - start_time
 78:         final_releases = expanded_df['discogs_release_id'].nunique() if len(expanded_df) > 0 else 0
 79:         final_tracks = len(expanded_df)
 80:         new_releases = final_releases - existing_releases
 81:         new_tracks = final_tracks - existing_tracks
 82:         logger.info("üéâ Full expansion completed!")
 83:         logger.info(f"‚è±Ô∏è Duration: {duration}")
 84:         logger.info(f"üìä Final totals:")
 85:         logger.info(f"   ‚Üí {final_releases} releases ({new_releases} new)")
 86:         logger.info(f"   ‚Üí {final_tracks} tracks ({new_tracks} new)")
 87:         if new_releases > 0:
 88:             avg_tracks_per_release = new_tracks / new_releases if new_releases > 0 else 0
 89:             logger.info(f"   ‚Üí Average {avg_tracks_per_release:.1f} tracks per release")
 90:         logger.info("‚úÖ Collection expansion successful!")
 91:         return 0
 92:     except KeyboardInterrupt:
 93:         logger.info("‚èπÔ∏è Expansion interrupted by user")
 94:         logger.info("üí° Progress has been saved. You can resume by running this script again.")
 95:         return 1
 96:     except Exception as e:
 97:         logger.error(f"‚ùå Expansion failed: {e}")
 98:         logger.exception("Full error details:")
 99:         return 1
100: if __name__ == "__main__":
101:     exit_code = main()
102:     sys.exit(exit_code)
</file>

<file path="test_collection_expander.py">
 1: import sys
 2: import os
 3: project_root = os.path.dirname(os.path.abspath(__file__))
 4: sys.path.insert(0, project_root)
 5: from src.python.core.collection_expander import CollectionExpander, expand_collection_cli, load_api_key_from_env
 6: def test_collection_expander():
 7:     print("üöÄ Testing Collection Expander...")
 8:     print("‚ö†Ô∏è  Testing with only 5 releases to respect rate limits")
 9:     api_key = load_api_key_from_env()
10:     if not api_key:
11:         print("‚ùå No Discogs API key found in .env file")
12:         return
13:     csv_file = "./data/gazmazk4ez-collection-20250608-1029.csv"
14:     if not os.path.exists(csv_file):
15:         print(f"‚ùå Discogs CSV file not found: {csv_file}")
16:         return
17:     try:
18:         expanded_df = expand_collection_cli(csv_file, max_releases=5)
19:         if expanded_df is not None and not expanded_df.empty:
20:             print(f"\nüìä Sample expanded tracks:")
21:             print(expanded_df[['artist', 'title', 'album', 'format_type', 'release_year']].head(10))
22:             print(f"\nüéµ Track breakdown by release:")
23:             track_counts = expanded_df.groupby(['album', 'discogs_release_id']).size()
24:             for (album, release_id), count in track_counts.items():
25:                 print(f"   {album[:40]:40} ({release_id}): {count} tracks")
26:             print(f"\nüíæ Testing database persistence...")
27:             expander = CollectionExpander(api_key)
28:             loaded_df = expander.load_physical_collection()
29:             print(f"‚úÖ Loaded {len(loaded_df)} tracks from database")
30:             stats = expander.get_expansion_stats()
31:             print(f"\nüìà Collection stats:")
32:             print(f"   Total tracks: {stats['total_tracks']}")
33:             print(f"   Total releases: {stats['total_releases']}")
34:             print(f"   Unique artists: {stats['unique_artists']}")
35:             print(f"   Format breakdown: {stats['format_breakdown']}")
36:             print(f"   Year range: {stats['year_range']['earliest']}-{stats['year_range']['latest']}")
37:             return expanded_df
38:         else:
39:             print("‚ùå No tracks were expanded")
40:             return None
41:     except Exception as e:
42:         print(f"‚ùå Error testing Collection Expander: {e}")
43:         import traceback
44:         traceback.print_exc()
45:         return None
46: if __name__ == "__main__":
47:     test_collection_expander()
</file>

<file path="test_discogs_api.py">
 1: import sys
 2: import os
 3: project_root = os.path.dirname(os.path.abspath(__file__))
 4: sys.path.insert(0, project_root)
 5: from src.python.core.discogs_client import DiscogsAPIClient, load_api_key_from_env
 6: from src.python.core.discogs_parser import DiscogsCSVParser
 7: def test_discogs_api():
 8:     api_key = load_api_key_from_env()
 9:     if not api_key:
10:         print("‚ùå No Discogs API key found in .env file")
11:         print("Please add DISCOGS_API_KEY=your_key_here to .env")
12:         return
13:     print("üéµ Testing Discogs API Client...")
14:     print("‚ö†Ô∏è  Testing with only 3 releases to respect rate limits")
15:     try:
16:         client = DiscogsAPIClient(api_key)
17:         csv_file = "./data/gazmazk4ez-collection-20250608-1029.csv"
18:         if os.path.exists(csv_file):
19:             parser = DiscogsCSVParser(csv_file)
20:             df = parser.parse()
21:             test_release_ids = df['release_id'].head(3).tolist()
22:             print(f"Testing with releases: {test_release_ids}")
23:             results = client.batch_get_releases(test_release_ids, max_requests=3)
24:             print(f"\n‚úÖ Successfully fetched {len(results)} releases")
25:             for release_id, release_data in results.items():
26:                 print(f"\nüìÄ Release {release_id}:")
27:                 print(f"   Title: {release_data.get('title', 'Unknown')}")
28:                 print(f"   Artists: {[a.get('name', '') for a in release_data.get('artists', [])]}")
29:                 print(f"   Label: {[l.get('name', '') for l in release_data.get('labels', [])]}")
30:                 print(f"   Year: {release_data.get('year', 'Unknown')}")
31:                 tracklist = client.get_release_tracklist(release_id)
32:                 print(f"   Tracks: {len(tracklist)} found")
33:                 for i, track in enumerate(tracklist[:3]):
34:                     if track['type_'] == 'track':
35:                         print(f"      {track['position']}: {track['title']}")
36:                 if len(tracklist) > 3:
37:                     print(f"      ... and {len(tracklist) - 3} more tracks")
38:             cache_stats = client.get_cache_stats()
39:             print(f"\nüìä Cache stats:")
40:             print(f"   Cached releases: {cache_stats['cached_releases']}")
41:             print(f"   Cache size: {cache_stats['cache_size_mb']:.2f} MB")
42:             return results
43:         else:
44:             print(f"‚ùå Discogs CSV file not found: {csv_file}")
45:             return None
46:     except Exception as e:
47:         print(f"‚ùå Error testing Discogs API: {e}")
48:         import traceback
49:         traceback.print_exc()
50:         return None
51: if __name__ == "__main__":
52:     test_discogs_api()
</file>

<file path="test_discogs_parser.py">
 1: import sys
 2: import os
 3: project_root = os.path.dirname(os.path.abspath(__file__))
 4: sys.path.insert(0, project_root)
 5: from src.python.core.discogs_parser import DiscogsCSVParser
 6: def test_discogs_parser():
 7:     csv_file = "./data/gazmazk4ez-collection-20250608-1029.csv"
 8:     if not os.path.exists(csv_file):
 9:         print(f"‚ùå Discogs CSV file not found: {csv_file}")
10:         return
11:     try:
12:         print("üéµ Testing Discogs CSV Parser...")
13:         parser = DiscogsCSVParser(csv_file)
14:         df = parser.parse()
15:         print(f"‚úÖ Successfully parsed {len(df)} releases")
16:         print("\nüìä Sample data:")
17:         print(df[['artist_clean', 'title', 'label', 'format_type', 'release_year']].head())
18:         print(f"\nüìà Summary stats:")
19:         print(f"- Unique artists: {df['artist_clean'].nunique()}")
20:         print(f"- Unique labels: {df['label'].nunique()}")
21:         print(f"- Collection folders: {df['collection_folder'].value_counts().to_dict()}")
22:         print(f"- Format types: {df['format_type'].value_counts().to_dict()}")
23:         print(f"- Release years: {df['release_year'].value_counts().head().to_dict()}")
24:         return df
25:     except Exception as e:
26:         print(f"‚ùå Error testing Discogs parser: {e}")
27:         import traceback
28:         traceback.print_exc()
29:         return None
30: if __name__ == "__main__":
31:     test_discogs_parser()
</file>

<file path="test_gap_analyzer.py">
 1: import sys
 2: import os
 3: project_root = os.path.dirname(os.path.abspath(__file__))
 4: sys.path.insert(0, project_root)
 5: from src.python.core.gap_analyzer import GapAnalyzer, analyze_gaps_cli
 6: def test_gap_analyzer():
 7:     print("üéØ Testing Gap Analyzer...")
 8:     print("Comparing 23 physical tracks vs 3,003 digital tracks")
 9:     nml_file = "./data/collection.nml"
10:     if not os.path.exists(nml_file):
11:         print(f"‚ùå NML file not found: {nml_file}")
12:         return
13:     try:
14:         gap_results = analyze_gaps_cli(nml_file, confidence=80)
15:         if gap_results is not None and not gap_results.empty:
16:             print(f"\nüîç Detailed Analysis:")
17:             found_tracks = gap_results[gap_results['status'] == 'found']
18:             missing_tracks = gap_results[gap_results['status'] == 'missing']
19:             if not found_tracks.empty:
20:                 print(f"\n‚úÖ FOUND TRACKS ({len(found_tracks)}):")
21:                 for idx, track in found_tracks.iterrows():
22:                     print(f"   üéµ {track['physical_artist']} - {track['physical_title']}")
23:                     print(f"      ‚Üí {track['digital_artist']} - {track['digital_title']} ({track['confidence']:.1f}%)")
24:                     print(f"      Album: {track['physical_album']} ‚Üí {track['digital_album']}")
25:                     print()
26:             if not missing_tracks.empty:
27:                 print(f"\n‚ùå MISSING TRACKS ({len(missing_tracks)}):")
28:                 for idx, track in missing_tracks.iterrows():
29:                     print(f"   üéµ {track['physical_artist']} - {track['physical_title']}")
30:                     print(f"      Album: {track['physical_album']} ({track['physical_year']})")
31:                     print(f"      Label: {track['physical_label']} [{track['physical_format']}]")
32:                     if track['confidence'] > 50:
33:                         print(f"      Best match: {track['digital_artist']} - {track['digital_title']} ({track['confidence']:.1f}%)")
34:                     print()
35:             print(f"üé™ Additional Insights:")
36:             print(f"   Confidence range: {gap_results['confidence'].min():.1f}% - {gap_results['confidence'].max():.1f}%")
37:             print(f"   Average artist score: {gap_results['artist_score'].mean():.1f}%")
38:             print(f"   Average title score: {gap_results['title_score'].mean():.1f}%")
39:             return gap_results
40:         else:
41:             print("‚ùå No gap analysis results")
42:             return None
43:     except Exception as e:
44:         print(f"‚ùå Error testing Gap Analyzer: {e}")
45:         import traceback
46:         traceback.print_exc()
47:         return None
48: if __name__ == "__main__":
49:     test_gap_analyzer()
</file>

<file path="test_nml_parser.py">
 1: import sys
 2: import os
 3: project_root = os.path.dirname(os.path.abspath(__file__))
 4: sys.path.insert(0, project_root)
 5: from src.python.core.nml_parser import NMLParser
 6: def test_nml_parser():
 7:     nml_file = "./data/collection.nml"
 8:     if not os.path.exists(nml_file):
 9:         print(f"‚ùå NML file not found: {nml_file}")
10:         return
11:     try:
12:         print("üéµ Testing NML Parser...")
13:         parser = NMLParser(nml_file)
14:         df = parser.parse()
15:         print(f"‚úÖ Successfully parsed {len(df)} tracks")
16:         print("\nüìä Sample data:")
17:         print(df[['artist', 'title', 'filetype', 'filesize', 'bpm']].head())
18:         print(f"\nüìà Summary stats:")
19:         print(f"- Unique artists: {df['artist'].nunique()}")
20:         print(f"- Unique albums: {df['album'].nunique()}")
21:         print(f"- Genres: {df['genre'].nunique()}")
22:         print(f"- Average BPM: {df['bpm'].mean():.1f}")
23:         print(f"- File types: {df['filetype'].value_counts().to_dict()}")
24:         print(f"- Total collection size: {df['filesize'].sum() / (1024**3):.2f} GB")
25:         return df
26:     except Exception as e:
27:         print(f"‚ùå Error testing NML parser: {e}")
28:         return None
29: if __name__ == "__main__":
30:     test_nml_parser()
</file>

</files>
